[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Kevin Callison\n   Tidewater 1915\n   kcallson@tulane.edu\n\n\n\n\n\n   Tuesday/Thursday\n   2:15-3:15 PM\n   Tidewater 1915 or Zoom\n\n\n\n\n\n\n   Tuesday/Thursday\n   1:00–2:15 PM\n   Tidewater 1202"
  },
  {
    "objectID": "syllabus.html#disclosures-of-gender-based-discrimination",
    "href": "syllabus.html#disclosures-of-gender-based-discrimination",
    "title": "Syllabus",
    "section": "Disclosures of gender-based discrimination",
    "text": "Disclosures of gender-based discrimination\nIn order to comply with the requirements of Title IX of the Education Amendments of 1972, Tulane University requires all faculty members to report incidents of gender-based discrimination. Please know that if you choose to confide in me, I am required by the university to share your disclosure in a Care Connection to the Office of Case Management and Victim Support Services to be sure you are connected with all the support the university can offer. The Title IX Coordinator is also notified of these disclosures. You choose whether or not you want to meet with these offices. You can also make a disclosure yourself, including an anonymous report, through this form."
  },
  {
    "objectID": "syllabus.html#statement-on-confidentiality-and-privacy",
    "href": "syllabus.html#statement-on-confidentiality-and-privacy",
    "title": "Syllabus",
    "section": "Statement on Confidentiality and Privacy",
    "text": "Statement on Confidentiality and Privacy\nTulane University is committed to protecting the privacy of all individuals involved in a disclosure of gender-based discrimination. Any and all of your communications on these matters will be treated as either “Confidential” or “Private.”\n\n\n\n\n\n\n\nConfidential\nPrivate\n\n\n\n\nCertain individuals and resources (see list below) are designated as confidential. Individuals and resources designated as confidential will not share any information, except in extreme circumstances involving imminent danger to one’s self or others, with the Office of Case Management and Victim Services, the Title IX Coordinator, or local law enforcement without the express permission of the disclosing party.\nPrivate resources means that information related to a disclosure of gender-based discrimination may be shared with key staff members of the University to assist in the review, investigation, or resolution of the disclosure or to deliver resources, accommodations, and support services. Information pertinent to the disclosure will be shared with the following Offices:\n\n\nCounseling & Psychological Services (CAPS): (504) 314-2277\nCase Management & Victim Support Services (CMVSS): (504) 314-2160\n\n\nStudent Health Center Downtown: (504) 988-6929 Uptown: (504) 865-5255\nTulane University Police (TUPD) Downtown: (504) 988-5531 Uptown: (504) 865-5911\n\n\nSexual Aggression Peer Hotline and Education (SAPHE): (504) 654-9543\nTitle IX Coordinator: (504) 865-5611 or email: titleix@tulane.edu"
  },
  {
    "objectID": "syllabus.html#title-ix-safeguards-for-pregnant-and-parenting-students",
    "href": "syllabus.html#title-ix-safeguards-for-pregnant-and-parenting-students",
    "title": "Syllabus",
    "section": "Title IX Safeguards for Pregnant and Parenting Students",
    "text": "Title IX Safeguards for Pregnant and Parenting Students\nTitle IX also provides reasonable protections and support for pregnant and parenting students. Discrimination on the basis of a student’s pregnancy, childbirth, false pregnancy, termination of pregnancy, or recovery from any of the previous conditions is prohibited by Title IX, and Tulane is committed to providing equal access to academic programs and extracurricular activities to students who might be, are, or have been pregnant. If you need support related to a pregnancy or any of the previously listed conditions, visit pregnancy.tulane.edu for more information, including a list of resources. Student who believe that they may have experienced pregnancy discrimination can file a complaint with the Title IX Office by contacting 504-865-5611 or titleix@tulane.eduor visiting the office in Jones Hall 308, filing a report at: https://cm.maxient.com/reportingform.php?TulaneUniv=&layout_id=0\n\nEmergency Preparedness & Response"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The detailed course schedule for the semester is below. Please note that dates and topics are subject to change and will depend on the pace of the course. Please stay apprised of any changes announced in class. You can find the materials for each course meeting under the “materials” links for that week in the schedule below. You should generally:\n\nwatch the lecture videos (if any) and complete the readings before the assigned class period\nsubmit any reading discussion questions, problem sets, or other assignments by 1pm on the assigned due date\n\nHere’s a guide to the schedule:\n\nMaterials (): This page contains the readings, slides, and videos (if any) for the topic. Do the readings and watch the videos before coming to class on the indicated date. Slides will typically be posted before each class.\nTutorial (): A link to the tutorial for that week (if any). Note that we will complete tutorials in class - these are NOT homework.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 10:30 AM on the day they’re listed.\n\n\n\n\n\nDate\n\n\nTopic\n\n\nMaterials\n\n\nTutorial\n\n\nAssignment\n\n\n\n\n\n\nWeek 1\n\n\n\n\nJanuary 14\n\n\nCourse Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 16\n\n\nU.S. Health Policy Landscape\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nJanuary 21\n\n\nPolicy Analysis Overview\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 23\n\n\nPolicy Analysis Overview\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nJanuary 28\n\n\nIntro to Computing Resources\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 30\n\n\nComputing Resources Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nFebruary 4\n\n\nImporting Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 6\n\n\nImporting Data Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\nFebruary 11\n\n\nNo Class - Mardi Gras!\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 13\n\n\nPerspectives on Federal Health Policy - Guest Speaker: Chip Kahn\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\nFebruary 18\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 20\n\n\nData Wrangling Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\nFebruary 25\n\n\nNo Class - Capitol Trip\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 27\n\n\nSummarizing Data Lab (Problem Statement Due)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\nMarch 4\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 6\n\n\nData Visualization Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\nMarch 11\n\n\nResearch Design - Randomization and Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 13\n\n\nResearch Design - Randomization and Regression (Preliminary Reference List Due)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\nMarch 18\n\n\nResearch Design - Natural Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 20\n\n\nResearch Design - Natural Experiments (Introduction Due)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\nMarch 25\n\n\nNo Class - Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 27\n\n\nNo Class - Spring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\nApril 1\n\n\nResearch Design - Quasi-Experimental Designs\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 3\n\n\nResearch Design - Quasi-Experimental Designs (Issue Analysis Draft Due)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\nApril 8\n\n\nNo Class\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 10\n\n\nNo Class (Proposed Solutions Draft Due)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\n\nApril 15\n\n\nPractice Case Study\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 17\n\n\nPractice Case Study\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\n\n\n\n\nApril 22\n\n\nDC Experience\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 24\n\n\nDC Experience (Strategic Recommendations Draft Due)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 16\n\n\n\n\nApril 29\n\n\nCase Presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1\n\n\nNo Class (Executive Summary and Conclusions Drafts Due)"
  },
  {
    "objectID": "resources/study-halls.html",
    "href": "resources/study-halls.html",
    "title": "Study Hall Schedule",
    "section": "",
    "text": "For the latest schedule, including changes to the schedule, please check Ed or the Gov 50 Google calendar.\n\n\n\n\nName\n\n\nday\n\n\nStart Time\n\n\nEnd Time\n\n\nLocation\n\n\n\n\n\n\nKate De Groote\n\n\nFriday\n\n\n1:00 PM\n\n\n3:00 PM\n\n\nLowell Dining Hall\n\n\n\n\nIsa Peña\n\n\nFriday\n\n\n2:00 PM\n\n\n4:00 PM\n\n\nCabot Dining Hall\n\n\n\n\nJulia Poulson\n\n\nFriday\n\n\n4:00 PM\n\n\n6:00 PM\n\n\nPfoho Dining Hall\n\n\n\n\nEric Forteza\n\n\nSunday\n\n\n5:00 PM\n\n\n7:00 PM\n\n\nCabot Dining Hall\n\n\n\n\nGowri Rangu\n\n\nSunday\n\n\n7:30 PM\n\n\n9:30 PM\n\n\nLowell Dining Hall\n\n\n\n\nMitja Bof\n\n\nSunday\n\n\n8:00 PM\n\n\n10:00 PM\n\n\nWinthrop Dining Hall\n\n\n\n\nJason Wang\n\n\nSunday\n\n\n8:00 PM\n\n\n10:00 PM\n\n\nLeverett Dining Hall\n\n\n\n\nIsa Peña\n\n\nMonday\n\n\n8:00 AM\n\n\n10:00 AM\n\n\nCabot Dining Hall\n\n\n\n\nRyan McCarthy\n\n\nMonday\n\n\n9:00 AM\n\n\n11:00 AM\n\n\nEliot Dining Hall\n\n\n\n\nAlex Heuss\n\n\nMonday\n\n\n4:30 PM\n\n\n6:30 PM\n\n\nCGIS Cafe\n\n\n\n\nFrank T Berrios\n\n\nMonday\n\n\n5:00 PM\n\n\n7:00 PM\n\n\nCabot Dining Hall\n\n\n\n\nChris Mesfin\n\n\nMonday\n\n\n7:00 PM\n\n\n9:00 PM\n\n\nPfoho Dining Hall\n\n\n\n\nAzeez Richardson\n\n\nMonday\n\n\n7:00 PM\n\n\n9:00 PM\n\n\nDunster Dining Hall\n\n\n\n\nVivian Nguyen\n\n\nMonday\n\n\n8:00 PM\n\n\n10:00 PM\n\n\nMather Dining Hall\n\n\n\n\nAlina Esanu\n\n\nTuesday\n\n\n9:00 AM\n\n\n11:00 AM\n\n\nQuincy Dining Hall\n\n\n\n\nZachary Mecca\n\n\nTuesday\n\n\n9:30 AM\n\n\n11:30 AM\n\n\nEliot Dining Hall\n\n\n\n\nTracy Jiang\n\n\nTuesday\n\n\n3:00 PM\n\n\n5:00 PM\n\n\nAdam Dining Hall\n\n\n\n\nCameron Snowden\n\n\nTuesday\n\n\n4:00 PM\n\n\n6:00 PM\n\n\nPfoho Dining Hall\n\n\n\n\nGroup Study Hall\n\n\nTuesday\n\n\n6:00 PM\n\n\n11:00 PM\n\n\nCGIS Cafe (First Floor)\n\n\n\n\nGowri Rangu\n\n\nWednesday\n\n\n2:00 PM\n\n\n4:00 PM\n\n\nLowell Dining Hall\n\n\n\n\nGroup Study Hall\n\n\nWednesday\n\n\n6:00 PM\n\n\n11:00 PM\n\n\nCGIS Cafe (First Floor)"
  },
  {
    "objectID": "resources/office-hours.html",
    "href": "resources/office-hours.html",
    "title": "Office House Schedule",
    "section": "",
    "text": "For the latest schedule, including changes to the schedule, please check Ed or the Gov 50 Google calendar.\n\n\n\n\nName\n\n\nDay\n\n\nStart Time\n\n\nEnd Time\n\n\nLocation\n\n\n\n\n\n\nAhmet Akbiyik\n\n\nThursday\n\n\n7:00 PM\n\n\n8:00 PM\n\n\nDunster House Dining Hall\n\n\n\n\nAda Cruz\n\n\nFriday\n\n\n10:00 AM\n\n\n12:00 PM\n\n\nEliot Dhall\n\n\n\n\nEthan Miles\n\n\nFriday\n\n\n2:00 PM\n\n\n3:00 PM\n\n\nCGIS K Café\n\n\n\n\nJerry Min\n\n\nFriday\n\n\n5:00:00 PM\n\n\n6:00 PM\n\n\nCGIS K Café\n\n\n\n\nShriya Yarlagadda\n\n\nSunday\n\n\n8:00 PM\n\n\n9:00 PM\n\n\nLowell DHall\n\n\n\n\nAndy Wang\n\n\nMonday\n\n\n3:00 PM\n\n\n4:00 PM\n\n\nFairfax Common Room\n\n\n\n\nKatherine Jackson\n\n\nMonday\n\n\n5:00 PM\n\n\n7:00 PM\n\n\nPfoho Dhall - 2nd Floor\n\n\n\n\nJulio Solis\n\n\nTuesday\n\n\n1:15 PM\n\n\n2:15 PM\n\n\nEmerson Hall\n\n\n\n\nJames Jolin\n\n\nTuesday\n\n\n4:00 PM\n\n\n5:00 PM\n\n\nCGIS K Café\n\n\n\n\nPranav Moudgalya\n\n\nTuesday\n\n\n8:00 PM\n\n\n9:00 PM\n\n\nLeverett Dining Hall\n\n\n\n\nChris Shen\n\n\nWednesday\n\n\n9:00 PM\n\n\n10:00 PM\n\n\nQuincy Dining Hall"
  },
  {
    "objectID": "resources/cheatsheet.html",
    "href": "resources/cheatsheet.html",
    "title": "R Programming Cheat Sheet",
    "section": "",
    "text": "You will need to load the following libraries in RStudio before attempting some of the techniques in this tutorial.\nlibrary(tidyverse)\ninstall.packages(\"nycflights13\")\n\nThe following package(s) will be installed:\n- nycflights13 [1.0.2]\nThese packages will be installed into \"~/Dropbox/Documents/Teaching Materials/Health Policy/GitHub Site/hpam7660-sp24/renv/library/R-4.3/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing nycflights13 ...                   OK [linked from cache]\nSuccessfully installed 1 package in 17 milliseconds.\n\nlibrary(nycflights13)\ninstall.packages(\"gapminder\")\n\nThe following package(s) will be installed:\n- gapminder [1.0.0]\nThese packages will be installed into \"~/Dropbox/Documents/Teaching Materials/Health Policy/GitHub Site/hpam7660-sp24/renv/library/R-4.3/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing gapminder ...                      OK [linked from cache]\nSuccessfully installed 1 package in 12 milliseconds.\n\nlibrary(gapminder)"
  },
  {
    "objectID": "resources/cheatsheet.html#r-basics",
    "href": "resources/cheatsheet.html#r-basics",
    "title": "R Programming Cheat Sheet",
    "section": "R Basics",
    "text": "R Basics\n\nCreating a vector\nYou can create a vector using the c function:\n\n## Any R code that begins with the # character is a comment\n## Comments are ignored by R\n\nmy_numbers &lt;- c(4, 8, 15, 16, 23, 42) # Anything after # is also a\n# comment\nmy_numbers\n\n[1]  4  8 15 16 23 42\n\n\n\n\nInstalling and loading a package\nYou can install a package with the install.packages function, passing the name of the package to be installed as a string (that is, in quotes):\n\ninstall.packages(\"ggplot2\")\n\nYou can load a package into the R environment by calling library() with the name of package without quotes. You should only have one package per library call.\n\nlibrary(ggplot2)\n\n\n\nCalling functions from specific packages\nWe can also use the mypackage:: prefix to access package functions without loading:\n\nknitr::kable(head(mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1"
  },
  {
    "objectID": "resources/cheatsheet.html#data-visualization",
    "href": "resources/cheatsheet.html#data-visualization",
    "title": "R Programming Cheat Sheet",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nScatter plot\nYou can produce a scatter plot with using the x and y aesthetics along with the geom_point() function.\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nSmoothed curves\nYou can add a smoothed curve that summarizes the relationship between two variables with the geom_smooth() function. By default, it uses a loess smoother to estimated the conditional mean of the y-axis variable as a function of the x-axis variable.\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nAdding a regression line\ngeom_smooth can also add a regression line by setting the argument method = \"lm\" and we can turn off the shaded regions around the line with se = FALSE\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nChanging the scale of the axes\nIf we want the scale of the x-axis to be logged to stretch out the data we can use the scale_x_log10():\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_x_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nAdding informative labels to a plot\nUse the labs() to add informative labels to the plot:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  scale_x_log10() +\n  labs(x = \"Population Density\",\n       y = \"Percent of County Below Poverty Line\",\n       title = \"Poverty and Population Density\",\n       subtitle = \"Among Counties in the Midwest\",\n       source = \"US Census, 2000\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMapping aesthetics to variables\nIf you would like to map an aesthetic to a variable for all geoms in the plot, you can put it in the aes call in the ggplot() function:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty,\n                     color = state,\n                     fill = state)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMapping aesthetics for a single geom\nYou can also map aesthetics for a specific geom using the mapping argument to that function:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point(mapping = aes(color = state)) +\n  geom_smooth(color = \"black\") +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nSetting the aesthetics for all observations\nIf you would like to set the color or size or shape of a geom for all data points (that is, not mapped to any variables), be sure to set these outside of aes():\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point(color = \"purple\") +\n  geom_smooth() +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nHistograms\n\nggplot(data = midwest,\n       mapping = aes(x = percbelowpoverty)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "resources/cheatsheet.html#data-wrangling",
    "href": "resources/cheatsheet.html#data-wrangling",
    "title": "R Programming Cheat Sheet",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nSubsetting a data frame\nUse the filter() function from the dplyr package to subset a data frame. In this example, you’ll use the nycflights13 data and filter by United Airlines flights.\n\nlibrary(nycflights13)\n\nflights |&gt; filter(carrier == \"UA\")\n\n# A tibble: 58,665 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      554            558        -4      740            728\n 4  2013     1     1      558            600        -2      924            917\n 5  2013     1     1      558            600        -2      923            937\n 6  2013     1     1      559            600        -1      854            902\n 7  2013     1     1      607            607         0      858            915\n 8  2013     1     1      611            600        11      945            931\n 9  2013     1     1      623            627        -4      933            932\n10  2013     1     1      628            630        -2     1016            947\n# ℹ 58,655 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can filter based on multiple conditions to subset to the rows that meet all conditions:\n\nflights |&gt; filter(carrier == \"UA\", origin == \"JFK\")\n\n# A tibble: 4,534 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      558            600        -2      924            917\n 2  2013     1     1      611            600        11      945            931\n 3  2013     1     1      803            800         3     1132           1144\n 4  2013     1     1      829            830        -1     1152           1200\n 5  2013     1     1     1112           1100        12     1440           1438\n 6  2013     1     1     1127           1130        -3     1504           1448\n 7  2013     1     1     1422           1425        -3     1748           1759\n 8  2013     1     1     1522           1530        -8     1858           1855\n 9  2013     1     1     1726           1729        -3     2042           2100\n10  2013     1     1     1750           1750         0     2109           2115\n# ℹ 4,524 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can use the | operator to match one of two conditions (“OR” rather than “AND”):\n\n  flights |&gt; filter(carrier == \"UA\" | carrier == \"AA\")\n\n# A tibble: 91,394 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            558        -4      740            728\n 5  2013     1     1      558            600        -2      753            745\n 6  2013     1     1      558            600        -2      924            917\n 7  2013     1     1      558            600        -2      923            937\n 8  2013     1     1      559            600        -1      941            910\n 9  2013     1     1      559            600        -1      854            902\n10  2013     1     1      606            610        -4      858            910\n# ℹ 91,384 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nTo test if a variable is one of several possible values, you can also use the %in% command:\n\nflights |&gt; filter(carrier %in% c(\"UA\", \"AA\"))\n\n# A tibble: 91,394 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            558        -4      740            728\n 5  2013     1     1      558            600        -2      753            745\n 6  2013     1     1      558            600        -2      924            917\n 7  2013     1     1      558            600        -2      923            937\n 8  2013     1     1      559            600        -1      941            910\n 9  2013     1     1      559            600        -1      854            902\n10  2013     1     1      606            610        -4      858            910\n# ℹ 91,384 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIf you want to subset to a set of specific row numbers, you can use the slice function:\n\n## subset to the first 5 rows\nflights |&gt; slice(1:5)\n\n# A tibble: 5 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nHere the 1:5 syntax tells R to produce a vector that starts at 1 and ends at 5, incrementing by 1:\n\n1:5\n\n[1] 1 2 3 4 5\n\n\n\n\nFiltering to the largest/smallest values of a variable\nTo subset to the rows that have the largest or smallest values of a given variable, use the slice_max and slice_max functions. For the largest values, use slice_max and use the n argument to specify how many rows you want:\n\nflights |&gt; slice_max(dep_time, n = 5)\n\n# A tibble: 29 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    10    30     2400           2359         1      327            337\n 2  2013    11    27     2400           2359         1      515            445\n 3  2013    12     5     2400           2359         1      427            440\n 4  2013    12     9     2400           2359         1      432            440\n 5  2013    12     9     2400           2250        70       59           2356\n 6  2013    12    13     2400           2359         1      432            440\n 7  2013    12    19     2400           2359         1      434            440\n 8  2013    12    29     2400           1700       420      302           2025\n 9  2013     2     7     2400           2359         1      432            436\n10  2013     2     7     2400           2359         1      443            444\n# ℹ 19 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nTo get lowest values, use slice_min:\n\nflights |&gt; slice_min(dep_time, n = 5)\n\n# A tibble: 25 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    13        1           2249        72      108           2357\n 2  2013     1    31        1           2100       181      124           2225\n 3  2013    11    13        1           2359         2      442            440\n 4  2013    12    16        1           2359         2      447            437\n 5  2013    12    20        1           2359         2      430            440\n 6  2013    12    26        1           2359         2      437            440\n 7  2013    12    30        1           2359         2      441            437\n 8  2013     2    11        1           2100       181      111           2225\n 9  2013     2    24        1           2245        76      121           2354\n10  2013     3     8        1           2355         6      431            440\n# ℹ 15 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nSorting rows by a variable\nYou can sort the rows of a data set using the arrange() function. By default, this will sort the rows from smallest to largest.\n\nflights |&gt; arrange(dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    13        1           2249        72      108           2357\n 2  2013     1    31        1           2100       181      124           2225\n 3  2013    11    13        1           2359         2      442            440\n 4  2013    12    16        1           2359         2      447            437\n 5  2013    12    20        1           2359         2      430            440\n 6  2013    12    26        1           2359         2      437            440\n 7  2013    12    30        1           2359         2      441            437\n 8  2013     2    11        1           2100       181      111           2225\n 9  2013     2    24        1           2245        76      121           2354\n10  2013     3     8        1           2355         6      431            440\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIf you would like to sort the rows from largest to smallest (descending order), you can wrap the variable name with desc():\n\nflights |&gt; arrange(desc(dep_time))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    10    30     2400           2359         1      327            337\n 2  2013    11    27     2400           2359         1      515            445\n 3  2013    12     5     2400           2359         1      427            440\n 4  2013    12     9     2400           2359         1      432            440\n 5  2013    12     9     2400           2250        70       59           2356\n 6  2013    12    13     2400           2359         1      432            440\n 7  2013    12    19     2400           2359         1      434            440\n 8  2013    12    29     2400           1700       420      302           2025\n 9  2013     2     7     2400           2359         1      432            436\n10  2013     2     7     2400           2359         1      443            444\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nSelecting/subsetting the columns\nYou can subset the data to only certain columns using the select() command:\n\nflights |&gt; select(dep_time, arr_time, dest)\n\n# A tibble: 336,776 × 3\n   dep_time arr_time dest \n      &lt;int&gt;    &lt;int&gt; &lt;chr&gt;\n 1      517      830 IAH  \n 2      533      850 IAH  \n 3      542      923 MIA  \n 4      544     1004 BQN  \n 5      554      812 ATL  \n 6      554      740 ORD  \n 7      555      913 FLL  \n 8      557      709 IAD  \n 9      557      838 MCO  \n10      558      753 ORD  \n# ℹ 336,766 more rows\n\n\nIf you want to select a range of columns from, say, callsign to ideology, you can use the : operator:\n\nflights |&gt; select(dep_time:arr_delay)\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1      517            515         2      830            819        11\n 2      533            529         4      850            830        20\n 3      542            540         2      923            850        33\n 4      544            545        -1     1004           1022       -18\n 5      554            600        -6      812            837       -25\n 6      554            558        -4      740            728        12\n 7      555            600        -5      913            854        19\n 8      557            600        -3      709            723       -14\n 9      557            600        -3      838            846        -8\n10      558            600        -2      753            745         8\n# ℹ 336,766 more rows\n\n\nYou can remove a variable from the data set by using the minus sign - in front of it:\n\nflights |&gt; select(-year)\n\n# A tibble: 336,776 × 18\n   month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1     1     1      517            515         2      830            819\n 2     1     1      533            529         4      850            830\n 3     1     1      542            540         2      923            850\n 4     1     1      544            545        -1     1004           1022\n 5     1     1      554            600        -6      812            837\n 6     1     1      554            558        -4      740            728\n 7     1     1      555            600        -5      913            854\n 8     1     1      557            600        -3      709            723\n 9     1     1      557            600        -3      838            846\n10     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can also drop several variables using the c() function or the (a:b) syntax:\n\nflights |&gt; select(-c(year, month, day))\n\n# A tibble: 336,776 × 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  \n 1      517            515         2      830            819        11 UA     \n 2      533            529         4      850            830        20 UA     \n 3      542            540         2      923            850        33 AA     \n 4      544            545        -1     1004           1022       -18 B6     \n 5      554            600        -6      812            837       -25 DL     \n 6      554            558        -4      740            728        12 UA     \n 7      555            600        -5      913            854        19 B6     \n 8      557            600        -3      709            723       -14 EV     \n 9      557            600        -3      838            846        -8 B6     \n10      558            600        -2      753            745         8 AA     \n# ℹ 336,766 more rows\n# ℹ 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nflights |&gt; select(-(year:day))\n\n# A tibble: 336,776 × 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  \n 1      517            515         2      830            819        11 UA     \n 2      533            529         4      850            830        20 UA     \n 3      542            540         2      923            850        33 AA     \n 4      544            545        -1     1004           1022       -18 B6     \n 5      554            600        -6      812            837       -25 DL     \n 6      554            558        -4      740            728        12 UA     \n 7      555            600        -5      913            854        19 B6     \n 8      557            600        -3      709            723       -14 EV     \n 9      557            600        -3      838            846        -8 B6     \n10      558            600        -2      753            745         8 AA     \n# ℹ 336,766 more rows\n# ℹ 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nYou can also select columns based on matching patterns in the names with functions like starts_with() or ends_with():\n\nflights |&gt; select(ends_with(\"delay\"))\n\n# A tibble: 336,776 × 2\n   dep_delay arr_delay\n       &lt;dbl&gt;     &lt;dbl&gt;\n 1         2        11\n 2         4        20\n 3         2        33\n 4        -1       -18\n 5        -6       -25\n 6        -4        12\n 7        -5        19\n 8        -3       -14\n 9        -3        -8\n10        -2         8\n# ℹ 336,766 more rows\n\n\nThis code finds all variables with column names that end with the string “delay”. See the help page for select() for more information on different ways to select.\n\n\nRenaming a variable\nYou can rename a variable useing the function rename(new_name = old_name):\n\nflights |&gt; rename(flight_number = flight)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight_number &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nCreating new variables\nYou can create new variables that are functions of old variables using the mutate() function:\n\nflights |&gt; mutate(flight_length = arr_time - dep_time) |&gt; select(arr_time, dep_time, flight_length)\n\n# A tibble: 336,776 × 3\n   arr_time dep_time flight_length\n      &lt;int&gt;    &lt;int&gt;         &lt;int&gt;\n 1      830      517           313\n 2      850      533           317\n 3      923      542           381\n 4     1004      544           460\n 5      812      554           258\n 6      740      554           186\n 7      913      555           358\n 8      709      557           152\n 9      838      557           281\n10      753      558           195\n# ℹ 336,766 more rows\n\n\n\n\nCreating new variables based on yes/no conditions\nIf you want to create a new variable that can take on two values based on a logical conditional, you should use the if_else() function inside of mutate(). For instance, if we want to create a more nicely labeled version of the sinclair2017 variable (which is 0/1), we could do:\n\nflights |&gt;\n  mutate(late = if_else(arr_delay &gt; 0,\n                             \"Flight Delayed\",\n                             \"Flight On Time\")) |&gt;\n  select(arr_delay, late)\n\n# A tibble: 336,776 × 2\n   arr_delay late          \n       &lt;dbl&gt; &lt;chr&gt;         \n 1        11 Flight Delayed\n 2        20 Flight Delayed\n 3        33 Flight Delayed\n 4       -18 Flight On Time\n 5       -25 Flight On Time\n 6        12 Flight Delayed\n 7        19 Flight Delayed\n 8       -14 Flight On Time\n 9        -8 Flight On Time\n10         8 Flight Delayed\n# ℹ 336,766 more rows\n\n\n\n\nSummarizing a variable\nYou can calculate summaries of variables in the data set using the summarize() function.\n\nflights |&gt;\n  summarize(\n    avg_dep_time = mean(dep_time),\n    sd_dep_time = sd(dep_time),\n    median_dep_time = median(dep_time)\n  )\n\n# A tibble: 1 × 3\n  avg_dep_time sd_dep_time median_dep_time\n         &lt;dbl&gt;       &lt;dbl&gt;           &lt;int&gt;\n1           NA          NA              NA\n\n\n\n\nSummarizing variables by groups of rows\nBy default, summarize() calculates the summaries of variables for all rows in the data frame. You can also calculate these summaries within groups of rows defined by another variable in the data frame using the group_by() function before summarizing.\n\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    avg_dep_time = mean(dep_time),\n    sd_dep_time = sd(dep_time),\n    median_dep_time = median(dep_time)\n  )\n\n# A tibble: 16 × 4\n   carrier avg_dep_time sd_dep_time median_dep_time\n   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1 9E               NA         NA                NA\n 2 AA               NA         NA                NA\n 3 AS               NA         NA                NA\n 4 B6               NA         NA                NA\n 5 DL               NA         NA                NA\n 6 EV               NA         NA                NA\n 7 F9               NA         NA                NA\n 8 FL               NA         NA                NA\n 9 HA              949.        53.6             954\n10 MQ               NA         NA                NA\n11 OO               NA         NA                NA\n12 UA               NA         NA                NA\n13 US               NA         NA                NA\n14 VX               NA         NA                NA\n15 WN               NA         NA                NA\n16 YV               NA         NA                NA\n\n\nHere, the summarize() function breaks apart the original data into smaller data frames for each carrier and applies the summary functions to those, then combines everything into one tibble.\n\n\nSummarizing by multiple variables\nYou can group by multiple variables and summarize() will create groups based on every combination of each variable:\n\nflights |&gt;\n  group_by(carrier, month) |&gt;\n  summarize(\n    avg_dep_time = mean(dep_time)\n  )\n\n`summarise()` has grouped output by 'carrier'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 185 × 3\n# Groups:   carrier [16]\n   carrier month avg_dep_time\n   &lt;chr&gt;   &lt;int&gt;        &lt;dbl&gt;\n 1 9E          1           NA\n 2 9E          2           NA\n 3 9E          3           NA\n 4 9E          4           NA\n 5 9E          5           NA\n 6 9E          6           NA\n 7 9E          7           NA\n 8 9E          8           NA\n 9 9E          9           NA\n10 9E         10           NA\n# ℹ 175 more rows\n\n\nYou’ll notice the message that summarize() sends after using to let us know that resulting tibble is grouped by carrier. By default, summarize() drops the last group you provided in group_by (month in this case). This isn’t an error message, it’s just letting us know some helpful information. If you want to avoid this messaging displaying, you need to specify what grouping you want after using the .groups argument:\n\nflights |&gt;\n  group_by(carrier, month) |&gt;\n  summarize(\n    avg_dep_time = mean(dep_time),\n    .groups = \"drop_last\"\n  )\n\n# A tibble: 185 × 3\n# Groups:   carrier [16]\n   carrier month avg_dep_time\n   &lt;chr&gt;   &lt;int&gt;        &lt;dbl&gt;\n 1 9E          1           NA\n 2 9E          2           NA\n 3 9E          3           NA\n 4 9E          4           NA\n 5 9E          5           NA\n 6 9E          6           NA\n 7 9E          7           NA\n 8 9E          8           NA\n 9 9E          9           NA\n10 9E         10           NA\n# ℹ 175 more rows\n\n\n\n\nSummarizing across many variables\nIf you want to apply the same summary to multiple variables, you can use the across(vars, fun) function, where vars is a vector of variable names (specified like with select()) and fun is a summary function to apply to those variables.\n\nflights |&gt;\n  group_by(carrier, month) |&gt;\n  summarize(\n    across(c(dep_time, dep_delay), mean)\n  )\n\n`summarise()` has grouped output by 'carrier'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 185 × 4\n# Groups:   carrier [16]\n   carrier month dep_time dep_delay\n   &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 9E          1       NA        NA\n 2 9E          2       NA        NA\n 3 9E          3       NA        NA\n 4 9E          4       NA        NA\n 5 9E          5       NA        NA\n 6 9E          6       NA        NA\n 7 9E          7       NA        NA\n 8 9E          8       NA        NA\n 9 9E          9       NA        NA\n10 9E         10       NA        NA\n# ℹ 175 more rows\n\n\nAs with select(), you can use the : operator to select a range of variables\n\nflights |&gt;\n  group_by(carrier, month) |&gt;\n  summarize(\n    across(dep_time:arr_delay, mean)\n  )\n\n`summarise()` has grouped output by 'carrier'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 185 × 8\n# Groups:   carrier [16]\n   carrier month dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1 9E          1       NA          1485.        NA       NA          1676.\n 2 9E          2       NA          1471.        NA       NA          1661.\n 3 9E          3       NA          1472.        NA       NA          1664.\n 4 9E          4       NA          1502.        NA       NA          1697.\n 5 9E          5       NA          1509.        NA       NA          1712.\n 6 9E          6       NA          1512.        NA       NA          1718.\n 7 9E          7       NA          1493.        NA       NA          1702.\n 8 9E          8       NA          1497.        NA       NA          1706.\n 9 9E          9       NA          1458.        NA       NA          1658.\n10 9E         10       NA          1432.        NA       NA          1632.\n# ℹ 175 more rows\n# ℹ 1 more variable: arr_delay &lt;dbl&gt;\n\n\n\n\nTable of counts of a categorical variable\nThere are two way to produce a table of counts of each category of a variable. The first is to use group_by and summarize along with the summary function n(), which returns the numbers of rows in each grouping (that is, each combination of the grouping variables):\n\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(n = n())\n\n# A tibble: 16 × 2\n   carrier     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 9E      18460\n 2 AA      32729\n 3 AS        714\n 4 B6      54635\n 5 DL      48110\n 6 EV      54173\n 7 F9        685\n 8 FL       3260\n 9 HA        342\n10 MQ      26397\n11 OO         32\n12 UA      58665\n13 US      20536\n14 VX       5162\n15 WN      12275\n16 YV        601\n\n\nA simpler way to acheive the same outcome is to use the count() function, which implements these two steps:\n\nflights |&gt; count(carrier)\n\n# A tibble: 16 × 2\n   carrier     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 9E      18460\n 2 AA      32729\n 3 AS        714\n 4 B6      54635\n 5 DL      48110\n 6 EV      54173\n 7 F9        685\n 8 FL       3260\n 9 HA        342\n10 MQ      26397\n11 OO         32\n12 UA      58665\n13 US      20536\n14 VX       5162\n15 WN      12275\n16 YV        601\n\n\n\n\nProducing nicely formatted tables with kable()\nYou can take any tibble in R and convert it into a more readable output by passing it to knitr::kable(). In our homework, generally, we will save the tibble as an object and then pass it to this function.\n\nmonth_summary &lt;- flights |&gt;\n  group_by(month) |&gt;\n  summarize(\n    avg_arr_delay = mean(arr_delay),\n    sd_arr_delay = sd(arr_delay)\n  )\n\nknitr::kable(month_summary)\n\n\n\n\nmonth\navg_arr_delay\nsd_arr_delay\n\n\n\n\n1\nNA\nNA\n\n\n2\nNA\nNA\n\n\n3\nNA\nNA\n\n\n4\nNA\nNA\n\n\n5\nNA\nNA\n\n\n6\nNA\nNA\n\n\n7\nNA\nNA\n\n\n8\nNA\nNA\n\n\n9\nNA\nNA\n\n\n10\nNA\nNA\n\n\n11\nNA\nNA\n\n\n12\nNA\nNA\n\n\n\n\n\nYou can add informative column names to the table using the col.names argument.\n\nknitr::kable(\n  month_summary,\n  col.names = c(\"Month\", \"Average Delay\", \"SD of Delay\")\n)\n\n\n\n\nMonth\nAverage Delay\nSD of Delay\n\n\n\n\n1\nNA\nNA\n\n\n2\nNA\nNA\n\n\n3\nNA\nNA\n\n\n4\nNA\nNA\n\n\n5\nNA\nNA\n\n\n6\nNA\nNA\n\n\n7\nNA\nNA\n\n\n8\nNA\nNA\n\n\n9\nNA\nNA\n\n\n10\nNA\nNA\n\n\n11\nNA\nNA\n\n\n12\nNA\nNA\n\n\n\n\n\nFinally, we can round the numbers in the table to be a bit nicer using the digits argument. This will tell kable() how many significant digits to show.\n\nknitr::kable(\n  month_summary,\n  col.names = c(\"Month\", \"Average Delay\", \"SD of Delay\"),\n  digits = 3\n)\n\n\n\n\nMonth\nAverage Delay\nSD of Delay\n\n\n\n\n1\nNA\nNA\n\n\n2\nNA\nNA\n\n\n3\nNA\nNA\n\n\n4\nNA\nNA\n\n\n5\nNA\nNA\n\n\n6\nNA\nNA\n\n\n7\nNA\nNA\n\n\n8\nNA\nNA\n\n\n9\nNA\nNA\n\n\n10\nNA\nNA\n\n\n11\nNA\nNA\n\n\n12\nNA\nNA\n\n\n\n\n\n\n\nBarplots of counts\nYou can visualize counts of a variable using a barplot:\n\nflights |&gt;\n  ggplot(mapping = aes(x = carrier)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nBarplots of other summaries\nWe can use barplots to visualize other grouped summaries like means, but we need to use the geom_col() geom instead and specify the variable you want to be the height of the bars. We also want to filter our data so that only values of arr_delay that are greater than zero are considered.\n\nflights |&gt;\n  filter(arr_delay &gt; 0) |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    avg_delay = mean(arr_delay)\n  ) |&gt;\n  ggplot(mapping = aes(x = carrier, y = avg_delay)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nReordering/sorting barplot axes\nOften we want to sort the barplot axes to be in the order of the variable of interest so we can quickly compare them. We can use the fct_reorder(group_var, ordering_var) function to do this where the group_var is the grouping variable that is going on the axes and the ordering_var is the variable that we will sort the groups on.\n\nflights |&gt;\n  filter(arr_delay &gt; 0) |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    avg_delay = mean(arr_delay)\n  ) |&gt;\n  ggplot(mapping = aes(x = fct_reorder(carrier, avg_delay),\n                       y = avg_delay)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nColoring barplots by another variable\nYou can color the barplots by a another variable using the fill aesthetic:\n\nflights |&gt;\n  filter(arr_delay &gt; 0) |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    avg_delay = mean(arr_delay)\n  ) |&gt;\n  slice_max(avg_delay, n = 10) |&gt;\n  ggplot(mapping = aes(y = fct_reorder(carrier, avg_delay),\n                       x = avg_delay)) +\n  geom_col(mapping = aes(fill = carrier))\n\n\n\n\n\n\n\n\n\n\nCreating logical vectors\nYou can create logical variables in your tibbles using mutate:\n\nflights |&gt;\n  mutate(\n    late = arr_delay &gt; 0,\n    fall = month == 9  | month == 10| month == 11,\n    .keep = \"used\"\n)\n\n# A tibble: 336,776 × 4\n   month arr_delay late  fall \n   &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1     1        11 TRUE  FALSE\n 2     1        20 TRUE  FALSE\n 3     1        33 TRUE  FALSE\n 4     1       -18 FALSE FALSE\n 5     1       -25 FALSE FALSE\n 6     1        12 TRUE  FALSE\n 7     1        19 TRUE  FALSE\n 8     1       -14 FALSE FALSE\n 9     1        -8 FALSE FALSE\n10     1         8 TRUE  FALSE\n# ℹ 336,766 more rows\n\n\nThe .keep = \"used\" argument here tells mutate to only return the variables created and any variables used to create them. We’re using it here for display purposes.\nYou can filter based on these logical variables. In particular, if we want to subset to rows where both late and fall were TRUE we could do the following filter:\n\nflights |&gt;\n  mutate(\n    late = arr_delay &gt; 0,\n    fall = month == 9  | month == 10| month == 11,\n    .keep = \"used\"\n  ) |&gt;\n  filter(late & fall)\n\n# A tibble: 26,307 × 4\n   month arr_delay late  fall \n   &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1    10        11 TRUE  TRUE \n 2    10        12 TRUE  TRUE \n 3    10         4 TRUE  TRUE \n 4    10        16 TRUE  TRUE \n 5    10         7 TRUE  TRUE \n 6    10         4 TRUE  TRUE \n 7    10         6 TRUE  TRUE \n 8    10         1 TRUE  TRUE \n 9    10         9 TRUE  TRUE \n10    10        83 TRUE  TRUE \n# ℹ 26,297 more rows\n\n\n\n\nUsing ! to negate logicals\nAny time you place the exclamation point in front of a logical, it will turn any TRUE into a FALSE and vice versa. For instance, if we wanted on-time flights in the fall, we could used\n\nflights |&gt;\n  mutate(\n    late = arr_delay &gt; 0,\n    fall = month == 9  | month == 10| month == 11,\n    .keep = \"used\"\n  ) |&gt;\n  filter(!late & fall)\n\n# A tibble: 56,292 × 4\n   month arr_delay late  fall \n   &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1    10       -34 FALSE TRUE \n 2    10       -22 FALSE TRUE \n 3    10       -46 FALSE TRUE \n 4    10       -26 FALSE TRUE \n 5    10       -16 FALSE TRUE \n 6    10       -20 FALSE TRUE \n 7    10       -23 FALSE TRUE \n 8    10       -12 FALSE TRUE \n 9    10       -10 FALSE TRUE \n10    10        -3 FALSE TRUE \n# ℹ 56,282 more rows\n\n\nOr if we wanted to subset to any combination except late flights and fall, we could negate the AND statement using parentheses:\n\nflights |&gt;\n  mutate(\n    late = arr_delay &gt; 0,\n    fall = month == 9  | month == 10| month == 11,\n    .keep = \"used\"\n  ) |&gt;\n  filter(!(late & fall))\n\n# A tibble: 309,337 × 4\n   month arr_delay late  fall \n   &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1     1        11 TRUE  FALSE\n 2     1        20 TRUE  FALSE\n 3     1        33 TRUE  FALSE\n 4     1       -18 FALSE FALSE\n 5     1       -25 FALSE FALSE\n 6     1        12 TRUE  FALSE\n 7     1        19 TRUE  FALSE\n 8     1       -14 FALSE FALSE\n 9     1        -8 FALSE FALSE\n10     1         8 TRUE  FALSE\n# ℹ 309,327 more rows\n\n\nThis is often used in combination with %in% to acheive a “not in” logical:\n\nflights |&gt;\n  filter(!(carrier %in% c(\"AA\", \"UA\")))\n\n# A tibble: 245,382 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      544            545        -1     1004           1022\n 2  2013     1     1      554            600        -6      812            837\n 3  2013     1     1      555            600        -5      913            854\n 4  2013     1     1      557            600        -3      709            723\n 5  2013     1     1      557            600        -3      838            846\n 6  2013     1     1      558            600        -2      849            851\n 7  2013     1     1      558            600        -2      853            856\n 8  2013     1     1      559            559         0      702            706\n 9  2013     1     1      600            600         0      851            858\n10  2013     1     1      600            600         0      837            825\n# ℹ 245,372 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nGrouped summaries with any() and all()\nOnce you group a tibble, you can summarize logicals within groups using two commands. any() will return TRUE if a logical is TRUE for any row in a group and FALSE otherwise. all() will return TRUE when the logical inside it is TRUE for all rows in a group and FALSE otherwise.\n\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    any_late = any(arr_delay &gt; 0),\n    never_late = all(arr_delay &lt;=0)\n  )\n\n# A tibble: 16 × 3\n   carrier any_late never_late\n   &lt;chr&gt;   &lt;lgl&gt;    &lt;lgl&gt;     \n 1 9E      TRUE     FALSE     \n 2 AA      TRUE     FALSE     \n 3 AS      TRUE     FALSE     \n 4 B6      TRUE     FALSE     \n 5 DL      TRUE     FALSE     \n 6 EV      TRUE     FALSE     \n 7 F9      TRUE     FALSE     \n 8 FL      TRUE     FALSE     \n 9 HA      TRUE     FALSE     \n10 MQ      TRUE     FALSE     \n11 OO      TRUE     FALSE     \n12 UA      TRUE     FALSE     \n13 US      TRUE     FALSE     \n14 VX      TRUE     FALSE     \n15 WN      TRUE     FALSE     \n16 YV      TRUE     FALSE"
  },
  {
    "objectID": "resources/VSCode.html",
    "href": "resources/VSCode.html",
    "title": "Using STATA with Visual Studio Code",
    "section": "",
    "text": "In this tutorial, we’re going to install VS Code and the necessary extensions to get LaTeX, STATA, R, and GitHub Copilot up and running.\nFirst, go here to download VS Code."
  },
  {
    "objectID": "resources/VSCode.html#installing-vs-code",
    "href": "resources/VSCode.html#installing-vs-code",
    "title": "Using STATA with Visual Studio Code",
    "section": "",
    "text": "In this tutorial, we’re going to install VS Code and the necessary extensions to get LaTeX, STATA, R, and GitHub Copilot up and running.\nFirst, go here to download VS Code."
  },
  {
    "objectID": "resources/VSCode.html#installing-extensions",
    "href": "resources/VSCode.html#installing-extensions",
    "title": "Using STATA with Visual Studio Code",
    "section": "Installing Extensions",
    "text": "Installing Extensions\nOnce you’ve downloaded and installed VS Code on your computer, you’ll need to add the necessary extensions to incorporate VS Code into your workflow.\nYou’ll want to install the LaTeX extension if you plan to do any manuscript editing in LaTeX or create any Beamer slides.\n\nWith the LaTeX extension installed, you can edit documents directly in VS Code and push changes to those documents to GitHub/Overleaf (see my VS_Code and GitHub and my Overleaf and GitHub for further instructions).\n\nNext we’ll install two extensions that will allow us to use STATA with VS Code. The key extension is called “stataRun” and actually allows VS Code to run STATA commands:\n\nOnce you install this extension you need to go to the extension settings by clicking on the gear icon:\n\nAnd then make sure the path is correct if you’re using Windows and that the STATA version is correct:\n\nI also recommend installing the “Stata Enhanced” extension that adds colors to your .do file syntax:\n\nThat will make your .do files easier to navigate. For example:\n\nA few helpful STATA shortcuts when using VS Code:\n\nshift+command_a -&gt; runs your entire .do file\nshift+command+s -&gt; runs a highlighted code chunk from your .do file\nshift+command+c -&gt; runs the current line of code in your .do file"
  },
  {
    "objectID": "resources/VSCode.html#github-copilot",
    "href": "resources/VSCode.html#github-copilot",
    "title": "Using STATA with Visual Studio Code",
    "section": "GitHub Copilot",
    "text": "GitHub Copilot\nYou can find instructions for signing up for a free student GitHub Copilot subscription here."
  },
  {
    "objectID": "resources/Stata_tables.html",
    "href": "resources/Stata_tables.html",
    "title": "Making Tables with STATA",
    "section": "",
    "text": "This guide will cover the basics of exporting summary statistics and regression coefficients from STATA to formatted tables in LaTeX and Word. Much of the information in this guide was drawn from Asjad Naqvi’s “The Stata-to-LaTex guide” published on Medium. You can download a STATA .do file that contains all of the code I used to generate the table examples here.\n\n\nSTATA\nTo create LaTeX tables in STATA, you’ll need to install the estout package.\n\nssc install estout, replace\n\nIf you already have the estout package installed, make sure that you’re using the latest version.\nLaTeX\nYou’ll also need some way to compile LaTeX files. Overleaf is probably the most commonly used compiler, though you could also download a compiler, like TeXLive to your computer. I recommend Overleaf because it stores all of your projects online and makes for easy collaboration.\nYou’ll need to add the following packages to your LaTeX preamble in order to compile the tables in this guide. These packages need to be added in addition to any other packages you need to compile your document.\n\nWord\nThere is no setup to do in Word. We’ll export STATA tables to .rtf files and you can open these files directly in Word.\n\n\n\nRun the following STATA code that will load an example data set, tabulate some basic summary statistics, and stores the results in STATA’s e() matrices.\n\nsysuse census, clear\n  foreach i of varlist pop-popurban death-divorce {\n    replace `i' = `i' / 1000\n  }\n\nest clear\nestpost tabstat pop pop65p medage death marriage divorce, c(stat) stat(sum mean sd min max n)\n\nNow we can use the esttab command, which is part of the estout package, to generate a summary statistics table. We’ll start with a basic table and add additional elements as we go.\nLaTeX\n\nesttab using \"~/table_1.tex\", replace cells(\"sum mean sd min max count\") booktabs\n\nWord\n\nesttab using \"~/tables.rtf\", replace /// \ncells(\"sum mean sd min max count\") title(\"Table 1: Summary statistics - Basic\")\n\nYou should replace the “~” with your desired path for file storage. You’ll probably want to set this path at the beginning of your .do file using a global.\nOnce you’ve generated the table_1.tex file, you can upload it into your Overleaf document and add the following code to display the table:\n\nThis code tells LaTeX that we’re placing a table in the document and that the table should use the table_1.tex file for its input. The !htbp command helps with table positioning. The exclamation point overrides some LaTeX defaults and may not be appropriate to use in all cases. If you’re struggling to get the table to display in a proper position in the document, try removing the exclamation point. The \\centering command tells LaTeX to put the table in the center of the page and the \\caption command allows us to add a title to the table. Note that you could also add a title using the title option in STATA’s esttab command. We’ll do this for our Word tables, but when using LaTeX, it’s better to add titles in the LaTeX document itself. Finally, the label command in LaTeX allows us to add a label to the table that we can reference throughout our LaTeX document. It’s always better to refer to something like “Table \\ref{tab:ss_basic}” rather than “Table 1” in your LaTeX document. That way, if the table order changes, the table numbers in your document will update automatically.\nBy default, LaTeX will include the subsection number in the table number, so you’ll get something like “Table 1.1”. To drop the subsection number, you can add the following command to your LaTeX preamble: \\renewcommand{\\thetable}{\\arabic{table}}.\nYou’ll see that this table contains the sum, mean, standard deviation, minimum, maximum, and observation count for six variables in the census data set. This looks pretty good, but there are some elements that aren’t quite right. For example, the table has only one column number for six columns and it contains the number of observations as both a column and row at the bottom. We can clean this up by adding the following options to the esttab command in STATA:\nLaTeX\n\nesttab using \"~/table_2.tex\", replace cells(\"sum mean sd min max count\") /// \nbooktabs nonumber nomtitle nonote noobs compress label\n\nWord\n\nesttab using \"~/tables.rtf\", append /// \ncells(\"sum mean sd min max count\") title(\"Table 2: Summary statistics - Basic, Clean\") /// \nnonumber nomtitle nonote noobs compress label\n\nAdd this new table to your LaTeX document like this:\n\nThe vspace{0.5in} command adds a half inch of blank space between the two tables. It’s not necessary, but makes things look better in this case.\nNote that for LaTeX, we’re creating separate .tex files for Table 1 and Table 2, but for Word, we’re creating one .rtf document called “tables” and appending Table 2 to Table 1. In LaTeX, you’ll want separate files for each table, but that’s not typically something that you’ll want in Word.\nOur table is looking better now. We got rid of the column numbering and the extra observation count. But there are still improvements that we could make. For example, suppose that we don’t need all these decimal places in each column. We can adjust that by formatting the column output in the esttab command as follows. Let’s also clean up some of the column names while we’re at it:\nLaTeX\n\nesttab using \"~/table_3.tex\", replace /// \ncells(\"sum(fmt(%8.0fc)) mean(fmt(%8.2fc)) sd(fmt(%8.2fc)) min max count\") /// \nbooktabs nonumber nomtitle nonote noobs compress label ///\ncollabels(\"Sum\" \"Mean\" \"SD\" \"Min\" \"Max\" \"Obs\")\n\nWord\n\nesttab using \"~/tables.rtf\", append /// \ncells(\"sum(fmt(%8.0fc)) mean(fmt(%8.2fc)) sd(fmt(%8.2fc)) min max count\") /// \ntitle(\"Table 3: Summary statistics - Basic, Formatted\") ///\nnonumber nomtitle nonote noobs compress label ///\ncollabels(\"Sum\" \"Mean\" \"SD\" \"Min\" \"Max\" \"Obs\")\n\nThis looks good, but oftentimes we want summary statistics tables that compare characteristics for a treatment group and a control group. Run the following STATA code that designates certain states in the census data as “treated” states and the others as “control states”.\n\ngen treat=1 if inlist(state, \"Louisiana\", \"Texas\", \"Arkansas\", \"Mississippi\")\nreplace treat=0 if treat==.\n\nNow run the following estpost and modified esttab commands that will generate a treatment vs. control summary statistics table. Note that, for this table, we’ll focus on the mean and the standard deviation of the variables of interest and present the information in the more standard format of estimate over the standard deviation.\n\nest clear\nestpost tabstat pop pop65p medage death marriage divorce, by(treat) c(stat) stat(mean sd) nototal\n\nLaTeX\n\nesttab using \"~/table_4.tex\", replace /// \ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWord\n\nesttab using \"~/tables.rtf\", append /// \ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 4: Summary statistics - Treat vs. Control\") /// \nnonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nOne thing to note here is that we’re using eqlables rather than collabels to add column labels. This is because when we use the by() option in the estpost command, the STATA output is stacked in one long column. We use the unstack option to split the summary statistics by treatment and control groups, and when we do this, STATA treats each column as an “equation”.\nAnother thing to be aware of is that we’re using the sd(par) option to put the standard errors in parentheses under the mean values. When we do this, any formatting that we use for the mean (e.g., %8.2fc) is passed on to the standard deviation unless we explicitly select a different formatting option for the standard deviation.\nNow suppose we want to group our summary statistic variables into categories. In this case, we’ll use two categories: Demographics and Status. To do this, we can modify the esttab code as follows:\nLaTeX\n\nesttab using \"~/table_5.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.1em} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 5: Summary statistics - Treat vs. Control, Grouped\") ///  \nnonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nThe key to adding the group names is the refcat command. The syntax requires us to tell LaTeX or Word the variable name that we want to be directly below the group name (e.g., pop for Demographic). The \\vspace command in LaTeX and the \\line command in Word allows us to control the space between the variable groupings in the table.\nThis looks pretty good, but the spaces between the variables in the table are a little large. We can fix that by replacing the gaps option with the nogaps option and re-running:\nLaTeX\n\nesttab using \"~/table_6.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.1em} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack nogaps ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 6: Summary statistics - Treat vs. Control, Spaced\") ///\nnonumber nomtitle nonote noobs compress label unstack nogaps ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWe’re getting pretty close to where we want to be with this table, but it would probably be helpful to readers if we added the number of observations in the treatment and control groups to the table. To do this, we need to save the number of observations for both the treatment and control groups in locals and include those local values in the stats option for esttab. Run the following STATA code (after the estpost command) and then see below for the esttab modifications:\n\ncount if treat == 1\nestadd local treat_N = r(N)\ncount if treat == 0\nestadd local control_N = r(N)\n\nLaTeX\n\nesttab using \"~/table_7.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.01in} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack nogaps  ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 7: Summary statistics - Treat vs. Control, Obs\") /// \nnonumber nomtitle nonote noobs compress label unstack nogaps ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nThis code counts the number of observations in the treatment group and stores that number in a local called “treat_N”. It does the same thing for the control group and stores the result in a local called “control_N”. We then use the stats option to include the observation values, the layout option to tell LaTeX or Word that we want these observation values to appear in the same row in the table (without this option, LaTeX or Word will put each value in a different row), and the labels option tells LaTeX or Word that we want to label this row “Observations”.\nSometimes it’s helpful to know whether differences in summary statistics across treatment and control groups are statistically different. We can check this by running a series of t-tests and adding the results to the table. This is going to take some extensive modifications to our current code because we’ll need to use the ttest command instead of the tabstat command. This means we’ll have to adjust some of our esttab code as a result. Run the following STATA code and then see below for the esttab modifications:\n\nglobal vars pop pop65p medage death marriage divorce\n\nest clear\nestpost ttest $vars, by(treat)\n\ncount if treat == 1\nestadd local treat_N = r(N)\ncount if treat == 0\nestadd local control_N = r(N)\n\nLaTeX\n\nesttab using \"~/table_8.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.01in} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mu_1 mu_2 p(star)\") /// \nstar(* 0.50 ** 0.20 *** 0.10) ///\nbooktabs nonumber nomtitle nonote noobs compress label nogaps ///\ncollabels(\"Treatment\" \"Control\" \"\\shortstack{p-Value of\\\\Difference}\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mu_1 mu_2 p(star)\") /// \nstar(* 0.50 ** 0.20 *** 0.10) ///\ntitle(\"Table 8: Summary statistics - Treat vs. Control, t-test\") /// \nnonumber nomtitle nonote noobs compress label nogaps ///\ncollabels(\"Treatment\" \"Control\" \"p-Value of Difference\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nFirst, we’re defining a global called vars that includes each of the variables we’ll include in our summary statistics table. We’re then running a t-test on each variable across the treatment and control groups. Next, we add the observation count for the treatment and control groups just like we did before. We then need to modify our esttab code so that the arguments in our cell command reflect the names that ttest uses (which are different than the names tabstat uses). We tell STATA to output two columns of means (one for the treatment group and one for the control group) and the p-value of the difference. The (star) option tells STATA that we want stars to represent statistical significance attached to the p-values. We then need to define our significance levels or else STATA will use the default *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001. Notice that I’m defining some pretty strange significance levels in this example. The reason is that there aren’t any differences in summary statistics between the treatment and control group where p&lt;0.10, so I’m bumping these up so that some stars will appear in the table. The last thing to notice is that we’re using collabels now instead of eqlabels. That’s again due to the change from tabstat to ttest. As a result, I dropped the unstack option, which is no longer needed. I also wanted to label the third column “p-Value of Difference”, but that was too long for the cell size in LaTeX. So I used the \\shortstack command along with the double backslashes to add a line break in the column label.\nFinally, let’s add some footnotes to explain to readers that we’re using some strange values to denote statistical significance.\nLaTeX\nLike table titles, table notes are best added in LaTeX and not through STATA’s esttab command. There are a few ways to do this, but I like to use the threeparttable package in LaTeX. Add the following to your LaTeX preamble:\n\\usepackage[flushleft]{threeparttable}\nAnd then include the following LaTeX code that adds a footnote to the summary statistics table:\n\nWord\nFor Word tables, we can include the footnotes in the esttab command by using the addnotes option:\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mu_1 mu_2 p(star)\") /// \nstar(* 0.50 ** 0.20 *** 0.10) ///\ntitle(\"Table 8: Summary statistics - Treat vs. Control, t-test\") /// \nnonumber nomtitle nonote noobs compress label nogaps ///\ncollabels(\"Treatment\" \"Control\" \"p-Value of Difference\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\")) ///\naddnotes(\"{\\i Notes:} The treatment group includes Louisiana, Texas, Arkansas, and Mississippi. The control group includes all other states.\" \"{\\i *p&lt;0.50, **p&lt;0.20, ***p&lt;0.10}\")\n\nThat should give you most of the flexibility you’ll need to export summary statistics tables directly from STATA to either LaTeX or Word. Now let’s take a look at exporting regression tables.\n\n\n\nRun the following STATA code that will load a data set and make some manipulations to the data:\n\nwebuse nlswork, clear\nxtset idcode year\ngen age2      = age^2\ngen ttl_exp2  = ttl_exp^2\ngen tenure2   = tenure^2\nlab var age      \"Age\"\nlab var age2     \"Age sq.\"\nlab var ttl_exp  \"Work experience\"\nlab var ttl_exp2 \"Work experience sq.\"\nlab var tenure   \"Job tenure\"\nlab var tenure2  \"Job tenure eq.\"\nlab var not_smsa \"SMSA (=1)\"\nlab var south    \"South (=1)\"\nlab var union    \"Union (=1)\"\n\nNo we’ll run a few basic regressions in STATA where the first specification regresses log wages on union status; the second specification adds controls for age, work experience, and job tenure; and the third specification adds controls for living in a rural area and living in the South census region:\n\nest clear\neststo: xtreg ln_w union\neststo: xtreg ln_w union age* ttl_exp* tenure*\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south\n\nWe’re using STATA’s eststo command to store the regression results from each specification. We can use the following code to export a basic table to LaTeX and Word and then we’ll add more elements to the table as we go:\nLaTeX\n\nesttab using \"~/table_9.tex\", replace  ///\nb(3) se(3) star(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle label\n\nThis code should look familiar with one slight change from the previous table code we’ve been running. Here, instead of using the %fmt command to format the table estimates, I’ve used the simpler b(#) and se(#). This tells STATA to export regression coefficients and standard errors at a number of decimal points equal to “#” (i.e., three decimal places in this case).\nYou can use the following LaTeX code to import the table into your document. Notice that I’ve added a title and table notes in LaTeX using the caption command and the threeparttable package.\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nb(3) se(3) star(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 9: Regression Table - Basic\") /// \nnomtitle nonote compress label ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nUnlike in LaTeX, when using Word I add the title and the table notes using the esttab command. The only other difference between the Word and LaTeX code is that I’ve dropped the booktabs option from the Word code since that’s a LaTeX-specific option.\nThe data we’re using for this example come from the NLSY, which is an individual-level panel data set. That means we can add both individual and time (year) fixed effects to our regression. Let’s do that so we can see how to add rows to the bottom of the regression table that indicate our use of fixed effects.\nFirst, we need to re-run the regressions in STATA and create local values that store the fixed effects information.\n\nest clear\neststo: xtreg ln_w union\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure*\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year\nestadd local  TE  \"Yes\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year, fe\nestadd local  TE  \"Yes\"\nestadd local  FE  \"Yes\"\n\nNow we can export the tables and add a row that provides information on which fixed effects we used in each specification.\nLaTeX\n\nesttab using \"~/table_10.tex\", replace   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle nonote compress label ///\nscalars(\"TE Year fixed effects\" \"IE Individual fixed effects\") sfmt(2 0)\n\nWord\n\nesttab using \"~/tables.rtf\", append   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 10: Regression Table - FE\") /// \nnomtitle nonote compress label ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects\") ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nThe esttab code is similar for exporting to LaTeX or Windows - we just need to drop the booktabs option and add the table title and notes when exporting to Word.\nThe table looks pretty good, but it would be nice if we had a row that included the mean of the dependent variable. I’d also like to move the row that contains the number of observations to the bottom of the table. Here’s how we can make those changes. First, run the following code in STATA:\n\nest clear\neststo: xtreg ln_w union\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure*\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"Yes\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year, fe\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"Yes\"\nestadd local  FE  \"Yes\"\n\nHere we’ve added a command to display the mean value of the dependent variable after each regression and store that value as a scalar. Next, run the following code in either LaTeX or Word to output the table:\nLaTeX\n\nesttab using \"~/table_11.tex\", replace   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle nonote noobs compress label ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects\" \"Mean \\vspace{0.01cm} \\\\ Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc)\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 11: Regression Table - FE\") /// \nnomtitle nonote noobs compress label ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects \\line\" \"Mean Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc) ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nNote a couple of important changes here. First, we’ve added the noobs command so that the number of observations isn’t automatically populated in the first row beneath the regression results. We still want the number of observations, but we want it in the bottom row of the table. To get that, we’re including N as a scalar option and placing it at the end of the scalars() list so that it shows up in the bottom row. We’re also adding the sfmt command that controls the formatting of the scalars. Without this command, they will display in the same format as the regression coefficient estimates.\nNow let’s look at an example where we include regression output with different dependent variables in the same table. Suppose we have three dependent variables: log wages, usual weekly hours worked, and average weeks unemployed in the previous year, and we want to estimate a specification with and without time and person fixed effects for each outcome. Run the following STATA code:\n\nest clear\nforeach i of varlist ln_wage hours wks_ue {\n    eststo: xtreg `i' union age* ttl_exp* tenure* not_smsa south\n    qui sum `e(depvar)' if e(sample)\n    estadd scalar Mean= r(mean)\n    estadd local  TE  \"No\"\n    estadd local  FE  \"No\"\n    \n    eststo: xtreg `i' union age* ttl_exp* tenure* not_smsa south i.year, fe\n    qui sum `e(depvar)' if e(sample)\n    estadd scalar Mean= r(mean)\n    estadd local  TE  \"Yes\"\n    estadd local  FE  \"Yes\"\n}\n\nThis is pretty much the same code we ran for the last example table, only here I’ve created a foreach loop that loops through the three dependent variables. We can output the tables in LaTeX and Word as follows:\nLaTeX\n\nesttab using \"~/table_12.tex\", replace   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle nonote noobs compress label ///\nmgroups(\"Ln(Wages)\" \"Hours worked\" \"Weeks unemployed\", pattern(1 0 1 0 1 0) /// \nprefix(\\multicolumn{@span}{c}{) suffix(}) span erepeat(\\cmidrule(lr){@span})) ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects\" \"Mean \\vspace{0.01cm} \\\\ Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc)\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 12: Regression Table - Grouped\") /// \nnomtitle nonote noobs compress label ///\nmgroups(\"Ln(Wages)\" \"Hours worked\" \"Weeks unemployed\", pattern(1 0 1 0 1 0)) /// \nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects \\line\" \"Mean Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc) ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nNote that we’ve added the mgroups option to the output code. This option groups together regressions with the same dependent variable into adjacent columns and adds column titles. We’ve also added some LaTeX formatting language that centers the column titles and adds a solid line under each title. The LaTeX output code works great and gives us exactly what we want. The Word code isn’t quite as good and so the table will require some manipulation when opened in Word (e.g., centering column titles).\nThis should cover most of the tables you’ll need to create. Asjad Naqvi’s “The Stata-to-LaTex guide” has a few other examples for rarely used cases. You can find some additional resources for exporting tables from STATA into LaTeX on Luke Stein’s GitHub repo."
  },
  {
    "objectID": "resources/Stata_tables.html#exporting-stata-tables-to-latex-and-word",
    "href": "resources/Stata_tables.html#exporting-stata-tables-to-latex-and-word",
    "title": "Making Tables with STATA",
    "section": "",
    "text": "This guide will cover the basics of exporting summary statistics and regression coefficients from STATA to formatted tables in LaTeX and Word. Much of the information in this guide was drawn from Asjad Naqvi’s “The Stata-to-LaTex guide” published on Medium. You can download a STATA .do file that contains all of the code I used to generate the table examples here.\n\n\nSTATA\nTo create LaTeX tables in STATA, you’ll need to install the estout package.\n\nssc install estout, replace\n\nIf you already have the estout package installed, make sure that you’re using the latest version.\nLaTeX\nYou’ll also need some way to compile LaTeX files. Overleaf is probably the most commonly used compiler, though you could also download a compiler, like TeXLive to your computer. I recommend Overleaf because it stores all of your projects online and makes for easy collaboration.\nYou’ll need to add the following packages to your LaTeX preamble in order to compile the tables in this guide. These packages need to be added in addition to any other packages you need to compile your document.\n\nWord\nThere is no setup to do in Word. We’ll export STATA tables to .rtf files and you can open these files directly in Word.\n\n\n\nRun the following STATA code that will load an example data set, tabulate some basic summary statistics, and stores the results in STATA’s e() matrices.\n\nsysuse census, clear\n  foreach i of varlist pop-popurban death-divorce {\n    replace `i' = `i' / 1000\n  }\n\nest clear\nestpost tabstat pop pop65p medage death marriage divorce, c(stat) stat(sum mean sd min max n)\n\nNow we can use the esttab command, which is part of the estout package, to generate a summary statistics table. We’ll start with a basic table and add additional elements as we go.\nLaTeX\n\nesttab using \"~/table_1.tex\", replace cells(\"sum mean sd min max count\") booktabs\n\nWord\n\nesttab using \"~/tables.rtf\", replace /// \ncells(\"sum mean sd min max count\") title(\"Table 1: Summary statistics - Basic\")\n\nYou should replace the “~” with your desired path for file storage. You’ll probably want to set this path at the beginning of your .do file using a global.\nOnce you’ve generated the table_1.tex file, you can upload it into your Overleaf document and add the following code to display the table:\n\nThis code tells LaTeX that we’re placing a table in the document and that the table should use the table_1.tex file for its input. The !htbp command helps with table positioning. The exclamation point overrides some LaTeX defaults and may not be appropriate to use in all cases. If you’re struggling to get the table to display in a proper position in the document, try removing the exclamation point. The \\centering command tells LaTeX to put the table in the center of the page and the \\caption command allows us to add a title to the table. Note that you could also add a title using the title option in STATA’s esttab command. We’ll do this for our Word tables, but when using LaTeX, it’s better to add titles in the LaTeX document itself. Finally, the label command in LaTeX allows us to add a label to the table that we can reference throughout our LaTeX document. It’s always better to refer to something like “Table \\ref{tab:ss_basic}” rather than “Table 1” in your LaTeX document. That way, if the table order changes, the table numbers in your document will update automatically.\nBy default, LaTeX will include the subsection number in the table number, so you’ll get something like “Table 1.1”. To drop the subsection number, you can add the following command to your LaTeX preamble: \\renewcommand{\\thetable}{\\arabic{table}}.\nYou’ll see that this table contains the sum, mean, standard deviation, minimum, maximum, and observation count for six variables in the census data set. This looks pretty good, but there are some elements that aren’t quite right. For example, the table has only one column number for six columns and it contains the number of observations as both a column and row at the bottom. We can clean this up by adding the following options to the esttab command in STATA:\nLaTeX\n\nesttab using \"~/table_2.tex\", replace cells(\"sum mean sd min max count\") /// \nbooktabs nonumber nomtitle nonote noobs compress label\n\nWord\n\nesttab using \"~/tables.rtf\", append /// \ncells(\"sum mean sd min max count\") title(\"Table 2: Summary statistics - Basic, Clean\") /// \nnonumber nomtitle nonote noobs compress label\n\nAdd this new table to your LaTeX document like this:\n\nThe vspace{0.5in} command adds a half inch of blank space between the two tables. It’s not necessary, but makes things look better in this case.\nNote that for LaTeX, we’re creating separate .tex files for Table 1 and Table 2, but for Word, we’re creating one .rtf document called “tables” and appending Table 2 to Table 1. In LaTeX, you’ll want separate files for each table, but that’s not typically something that you’ll want in Word.\nOur table is looking better now. We got rid of the column numbering and the extra observation count. But there are still improvements that we could make. For example, suppose that we don’t need all these decimal places in each column. We can adjust that by formatting the column output in the esttab command as follows. Let’s also clean up some of the column names while we’re at it:\nLaTeX\n\nesttab using \"~/table_3.tex\", replace /// \ncells(\"sum(fmt(%8.0fc)) mean(fmt(%8.2fc)) sd(fmt(%8.2fc)) min max count\") /// \nbooktabs nonumber nomtitle nonote noobs compress label ///\ncollabels(\"Sum\" \"Mean\" \"SD\" \"Min\" \"Max\" \"Obs\")\n\nWord\n\nesttab using \"~/tables.rtf\", append /// \ncells(\"sum(fmt(%8.0fc)) mean(fmt(%8.2fc)) sd(fmt(%8.2fc)) min max count\") /// \ntitle(\"Table 3: Summary statistics - Basic, Formatted\") ///\nnonumber nomtitle nonote noobs compress label ///\ncollabels(\"Sum\" \"Mean\" \"SD\" \"Min\" \"Max\" \"Obs\")\n\nThis looks good, but oftentimes we want summary statistics tables that compare characteristics for a treatment group and a control group. Run the following STATA code that designates certain states in the census data as “treated” states and the others as “control states”.\n\ngen treat=1 if inlist(state, \"Louisiana\", \"Texas\", \"Arkansas\", \"Mississippi\")\nreplace treat=0 if treat==.\n\nNow run the following estpost and modified esttab commands that will generate a treatment vs. control summary statistics table. Note that, for this table, we’ll focus on the mean and the standard deviation of the variables of interest and present the information in the more standard format of estimate over the standard deviation.\n\nest clear\nestpost tabstat pop pop65p medage death marriage divorce, by(treat) c(stat) stat(mean sd) nototal\n\nLaTeX\n\nesttab using \"~/table_4.tex\", replace /// \ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWord\n\nesttab using \"~/tables.rtf\", append /// \ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 4: Summary statistics - Treat vs. Control\") /// \nnonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nOne thing to note here is that we’re using eqlables rather than collabels to add column labels. This is because when we use the by() option in the estpost command, the STATA output is stacked in one long column. We use the unstack option to split the summary statistics by treatment and control groups, and when we do this, STATA treats each column as an “equation”.\nAnother thing to be aware of is that we’re using the sd(par) option to put the standard errors in parentheses under the mean values. When we do this, any formatting that we use for the mean (e.g., %8.2fc) is passed on to the standard deviation unless we explicitly select a different formatting option for the standard deviation.\nNow suppose we want to group our summary statistic variables into categories. In this case, we’ll use two categories: Demographics and Status. To do this, we can modify the esttab code as follows:\nLaTeX\n\nesttab using \"~/table_5.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.1em} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 5: Summary statistics - Treat vs. Control, Grouped\") ///  \nnonumber nomtitle nonote noobs compress label unstack gap ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nThe key to adding the group names is the refcat command. The syntax requires us to tell LaTeX or Word the variable name that we want to be directly below the group name (e.g., pop for Demographic). The \\vspace command in LaTeX and the \\line command in Word allows us to control the space between the variable groupings in the table.\nThis looks pretty good, but the spaces between the variables in the table are a little large. We can fix that by replacing the gaps option with the nogaps option and re-running:\nLaTeX\n\nesttab using \"~/table_6.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.1em} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack nogaps ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 6: Summary statistics - Treat vs. Control, Spaced\") ///\nnonumber nomtitle nonote noobs compress label unstack nogaps ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\")\n\nWe’re getting pretty close to where we want to be with this table, but it would probably be helpful to readers if we added the number of observations in the treatment and control groups to the table. To do this, we need to save the number of observations for both the treatment and control groups in locals and include those local values in the stats option for esttab. Run the following STATA code (after the estpost command) and then see below for the esttab modifications:\n\ncount if treat == 1\nestadd local treat_N = r(N)\ncount if treat == 0\nestadd local control_N = r(N)\n\nLaTeX\n\nesttab using \"~/table_7.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.01in} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") /// \nbooktabs nonumber nomtitle nonote noobs compress label unstack nogaps  ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mean(fmt(%8.2fc))\" \"sd(par)\") ///\ntitle(\"Table 7: Summary statistics - Treat vs. Control, Obs\") /// \nnonumber nomtitle nonote noobs compress label unstack nogaps ///\ncollabels(none) eqlabels(\"Treatment\" \"Control\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nThis code counts the number of observations in the treatment group and stores that number in a local called “treat_N”. It does the same thing for the control group and stores the result in a local called “control_N”. We then use the stats option to include the observation values, the layout option to tell LaTeX or Word that we want these observation values to appear in the same row in the table (without this option, LaTeX or Word will put each value in a different row), and the labels option tells LaTeX or Word that we want to label this row “Observations”.\nSometimes it’s helpful to know whether differences in summary statistics across treatment and control groups are statistically different. We can check this by running a series of t-tests and adding the results to the table. This is going to take some extensive modifications to our current code because we’ll need to use the ttest command instead of the tabstat command. This means we’ll have to adjust some of our esttab code as a result. Run the following STATA code and then see below for the esttab modifications:\n\nglobal vars pop pop65p medage death marriage divorce\n\nest clear\nestpost ttest $vars, by(treat)\n\ncount if treat == 1\nestadd local treat_N = r(N)\ncount if treat == 0\nestadd local control_N = r(N)\n\nLaTeX\n\nesttab using \"~/table_8.tex\", replace ///\nrefcat(pop \"\\emph{Demographic}\" death \"\\vspace{0.01in} \\\\ \\emph{Status}\", nolabel) ///\ncells(\"mu_1 mu_2 p(star)\") /// \nstar(* 0.50 ** 0.20 *** 0.10) ///\nbooktabs nonumber nomtitle nonote noobs compress label nogaps ///\ncollabels(\"Treatment\" \"Control\" \"\\shortstack{p-Value of\\\\Difference}\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mu_1 mu_2 p(star)\") /// \nstar(* 0.50 ** 0.20 *** 0.10) ///\ntitle(\"Table 8: Summary statistics - Treat vs. Control, t-test\") /// \nnonumber nomtitle nonote noobs compress label nogaps ///\ncollabels(\"Treatment\" \"Control\" \"p-Value of Difference\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\"))\n\nFirst, we’re defining a global called vars that includes each of the variables we’ll include in our summary statistics table. We’re then running a t-test on each variable across the treatment and control groups. Next, we add the observation count for the treatment and control groups just like we did before. We then need to modify our esttab code so that the arguments in our cell command reflect the names that ttest uses (which are different than the names tabstat uses). We tell STATA to output two columns of means (one for the treatment group and one for the control group) and the p-value of the difference. The (star) option tells STATA that we want stars to represent statistical significance attached to the p-values. We then need to define our significance levels or else STATA will use the default *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001. Notice that I’m defining some pretty strange significance levels in this example. The reason is that there aren’t any differences in summary statistics between the treatment and control group where p&lt;0.10, so I’m bumping these up so that some stars will appear in the table. The last thing to notice is that we’re using collabels now instead of eqlabels. That’s again due to the change from tabstat to ttest. As a result, I dropped the unstack option, which is no longer needed. I also wanted to label the third column “p-Value of Difference”, but that was too long for the cell size in LaTeX. So I used the \\shortstack command along with the double backslashes to add a line break in the column label.\nFinally, let’s add some footnotes to explain to readers that we’re using some strange values to denote statistical significance.\nLaTeX\nLike table titles, table notes are best added in LaTeX and not through STATA’s esttab command. There are a few ways to do this, but I like to use the threeparttable package in LaTeX. Add the following to your LaTeX preamble:\n\\usepackage[flushleft]{threeparttable}\nAnd then include the following LaTeX code that adds a footnote to the summary statistics table:\n\nWord\nFor Word tables, we can include the footnotes in the esttab command by using the addnotes option:\n\nesttab using \"~/tables.rtf\", append ///\nrefcat(pop \"{\\i Demographic}\" death \"\\line {\\i Status}\", nolabel) ///\ncells(\"mu_1 mu_2 p(star)\") /// \nstar(* 0.50 ** 0.20 *** 0.10) ///\ntitle(\"Table 8: Summary statistics - Treat vs. Control, t-test\") /// \nnonumber nomtitle nonote noobs compress label nogaps ///\ncollabels(\"Treatment\" \"Control\" \"p-Value of Difference\") /// \nstats(treat_N control_N, layout(\"@ @\") labels(\"Observations\")) ///\naddnotes(\"{\\i Notes:} The treatment group includes Louisiana, Texas, Arkansas, and Mississippi. The control group includes all other states.\" \"{\\i *p&lt;0.50, **p&lt;0.20, ***p&lt;0.10}\")\n\nThat should give you most of the flexibility you’ll need to export summary statistics tables directly from STATA to either LaTeX or Word. Now let’s take a look at exporting regression tables.\n\n\n\nRun the following STATA code that will load a data set and make some manipulations to the data:\n\nwebuse nlswork, clear\nxtset idcode year\ngen age2      = age^2\ngen ttl_exp2  = ttl_exp^2\ngen tenure2   = tenure^2\nlab var age      \"Age\"\nlab var age2     \"Age sq.\"\nlab var ttl_exp  \"Work experience\"\nlab var ttl_exp2 \"Work experience sq.\"\nlab var tenure   \"Job tenure\"\nlab var tenure2  \"Job tenure eq.\"\nlab var not_smsa \"SMSA (=1)\"\nlab var south    \"South (=1)\"\nlab var union    \"Union (=1)\"\n\nNo we’ll run a few basic regressions in STATA where the first specification regresses log wages on union status; the second specification adds controls for age, work experience, and job tenure; and the third specification adds controls for living in a rural area and living in the South census region:\n\nest clear\neststo: xtreg ln_w union\neststo: xtreg ln_w union age* ttl_exp* tenure*\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south\n\nWe’re using STATA’s eststo command to store the regression results from each specification. We can use the following code to export a basic table to LaTeX and Word and then we’ll add more elements to the table as we go:\nLaTeX\n\nesttab using \"~/table_9.tex\", replace  ///\nb(3) se(3) star(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle label\n\nThis code should look familiar with one slight change from the previous table code we’ve been running. Here, instead of using the %fmt command to format the table estimates, I’ve used the simpler b(#) and se(#). This tells STATA to export regression coefficients and standard errors at a number of decimal points equal to “#” (i.e., three decimal places in this case).\nYou can use the following LaTeX code to import the table into your document. Notice that I’ve added a title and table notes in LaTeX using the caption command and the threeparttable package.\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nb(3) se(3) star(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 9: Regression Table - Basic\") /// \nnomtitle nonote compress label ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nUnlike in LaTeX, when using Word I add the title and the table notes using the esttab command. The only other difference between the Word and LaTeX code is that I’ve dropped the booktabs option from the Word code since that’s a LaTeX-specific option.\nThe data we’re using for this example come from the NLSY, which is an individual-level panel data set. That means we can add both individual and time (year) fixed effects to our regression. Let’s do that so we can see how to add rows to the bottom of the regression table that indicate our use of fixed effects.\nFirst, we need to re-run the regressions in STATA and create local values that store the fixed effects information.\n\nest clear\neststo: xtreg ln_w union\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure*\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year\nestadd local  TE  \"Yes\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year, fe\nestadd local  TE  \"Yes\"\nestadd local  FE  \"Yes\"\n\nNow we can export the tables and add a row that provides information on which fixed effects we used in each specification.\nLaTeX\n\nesttab using \"~/table_10.tex\", replace   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle nonote compress label ///\nscalars(\"TE Year fixed effects\" \"IE Individual fixed effects\") sfmt(2 0)\n\nWord\n\nesttab using \"~/tables.rtf\", append   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 10: Regression Table - FE\") /// \nnomtitle nonote compress label ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects\") ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nThe esttab code is similar for exporting to LaTeX or Windows - we just need to drop the booktabs option and add the table title and notes when exporting to Word.\nThe table looks pretty good, but it would be nice if we had a row that included the mean of the dependent variable. I’d also like to move the row that contains the number of observations to the bottom of the table. Here’s how we can make those changes. First, run the following code in STATA:\n\nest clear\neststo: xtreg ln_w union\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure*\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"No\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"Yes\"\nestadd local  FE  \"No\"\n\neststo: xtreg ln_w union age* ttl_exp* tenure* not_smsa south i.year, fe\nqui sum `e(depvar)' if e(sample)\nestadd scalar Mean= r(mean)\nestadd local  TE  \"Yes\"\nestadd local  FE  \"Yes\"\n\nHere we’ve added a command to display the mean value of the dependent variable after each regression and store that value as a scalar. Next, run the following code in either LaTeX or Word to output the table:\nLaTeX\n\nesttab using \"~/table_11.tex\", replace   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle nonote noobs compress label ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects\" \"Mean \\vspace{0.01cm} \\\\ Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc)\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 11: Regression Table - FE\") /// \nnomtitle nonote noobs compress label ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects \\line\" \"Mean Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc) ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nNote a couple of important changes here. First, we’ve added the noobs command so that the number of observations isn’t automatically populated in the first row beneath the regression results. We still want the number of observations, but we want it in the bottom row of the table. To get that, we’re including N as a scalar option and placing it at the end of the scalars() list so that it shows up in the bottom row. We’re also adding the sfmt command that controls the formatting of the scalars. Without this command, they will display in the same format as the regression coefficient estimates.\nNow let’s look at an example where we include regression output with different dependent variables in the same table. Suppose we have three dependent variables: log wages, usual weekly hours worked, and average weeks unemployed in the previous year, and we want to estimate a specification with and without time and person fixed effects for each outcome. Run the following STATA code:\n\nest clear\nforeach i of varlist ln_wage hours wks_ue {\n    eststo: xtreg `i' union age* ttl_exp* tenure* not_smsa south\n    qui sum `e(depvar)' if e(sample)\n    estadd scalar Mean= r(mean)\n    estadd local  TE  \"No\"\n    estadd local  FE  \"No\"\n    \n    eststo: xtreg `i' union age* ttl_exp* tenure* not_smsa south i.year, fe\n    qui sum `e(depvar)' if e(sample)\n    estadd scalar Mean= r(mean)\n    estadd local  TE  \"Yes\"\n    estadd local  FE  \"Yes\"\n}\n\nThis is pretty much the same code we ran for the last example table, only here I’ve created a foreach loop that loops through the three dependent variables. We can output the tables in LaTeX and Word as follows:\nLaTeX\n\nesttab using \"~/table_12.tex\", replace   ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\nbooktabs nomtitle nonote noobs compress label ///\nmgroups(\"Ln(Wages)\" \"Hours worked\" \"Weeks unemployed\", pattern(1 0 1 0 1 0) /// \nprefix(\\multicolumn{@span}{c}{) suffix(}) span erepeat(\\cmidrule(lr){@span})) ///\nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects\" \"Mean \\vspace{0.01cm} \\\\ Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc)\n\nWord\n\nesttab using \"~/tables.rtf\", append ///\nb(3) se(3) keep(union age* ttl_exp* tenure* not_smsa south) ///\nstar(* 0.10 ** 0.05 *** 0.01) ///\ntitle(\"Table 12: Regression Table - Grouped\") /// \nnomtitle nonote noobs compress label ///\nmgroups(\"Ln(Wages)\" \"Hours worked\" \"Weeks unemployed\", pattern(1 0 1 0 1 0)) /// \nscalars(\"TE Year fixed effects\" \"FE Individual fixed effects \\line\" \"Mean Dep. Var. Mean\" \"N Observations\") sfmt(0 0 2 %8.0fc) ///\naddnotes(\"{\\i Notes:} This is an example of a regression table where we estimate three different specifications adding controls as we go.\" \"{\\i *p&lt;0.10, **p&lt;0.05, ***p&lt;0.01}\")\n\nNote that we’ve added the mgroups option to the output code. This option groups together regressions with the same dependent variable into adjacent columns and adds column titles. We’ve also added some LaTeX formatting language that centers the column titles and adds a solid line under each title. The LaTeX output code works great and gives us exactly what we want. The Word code isn’t quite as good and so the table will require some manipulation when opened in Word (e.g., centering column titles).\nThis should cover most of the tables you’ll need to create. Asjad Naqvi’s “The Stata-to-LaTex guide” has a few other examples for rarely used cases. You can find some additional resources for exporting tables from STATA into LaTeX on Luke Stein’s GitHub repo."
  },
  {
    "objectID": "resources/Overleaf_Git.html",
    "href": "resources/Overleaf_Git.html",
    "title": "Using GitHub with Overleaf",
    "section": "",
    "text": "This guide will cover the basics of linking an Overleaf project to a GitHub repository. Before beginning this tutorial, you’ll need to install Git on your computer and sign up for a GitHub account (see this guide for instructions). You’ll also need to have a premium Overleaf account- unfortunately, free Overleaf does not support GitHub synchronization.\n\n\nAfter installing the software on your computer, go to the “Account Settings” tab in your Overleaf page and scroll down to “Integrations”. You’ll want to select the Link button under “GitHub Sync” (another option is Git Integration, but that’s not what we want to do here).\n\nNext, you’ll need to create a GitHub repo for your project. If this is your first time creating a GitHub repo, you can follow this guide.\nOnce you’ve created the repo, you need to link the repo to an Overleaf project. To do so, go to the “New Project” tab and select “Import from GitHub”.\n\nThis should bring up a list of your active GitHub repos and you’ll want to choose the repo that is associated with the Overleaf project by selecting “Import to Overleaf”.\n\nNow that you’ve linked your GitHub repo and Overleaf project, you can make edits to your Overleaf document and push those edits to GitHub. However, I would suggest not editing your documents directly in Overleaf. I like to maintain a single IDE for running code, editing manuscripts, creating slides, etc. Instead of running your code in R or STATA, uploading .tex files into Overleaf, and editing your manuscript or slides on Overleaf (which requires an active internet connection), you can do all that in one place with either VS Code (my preferred IDE) or RStudio.\nHere’s my suggested workflow if you’re using VS Code, GitHub, and Overleaf in tandem:\n\n\nLink your VS Code project folder to a GitHub repo (see here for instructions).\n\n\nGo to Overleaf and follow the steps listed above.\n\n\nEdit your .tex files in VS Code and push changes to your GitHub repo.\n\n\nPull those changes into Overleaf using the following steps…\n\n\nGo to the “Menu” tab in your Overleaf project:\n\nThen select “GitHub”:\n\nThat’ll bring up a pop-up menu where you can select “Pull GitHub changes into Overleaf”:\n\nBe careful here. You do NOT want to “Push Overleaf changes to GitHub” because you’re not making any changes directly in Overleaf. Be sure to select the Pull option and not the Push option.\nOnce you’ve pulled the changes into Overleaf, you should get a confirmation message that looks like this:\n\nThe reason I like this workflow is because you have everything in one project folder/GitHub repo and can do all of your data work, document editing, and slide creation in one place. You can then push updates to Overleaf so that you can access your manuscript files or Beamer slides in Overleaf if you’d like (primarily useful for collaboration purposes)."
  },
  {
    "objectID": "resources/Overleaf_Git.html#overleaf-and-github",
    "href": "resources/Overleaf_Git.html#overleaf-and-github",
    "title": "Using GitHub with Overleaf",
    "section": "",
    "text": "This guide will cover the basics of linking an Overleaf project to a GitHub repository. Before beginning this tutorial, you’ll need to install Git on your computer and sign up for a GitHub account (see this guide for instructions). You’ll also need to have a premium Overleaf account- unfortunately, free Overleaf does not support GitHub synchronization.\n\n\nAfter installing the software on your computer, go to the “Account Settings” tab in your Overleaf page and scroll down to “Integrations”. You’ll want to select the Link button under “GitHub Sync” (another option is Git Integration, but that’s not what we want to do here).\n\nNext, you’ll need to create a GitHub repo for your project. If this is your first time creating a GitHub repo, you can follow this guide.\nOnce you’ve created the repo, you need to link the repo to an Overleaf project. To do so, go to the “New Project” tab and select “Import from GitHub”.\n\nThis should bring up a list of your active GitHub repos and you’ll want to choose the repo that is associated with the Overleaf project by selecting “Import to Overleaf”.\n\nNow that you’ve linked your GitHub repo and Overleaf project, you can make edits to your Overleaf document and push those edits to GitHub. However, I would suggest not editing your documents directly in Overleaf. I like to maintain a single IDE for running code, editing manuscripts, creating slides, etc. Instead of running your code in R or STATA, uploading .tex files into Overleaf, and editing your manuscript or slides on Overleaf (which requires an active internet connection), you can do all that in one place with either VS Code (my preferred IDE) or RStudio.\nHere’s my suggested workflow if you’re using VS Code, GitHub, and Overleaf in tandem:\n\n\nLink your VS Code project folder to a GitHub repo (see here for instructions).\n\n\nGo to Overleaf and follow the steps listed above.\n\n\nEdit your .tex files in VS Code and push changes to your GitHub repo.\n\n\nPull those changes into Overleaf using the following steps…\n\n\nGo to the “Menu” tab in your Overleaf project:\n\nThen select “GitHub”:\n\nThat’ll bring up a pop-up menu where you can select “Pull GitHub changes into Overleaf”:\n\nBe careful here. You do NOT want to “Push Overleaf changes to GitHub” because you’re not making any changes directly in Overleaf. Be sure to select the Pull option and not the Push option.\nOnce you’ve pulled the changes into Overleaf, you should get a confirmation message that looks like this:\n\nThe reason I like this workflow is because you have everything in one project folder/GitHub repo and can do all of your data work, document editing, and slide creation in one place. You can then push updates to Overleaf so that you can access your manuscript files or Beamer slides in Overleaf if you’d like (primarily useful for collaboration purposes)."
  },
  {
    "objectID": "materials/16_case_study.html",
    "href": "materials/16_case_study.html",
    "title": "HPAM 7660 - Week 16 Materials",
    "section": "",
    "text": "Zoom Link\nCase Study\n\nThe Road Ahead for Medicare Advantage: Strategies for Sustainable Growth\n\nCase Teams\n\nTeam 1, 12:00pm to 12:30pm: Black, Clements, Zuhaira\nTeam 2, 12:30pm to 1:00pm: English, Howland, Johnson, LaBattes\nTeam 3, 1:15pm to 1:45pm: Letsinger, Shoemaker, Varady, Waugh\nTeam 4, 1:45pm to 2:15pm: Assefa, Cohen, Shah, Shimanovsky\nTeam 5, 2:30pm to 3:00pm: Brennan, Lindgren, Schindel\nTeam 6, 3:00pm to 3:30pm: Arkfeld, Pachpor, Shiferaw, Toole\n\nCase Judges\n\nJennifer Friedman, Senior Vice President Federal Health Policy Strategies, former Counselor to the Secretary of the U.S. Department of Health & Human Services and Deputy Staff Director of the Ways and Means Health Subcommittee in the U.S House of Representatives.\nRyann Hill, Vice President for Government Relations, Federation of American Hospitals, former Head of Public, Government, and Community Affairs for SCAN Health Plans.\n\nCase Rubric\n\nJudges’ Rubric\n\nPeer Assessment Form and Submission Link\n-Peer Assessment Form\n-Submit Here"
  },
  {
    "objectID": "materials/16_case_study.html#tuesday-april-30",
    "href": "materials/16_case_study.html#tuesday-april-30",
    "title": "HPAM 7660 - Week 16 Materials",
    "section": "",
    "text": "Zoom Link\nCase Study\n\nThe Road Ahead for Medicare Advantage: Strategies for Sustainable Growth\n\nCase Teams\n\nTeam 1, 12:00pm to 12:30pm: Black, Clements, Zuhaira\nTeam 2, 12:30pm to 1:00pm: English, Howland, Johnson, LaBattes\nTeam 3, 1:15pm to 1:45pm: Letsinger, Shoemaker, Varady, Waugh\nTeam 4, 1:45pm to 2:15pm: Assefa, Cohen, Shah, Shimanovsky\nTeam 5, 2:30pm to 3:00pm: Brennan, Lindgren, Schindel\nTeam 6, 3:00pm to 3:30pm: Arkfeld, Pachpor, Shiferaw, Toole\n\nCase Judges\n\nJennifer Friedman, Senior Vice President Federal Health Policy Strategies, former Counselor to the Secretary of the U.S. Department of Health & Human Services and Deputy Staff Director of the Ways and Means Health Subcommittee in the U.S House of Representatives.\nRyann Hill, Vice President for Government Relations, Federation of American Hospitals, former Head of Public, Government, and Community Affairs for SCAN Health Plans.\n\nCase Rubric\n\nJudges’ Rubric\n\nPeer Assessment Form and Submission Link\n-Peer Assessment Form\n-Submit Here"
  },
  {
    "objectID": "materials/14_practice_case.html",
    "href": "materials/14_practice_case.html",
    "title": "HPAM 7660 - Week 14 Materials",
    "section": "",
    "text": "Reading - Practice Case Study"
  },
  {
    "objectID": "materials/14_practice_case.html#tuesday-april-16",
    "href": "materials/14_practice_case.html#tuesday-april-16",
    "title": "HPAM 7660 - Week 14 Materials",
    "section": "",
    "text": "Reading - Practice Case Study"
  },
  {
    "objectID": "materials/10_natural_experiment.html",
    "href": "materials/10_natural_experiment.html",
    "title": "HPAM 7660 - Week 10 Materials",
    "section": "",
    "text": "Readings - Khullar and Jena (2021) - Natural Experiments in Health Care Research and Allen et al. (2013) - The Oregon Health Insurance Experiment: When Limited Resources Provide Research Opportunities\nAudio - Austin Frakt on Medicaid and the Oregon Medicaid Study and Jim Manzi on the Oregon Medicaid Study, Experimental Evidence, and Causality\nAssignment - Reading/Audio Discussion Questions\nSlides - Natural Experiments & The Oregon Medicaid Study"
  },
  {
    "objectID": "materials/10_natural_experiment.html#tuesday-march-19",
    "href": "materials/10_natural_experiment.html#tuesday-march-19",
    "title": "HPAM 7660 - Week 10 Materials",
    "section": "",
    "text": "Readings - Khullar and Jena (2021) - Natural Experiments in Health Care Research and Allen et al. (2013) - The Oregon Health Insurance Experiment: When Limited Resources Provide Research Opportunities\nAudio - Austin Frakt on Medicaid and the Oregon Medicaid Study and Jim Manzi on the Oregon Medicaid Study, Experimental Evidence, and Causality\nAssignment - Reading/Audio Discussion Questions\nSlides - Natural Experiments & The Oregon Medicaid Study"
  },
  {
    "objectID": "materials/10_natural_experiment.html#thursday-march-7",
    "href": "materials/10_natural_experiment.html#thursday-march-7",
    "title": "HPAM 7660 - Week 10 Materials",
    "section": "Thursday, March 7",
    "text": "Thursday, March 7\n\nAssignment - Policy Memo Introduction (See here for an example. You can also find a link to this document on the Resources page.)"
  },
  {
    "objectID": "materials/08_visualization.html",
    "href": "materials/08_visualization.html",
    "title": "HPAM 7660 - Week 8 Materials",
    "section": "",
    "text": "Reading - ModernDive Chapter 2 - Data Visualization\nAssignment - Data Assignment 4\nTutorial - Tutorial 7 - Summarizing Data Tutorial - Parish Cancer Rates by Race and Cancer Site"
  },
  {
    "objectID": "materials/08_visualization.html#tuesday-march-5",
    "href": "materials/08_visualization.html#tuesday-march-5",
    "title": "HPAM 7660 - Week 8 Materials",
    "section": "",
    "text": "Reading - ModernDive Chapter 2 - Data Visualization\nAssignment - Data Assignment 4\nTutorial - Tutorial 7 - Summarizing Data Tutorial - Parish Cancer Rates by Race and Cancer Site"
  },
  {
    "objectID": "materials/08_visualization.html#thursday-march-7",
    "href": "materials/08_visualization.html#thursday-march-7",
    "title": "HPAM 7660 - Week 8 Materials",
    "section": "Thursday, March 7",
    "text": "Thursday, March 7\n\nTutorial - Tutorial 8 - Data Visualization Code"
  },
  {
    "objectID": "materials/06_wrangling.html",
    "href": "materials/06_wrangling.html",
    "title": "HPAM 7660 - Week 6 Materials",
    "section": "",
    "text": "Reading - ModernDive Chapter 3 through 3.4 - Data Wrangling (Note that you only need to read through section 3.4 this week.)\nAssignment - Data Assignment 2\nTutorial - Tutorial 4 - Data Description Tutorial"
  },
  {
    "objectID": "materials/06_wrangling.html#tuesday-february-20",
    "href": "materials/06_wrangling.html#tuesday-february-20",
    "title": "HPAM 7660 - Week 6 Materials",
    "section": "",
    "text": "Reading - ModernDive Chapter 3 through 3.4 - Data Wrangling (Note that you only need to read through section 3.4 this week.)\nAssignment - Data Assignment 2\nTutorial - Tutorial 4 - Data Description Tutorial"
  },
  {
    "objectID": "materials/06_wrangling.html#thursday-february-22",
    "href": "materials/06_wrangling.html#thursday-february-22",
    "title": "HPAM 7660 - Week 6 Materials",
    "section": "Thursday, February 22",
    "text": "Thursday, February 22\n\nTutorial - Tutorial 5 - Data Wrangling in RStudio"
  },
  {
    "objectID": "materials/04_import.html",
    "href": "materials/04_import.html",
    "title": "HPAM 7660 - Week 4 Materials",
    "section": "",
    "text": "Reading - ModernDive Chapter 4 - Data Importing and Tidy Data\nAssignment - Data Assignment 1"
  },
  {
    "objectID": "materials/04_import.html#tuesday-february-6",
    "href": "materials/04_import.html#tuesday-february-6",
    "title": "HPAM 7660 - Week 4 Materials",
    "section": "",
    "text": "Reading - ModernDive Chapter 4 - Data Importing and Tidy Data\nAssignment - Data Assignment 1"
  },
  {
    "objectID": "materials/04_import.html#thursday-february-8",
    "href": "materials/04_import.html#thursday-february-8",
    "title": "HPAM 7660 - Week 4 Materials",
    "section": "Thursday, February 8",
    "text": "Thursday, February 8\n\nTutorial - Tutorial 3 - Data Importing in RStudio"
  },
  {
    "objectID": "materials/02_overview.html",
    "href": "materials/02_overview.html",
    "title": "HPAM 7660 - Week 2 Materials",
    "section": "",
    "text": "Intro Scenario Results, Data, R Script\nReading - Fedorowicz and Aron (2021) – Improving Evidence-Based Policymaking\nReading - NYT Opinion Piece on Chevron (non-required reading for those interested)\nAssignment - Reading Discussion Questions\nSlides - Policy Analysis Overview"
  },
  {
    "objectID": "materials/02_overview.html#tuesday-january-23",
    "href": "materials/02_overview.html#tuesday-january-23",
    "title": "HPAM 7660 - Week 2 Materials",
    "section": "",
    "text": "Intro Scenario Results, Data, R Script\nReading - Fedorowicz and Aron (2021) – Improving Evidence-Based Policymaking\nReading - NYT Opinion Piece on Chevron (non-required reading for those interested)\nAssignment - Reading Discussion Questions\nSlides - Policy Analysis Overview"
  },
  {
    "objectID": "materials/02_overview.html#thursday-january-25",
    "href": "materials/02_overview.html#thursday-january-25",
    "title": "HPAM 7660 - Week 2 Materials",
    "section": "Thursday, January 25",
    "text": "Thursday, January 25\n\nReading - Erdman (2018) - An 8-step proces to making well-informed decisions\nMemo Example - Prescription Drug Costs (non-required reading, good example of Bardach Steps 1 and 4)\nMemo Example - California EITC (non-required reading, good example of Bardach Step 3)\nSlides - Policy Analysis Overview - Writing and Effective Policy Memo using the Bardach Eightfold Path"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Health Policy Analysis\n        ",
    "section": "",
    "text": "Using data and causal inference to inform evidence-based policy\n        \n        \n            HPAM 7660 • Spring 2024Tulane University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nInstructor\n\n   Kevin Callison\n   Tidewater 1915\n   kcallson@tulane.edu\n\n\n\nCourse details\n\n   Tuesday/Thursday\n   1:00–2:15 PM\n   Tidewater 1202\n\n\n\nContacting me\nI’ll have office hours in Tidwater 1915 and on Zoom on Tuesdays and Thursdays after class. If that time doesn’t work for you, please email me and we can find a time that does. I’m also in my office most days from 9 to 5, so please feel free to stop by."
  },
  {
    "objectID": "assignments/tutorial_7.html",
    "href": "assignments/tutorial_7.html",
    "title": "Summarizing Data Tutorial - Parish Cancer Rates by Race and Cancer Site",
    "section": "",
    "text": "In the last tutorial, we created a data frame that included population weighted, age-adjusted cancer mortality rates for parishes in Cancer Alley and parishes in the rest of Louisiana. Today, we’re going to continue to refine our analytic sample by revising list of parishes comprising Cancer Alley and calculating race- and site-specific cancer rates.\nWith a couple of small exceptions, you have all of the tools you need to make these revisions to the analytic sample. You’ve already used all the commands that you’ll need for this tutorial in previous tutorials and homework assignments. The key here will be how to put those tools to use to get the data to look the way you want it.\nI’m going to outline the steps you’ll need to follow to revise the analytic sample and provide a few hints along the way, but much of this will be up to you!"
  },
  {
    "objectID": "assignments/tutorial_7.html#calculating-cancer-mortality-rates-by-race-and-cancer-site",
    "href": "assignments/tutorial_7.html#calculating-cancer-mortality-rates-by-race-and-cancer-site",
    "title": "Summarizing Data Tutorial - Parish Cancer Rates by Race and Cancer Site",
    "section": "",
    "text": "In the last tutorial, we created a data frame that included population weighted, age-adjusted cancer mortality rates for parishes in Cancer Alley and parishes in the rest of Louisiana. Today, we’re going to continue to refine our analytic sample by revising list of parishes comprising Cancer Alley and calculating race- and site-specific cancer rates.\nWith a couple of small exceptions, you have all of the tools you need to make these revisions to the analytic sample. You’ve already used all the commands that you’ll need for this tutorial in previous tutorials and homework assignments. The key here will be how to put those tools to use to get the data to look the way you want it.\nI’m going to outline the steps you’ll need to follow to revise the analytic sample and provide a few hints along the way, but much of this will be up to you!"
  },
  {
    "objectID": "assignments/tutorial_7.html#steps-for-completing-the-tutorial",
    "href": "assignments/tutorial_7.html#steps-for-completing-the-tutorial",
    "title": "Summarizing Data Tutorial - Parish Cancer Rates by Race and Cancer Site",
    "section": "Steps for Completing the Tutorial",
    "text": "Steps for Completing the Tutorial\nStep 1: Create a Markdown document\nLet’s start by opening the .Rproj file in your hpam7660_Cancer_Alley folder. Then open a new Markdown document. We’ll want to start fresh for this tutorial rather than continuing to work off of the previous document. We’ll also want to remove any existing data frames from our global environment so that we’re not confusing new data for old data. Run the following command to clear your environment:\n\nrm(list = ls())\n\nStep 2: Load packages\nIn the past tutorials, we’ve been loading individual packages like dplyr, reader, etc. It turns out that the tidyverse package contains all of these individual packages and more. So, when you go to load the packages for this tutorial, you can simply load the tidyverse package and the knitr package and they will include all of the packages needed.\nStep 3: Read in the data\nFor this tutorial, you’ll need the la_mort data, the la_pop data, and the stnrd_pop data. You can use the code below to load these files:\n\nla_mort &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fzsnhfd3lq80v2o3sag6c/la_mort.csv?rlkey=h1vyjm2b8ppgejgsg3e8evm7i&dl=1\")\n\nla_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/650k1obpczky6bwa19ex6/la_county_pop.csv?rlkey=0aokd9m76q7mxwus97uslsx7g&dl=1\")\n\nstnrd_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/xzd2o5lza237so6vamqwb/stnrd_pop.csv?rlkey=zp90au2tuq6eptvi1yiyjfzua&dl=1\")\n\nStep 4: Define Cancer Alley Parishes\nWe’ve done this previously using the traditional definition of Cancer Alley parishes, but a recent mapping of air toxicity shows that high levels of airborne carcinogens are concentrated among a subset of the Cancer Alley parishes:\n  Source:ProPublica \nThose parishes are:\n\n5 - Ascension\n47 - Iberville\n89 - St. Charles\n93 - St. James\n95 - St. John the Baptist\n121 - West Baton Rouge\n\nCreate a variable that is equal to 1 if a decedent’s parish of residence was one of these refined Cancer Alley parish and 0 otherwise.\nStep 5: Define Cancer Deaths by Cancer Site\nUse the ucr39 variable in the la_mort file to define deaths from the following cancer types:\n\nstomach, ucr39 == 5\ncolon, ucr39 == 6\npancreas, ucr39 == 7\nlung, ucr39 == 8\nbreast, ucr39 == 9\ncervix, ucr39 == 10\nprostate, ucr39 == 11\nbladder, ucr39 == 12\nlymphoma, ucr39 == 13\nleukemia, ucr39 == 14\nother_site, ucr39 == 15\ntotal, ucr39 5:15\n\nStep 6: Adjust Age Groupings\nRemove anyone whose age is coded as “9999”, define the age break groupings and the corresponding labels, and use the cut() function to create the age groupings. (Hint: we completed this exact same step in the previous tutorial.)\nStep 7: Define Race in the Mortality File\nBecause Louisiana’s population is 90%+ non-Hispanic Black and non-Hispanic white, these are really the only two race categories for which we’ll be able to calculate accurate cancer mortality rates at the parish level. First, filter the data frame so that you only retain Black and white decedents (Hint: you can use the racer5 variable where values of 1 indicate a white decedent and values of 2 indicate a Black decedent.)\nNext create a variable called black that is an indicator for a Black decedent (e.g., this variable will equal 1 for a Black decedent and 0 for a white decedent). Alternately, you could define an indicator for white race instead of Black race, but we’ll end up with the same results in the end either way.\nStep 8: Create Parish Counts of Cancer Deaths by Cancer Site and by Race\nTake the data frame you used in the previous step and sum cancer deaths by site and by race for each parish in the data. (Hint: to accomplish this, you’ll use the group_by and summarize functions.)\nStep 9: Define Race in the Population File\nNow you’ll need to calculate population values by age groupings for Black and white people in Louisiana using the la_pop file. This get’s a little tricky, so I’m providing you with the code you’ll need to do this.\n\nla_pop &lt;- la_pop %&gt;%\n  mutate(\n    black_pop = rowSums(select(., c(\"ba_male\", \"ba_female\"))),\n    white_pop = rowSums(select(., c(\"wa_male\", \"wa_female\"))),\n  )\nla_pop_black &lt;- select(la_pop, county, year, agegrp, black_pop)\nla_pop_white &lt;- select(la_pop, county, year, agegrp, white_pop)\n\nOne important thing to note about this code chunk is that it creates two separate data frames: one called la_pop_black that contains population data for Black people in Louisiana and one called la_pop_white that contains population data for white people in Louisiana.\nStep 10: Join the Mortality and Population Data Frames\nHere’s another tricky step. The reason this is tricky is because we needed to create two separate data frames for population values and so we’ll want to join these data frames to the race-specific mortality data.\nFirst, create a data frame called la_joined_black that filters or subsets your data frame from Step 8 so that it only includes Black decedents and use an inner_join to join this data frame to the la_pop_black data frame from Step 9. Do the same thing for white decedents.\nOnce you have these two data frames, you’re going to want to stack them together. In R, stacking two data frames is done using the rbind function (FYI…STATA calls this an append, SQL calls this a union, and R calls this a bind, I have no idea why we can’t settle on one naming convention for stacking!). It’s also important that the data frames you’re trying to bind have the same column titles. This is not the case with the la_pop_black and la_pop_white data frames because one has a column called black_pop and the other has a column called white_pop. You’ll want to use the rename function to change both of these column names to something like population. Be sure to do this before attempting the bind.\nHere’s the syntax for rbind:\n\nla_bind &lt;- rbind(la_joined_black, la_joined_white)\n\nStep 11: Join the Mortality/Population Data to the Standard Population Data\nWe now need to join the standard population data so that we can age-adjust our mortality rates. Nothing too tricky here, just join on agegrp like you did in the last tutorial. Note that we’re not using race-specific standard populations, but instead using the full population distribution of the U.S. in 2000.\nStep 12: Calculate Population Weights\nAgain, this is done exactly the same way we did it in the previous tutorial. No changes needed here.\nStep 13: Calculate Cancer Mortality Rates by Cancer Site and Race\nWhen we calculated cancer mortality rates in the last tutorial, we used the following code:\n\ndf$cancer_rate_adj &lt;- ((df$cancer39) / (df$tot_pop / 100000)) * df$stnrd_pop_weight\n\nWhere we calculated a variable called cancer_rate_adj that was equal to the sum of cancer deaths (from all sites and all race/ethnicities) divided by the total population (which was scaled by 100,000) and then we multiplied this by the standard population weight. We want to do the same thing here, but we want the cancer mortality rates we calculate to be specific to cancer site and race. (Hint: the data frame already contains separate rows for Black and white decedents, so we don’t need to make any additional accommodation to calculate race-specific rates. You can simply alter the line of code above so that it reflects a particular cancer site and then include a line for each cancer site - DON’T FORGET TO CALCULATE THE TOTAL RATE.)\nOnce you get this far in Step 13 you might see something strange if you look at the data. A couple of cells will contain a value that says “inf”. This happens when the number of cancer deaths is greater than 0, but the age-race population is 0. You have to remember that the population numbers are estimates. So in sparsely populated parishes, the Census might estimate 0 Black people between the ages of 80 and 84 (for example), but the mortality data (which are not estimates) tell us that at least 1 Black person in that age range did in fact die of cancer in that parish.\nSince this only happens a couple of times and we’re weighting by population anyway, you can run the following code that will essentially drop any cases where a cell value is “inf”. Remember to replace df with the actual name of your data frame.\n\nfor (col in names(df)) {\n  df[[col]][is.infinite(df[[col]])] &lt;- NA\n}\n\nStep 14: Aggregate to the Parish-Year Level\nIn the last tutorial, we used the following code to aggregate our data to the parish-year level:\n\nparish_rates &lt;- la_joined_stnrd %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer_rate_adj = sum(cancer_rate_adj, na.rm = TRUE), tot_pop = sum(tot_pop))\n\nNotice that this grouped by county, the indicator for a Cancer Alley parish, and year. We want to do the same thing here, but we now have an extra grouping variable, black. We also only calculated total cancer mortality rates in the previous tutorial, whereas this time, we want to calculate total rates and rates by site. Make the necessary adjustments to the code above.\nStep 15: Weight by Parish Population\nNow that we have age-adjusted cancer mortality rates, we want to weight by parish population so that more populous parishes receive more weight in the aggregate Cancer Ally vs. non-Cancer Alley rates. Again, we did a version of this in the last tutorial using the following code:\n\nparish_rates$pop_weight &lt;- (parish_rates$cancer_rate_adj) * (parish_rates$tot_pop)\n\nYou can do the same thing here, but make sure you calculate site-specific measures.\nStep 16: Aggregate to Cancer Alley and non-Cancer Alley Parishes\nIf you’ve made it this far, you’re almost done. This is the last step! Here we’re taking our parish-year level measures of cancer mortality rates and aggregating them into two groups: rates for Cancer Alley parishes (for Black and white people) and rates for non-Cancer Alley parishes (for Black and white people). In the previous tutorial, we used this code for the aggregation:\n\ncancer_alley_rates &lt;- parish_rates %&gt;%\n  group_by(cancer_parish, year) %&gt;%\n  summarize(cancer_rate_adj_wt = sum(pop_weight) / sum(tot_pop))\n\nYou need to modify this code so that you’re aggregating by the black indicator and across all cancer sites. Once you’ve done that you’re finished. Next time, we’ll (mostly) wrap up the Cancer Alley work by exploring visualizations that attempt to tell a descriptive story of differences in cancer mortality between Cancer Alley parishes and the rest of Louisiana.\nBe sure to save your Markdown file and push the file to your hpam7660_Cancer_Alley GitHub repo."
  },
  {
    "objectID": "assignments/tutorial_5.html",
    "href": "assignments/tutorial_5.html",
    "title": "Data Wrangling Tutorial - Louisiana Mortality File",
    "section": "",
    "text": "In this tutorial, we’ll construct an analytic sample from the Louisiana Mortality file so that we can analyze cancer mortality rates in Cancer Alley parishes.\nBefore beginning the tutorial, open the RStudio project file (.Rproj) you saved for the previous tutorial (this is the project that you used when you created your “Louisiana Mortality Data Description” Markdown document). Note that this project file is already linked to your hpam7660_Cancer_Alley GitHub repo. From now on, we’ll save all work related to the Cancer Alley scenario to this same GitHub repo.\nNext, open a new Markdown document and give it a YAML header that includes the title “Louisiana Mortality Analytic Sample”, your name, the date, and “pdf_document” as the output format.\n\n\nJust like we did in the previous tutorial, you’ll want to first load the Louisiana Mortality File. Include the following R code in your Markdown document.\n\nlibrary(readr)\nla_mort &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fzsnhfd3lq80v2o3sag6c/la_mort.csv?rlkey=h1vyjm2b8ppgejgsg3e8evm7i&dl=1\")\n\n\n\n\n\nCancer Alley is typically defined as an 85-mile stretch along the Mississippi river where a high concentration of petrochemical plants are located. These plants are known to emit toxic chemicals, including ethylene oxide, which is a proven carcinogen. The parishes comprising Cancer Alley include: Ascension, East Baton Rouge, Iberville, Jefferson, Orleans, St. Charles, St. James, St. John the Baptist, and West Baton Rouge.\nTo determine whether cancer mortality tends to be elevated in Cancer Alley parishes, we first need to define a new variable that identifies Cancer Alley parishes. We can create this identifier using the following R code:\n\nla_mort$cancer_parish &lt;- ifelse(la_mort$cntyrsd %in% c(5, 33, 47, 51, 71, 89, 93, 95, 121), 1, 0)\n\nHere we’re telling R to create a new variable in the la_mort data set called cancer_parish. The way to read the code on the right hand side of the &lt;- symbol is as follows: “If county of residence was equal to 5, 33, 47, 51, 71, 89, 93, 95, or 121, the cancer_parish variable should take the value of 1, otherwise the cancer_parish variable should take the value of 0.”\nYou’re probably wondering what the numbers 5, 33, 47, 51, 71, 89, 93, 95, and 121 mean. These are county FIPS codes. Each county in the U.S. is given a unique code. In our case, these codes represent the following parishes:\n\n5 - Ascension\n33 - East Baton Rouge\n47 - Iberville\n51 - Jefferson\n71 - Orleans\n89 - St. Charles\n93 - St. James\n95 - St. John the Baptist\n121 - West Baton Rouge\n\nLet’s check the new variable we created to make sure that it defines the correct parishes as belonging to Cancer Alley. We can use the following R code to double check our work:\n\ntable(la_mort$cancer_parish)\ntable(la_mort$cntyrsd[la_mort$cancer_parish == 1])\n\nThe first table command shows us that our new cancer_parish variable has 445,138 rows where the value is equal to 0 and 197,558 rows where the value is equal to 1. That means that, of the 642,696 rows (i.e., deaths) in our data, a little more than 30% represent people who lived in a Cancer Alley parish.\nThe second table command shows us the parish FIPS code values that correspond to a cancer_parish value of 1. We can see that the parish values exactly match the FIPS codes for Cancer Alley parishes, so our code worked as intended. Had we mistyped a parish FIPS code in our ifelse command, we would see the mistake here and could correct it.\n\n\n\nNow that we have created a variable to categorize Cancer Alley parishes, we need to identify which deaths in the Louisiana Mortality File were due to cancer. As you know from the previous tutorial, the death certificate data contain four levels of “cause of death” codes. The most granular level is the ucod variable, which includes ICD 10 codes. The ucr358 variable aggregates these ICD 10 codes into 358 separate cause of death categories. Similarly, the ucr113 and ucr39 variables aggregate the ICD 10 codes into 113 and 39 separate cause of death categories, respectively.\nLet’s start with the most aggregate level of cause of death codes and work our way down from there to see whether it matters which categorization scheme we use. Similar to what we did with Cancer Alley parishes, we want to create a variable that identifies cancer deaths using the ucr39 variable. Here’s the R code we can use to do that:\n\nla_mort$cancer39 &lt;- ifelse(la_mort$ucr39 %in% c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), 1, 0)\n\nRun the following table command to see how many cancer deaths are included in the data when we define a cancer death using the ucr39 variable:\n\ntable(la_mort$cancer39)\n\nYou should see that, according to the ucr39 classification, 138,677 people in the data died from cancer and 504,019 died from non-cancer causes.\nOne thing to notice about the c() vector in our ifelse command is that the numbers corresponding to cancer deaths are sequential - that is they go from 5 to 15 in units of 1. Because of this, we could express our ifelse statement more succinctly as follows:\n\nla_mort$cancer39 &lt;- ifelse(la_mort$ucr39 %in% c(5:15), 1, 0)\n\nThis will give us the exact same result as the previous command, but allows us to avoid typing all the individual cause of death numbers.\nNow see what happens if you use the ucr113 variable to define cancer deaths. Create a variable called cancer113 that uses the ucr113 variable and codes 19 through 44 to define cancer deaths. Do you get the same number of cancer deaths that you did when using ucr39?\nSpoiler: you do not. Let’s investigate the discrepancy in the coding. First, we know that we have 141,747 cancer deaths when we use ucr113 and 138,677 cancer deaths when we use ucr39. That means that 3,070 people who were coded as having died of cancer in the ucr113 variable were coded as dying of some other cause in the ucr39 variable. Let’s see what those causes are:\n\ntable(la_mort$ucr39[la_mort$cancer113 == 1 & la_mort$cancer39 == 0])\n\nTurns out all 3,070 people were coded as dying from “all other disease” in the ucr39 variable. We can dig a little deeper by examining the ICD 10 codes for these 3,070 people. Run the following R code:\n\ntable(la_mort$ucod[la_mort$cancer113 == 1 & la_mort$cancer39 == 0])\n\nThis shows us that all 3,070 people had an ICD 10 cause of death code between D000 and D489. According to the data description, these ICD 10 codes correspond to a category called “in situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior”. Now we can see what’s happening: in situ neoplasms and benign neoplasms are not malignant neoplasms, so these causes of death were not included in our list of cancers when we used ucr39 codes. It’s not all that clear whether we should consider “in situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior” to be deaths caused by cancer. To be conservative, let’s exclude those deaths from our cancer113 variable. You’ll need to recreate the cancer113 variable and exclude code 44. Once you do that, you should find that you get the same number of cancer deaths, 138,677 whether you use ucr39 or ucr113.\nWe’ll skip replicating this exercise using the ucr358 variable, but if you try it on your own, you should find that it also gives you 138,677 deaths attributable to malignant cancers.\n\n\n\nNow that we’ve identified cancer deaths, we’ll want to use this information to build annual parish-specific cancer mortality rates. As the word “rate” implies, we’re going to need to create both a numerator and a denominator. Our numerator should be the count of cancer deaths in each parish by year and our denominator should be parish population. We’ll bring in population measures in the next step, but first, let’s aggregate cancer death counts by parish and year, and that we want to ignore cases where cancer39 is missing (though there shouldn’t be any rows where cancer39 is missing, we’ll still add this option as a precaution).\n\nlibrary(dplyr)\nparish_count &lt;- la_mort %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer39 = sum(cancer39, na.rm = TRUE))\n\nIt’s worth taking a minute to clearly explain what’s going on here. First, we’re loading the dplyr library because we want to use the group_by command and that command is part of the dplyr package. Next, we’re creating a new data frame called parish_count that is going to be a transformation of the la_mort data frame that we’ve been working with. The last two lines of this code tell R that we want to calculate the sum of the cancer39 variable by parish (cntyrsd), cancer alley parishes (cancer_parish), and year.\nWe can get a sense of what this data aggregation looks like by using the summary command (note that summarize and summary are two different commands):\n\nsummary(parish_count$cancer39)\n\nThe results of the summary command tell us that, on average, there were 144.5 cancer deaths per parish per year. The parish-year grouping with the fewest cancer deaths had 3 deaths and the parish-year grouping with the most cancer deaths had 992 cancer deaths.\nWe can also take a more complete look at our aggregate data frame by using the View(parish_count) command. Remember that Markdown doesn’t like the View() command, so you’ll want to type this command into the Console Window command line instead of including it in your Markdown document.\n\n\n\nAs you know, we have a very rough measure of parish population in the mortality file, but it doesn’t provide the level of precision we need to calculate accurate cancer mortality rates. So we’re going to have to bring in parish-level population data from another source.\nFirst you’ll need to download a data file that contains county population estimates for Louisiana. I’ve created this file using data from the Census. If you’re interested, you can find the raw data here. Otherwise, you can download the file you’ll need for this project by using the code below.\n\nla_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/650k1obpczky6bwa19ex6/la_county_pop.csv?rlkey=0aokd9m76q7mxwus97uslsx7g&dl=1\")\n\nOne you read this file into R, you’ll see that there are 24,320 rows and 23 columns. Some of the column names should be displayed in the notes that follow the read_csv command. We’ll go over this population file in more detail in a later class, but you can get some sense of the contents from the column names.\nNow we’ll need to use the join command to merge the population data from the la_pop data frame to the parish_count data frame. However, before we can do that, there’s an important modification that we’ll need to make to our data frames. In order to join two different data frames, we need at least one common variable between the data frames. These common variables are called “key variables”. In this case, we’ll have two key variables: county and year. There’s just one problem here - while the year variable has the same name in both the parish_count and la_pop data frames, the county variable does not. In the parish_count data frame, the variable that identifies county is called cntyrsd, while in the la_pop data frame, the variable that identifies county is called county. Let’s change the name of the cntyrsd variable in the parish_count data frame to county so that the key variable names are consistent across the two data frames:\n\nparish_count &lt;- parish_count %&gt;%\n  rename(county = cntyrsd)\n\nYou can double check to make sure that the rename command actually worked by typing either parish_count or glimpse(parish_count) into the Console Window command line.\nNow that we’ve taken care of the variable name issue, we can join the two data frames:\n\nla_joined &lt;- parish_count %&gt;%\n  inner_join(la_pop, by = c(\"county\", \"year\"))\n\nYou’ll get more practice with joins and key variables in your next homework assignment, but a quick explanation of what is happening here is that R is merging the columns from the parish_count and la_pop data frames into a single data frame called la_joined. The common columns linking the two data frames are the county and year columns. We use the command inner_join because we only want to keep counties and years that are present in both data frames (e.g., the la_pop data frame contains population estimates for 2020, but we only have data through 2019 in the parish_count data frame). This page provides a nice description of the different types of joins and very helpful picture.\nNote that there is a way to use key variables with different names that avoids the renaming step (you’ll see an example of this in ModernDive chapter 3.7.2), but that can get confusing. For the sake of this tutorial, renaming the key variables so that they share the same names in both data frames is more straightforward.\nTake a look at the new la_joined data frame by typing View(la_joined) in the Console Window command line. You should see that the columns from both the parish_count and la_pop data frames are now combined into this new data frame.\n\n\n\nNow that we have a data frame that contains our numerator (cancer deaths by parish and year) and our denominator (parish population by year), we can calculate parish-specific cancer mortality rates. However, one last data modification is required before we do so. If you take a look at either the la_joined or la_pop data frames, you’ll notice that we have population estimates for several different age groups. We’ll calculate age-adjusted cancer mortality rates in an upcoming tutorial. For now, we’ll just calculate crude cancer mortality rates using the entire parish population. Practically, this means we’ll want to delete rows where the agegrp variable is not equal to “all” (you can type table(la_joined$agegrp) to see the different values of the agegrp variable). We’ve seen a couple of different ways to do this in our tutorials and homework assignments. You could use either of the two options below to achieve the same result:\n\nla_joined_all &lt;- subset(la_joined, agegrp == \"all\")\n\nOR\n\nla_joined_all &lt;- la_joined %&gt;%\n  filter(agegrp == \"all\")\n\nYou can double check to make sure the la_joined_all data frame contains only rows where agegrp is equal to “all” by typing table(la_joined_all$agegrp).\nNow it’s pretty easy to calculate parish-level cancer mortality rates by year. We’ll just create a new variable called cancer_rate_total that is the ratio of cancer deaths (cancer39) to population (tot_pop) in each parish in each year.\n\nla_joined_all$cancer_rate_total &lt;- (la_joined_all$cancer39) / (la_joined_all$tot_pop)\n\nLet’s use the summary command to take a look at this new variable.\n\nsummary(la_joined_all$cancer_rate_total)\n\nYou’ll see that these numbers are very small! It’s common to calculate cancer mortality rates as deaths per 100,000 population. Let’s modify our cancer_rate_total variable so that it represents cancer deaths per 100,000 parish population.\n\nla_joined_all$cancer_rate_total &lt;- ((la_joined_all$cancer39) / (la_joined_all$tot_pop / 100000))\n\nIf you run the summary command again, you’ll see that the cancer mortality rates we’ve calculated are the same as before, but we’ve moved the decimal place to make the numbers easier to interpret.\nFinally, let’s create a simple table of parish-level cancer mortality rates for 2019. We can use the subset and kable commands to create the table as follows:\n\nparish_cancer_2019 &lt;- subset(la_joined_all, year == 2019)\nlibrary(knitr)\nkable(parish_cancer_2019[, c(\"county\", \"cancer_rate_total\")])\n\nThis doesn’t quite get us to where we want to be yet (i.e., comparing cancer mortality rates for Cancer Alley parishes to the rest of Louisiana), but we now have an analytic data set that we can use to get there.\nBe sure to save your Markdown file and push the file to your hpam7660_Cancer_Alley GitHub repo."
  },
  {
    "objectID": "assignments/tutorial_5.html#analyic-sample-construction",
    "href": "assignments/tutorial_5.html#analyic-sample-construction",
    "title": "Data Wrangling Tutorial - Louisiana Mortality File",
    "section": "",
    "text": "In this tutorial, we’ll construct an analytic sample from the Louisiana Mortality file so that we can analyze cancer mortality rates in Cancer Alley parishes.\nBefore beginning the tutorial, open the RStudio project file (.Rproj) you saved for the previous tutorial (this is the project that you used when you created your “Louisiana Mortality Data Description” Markdown document). Note that this project file is already linked to your hpam7660_Cancer_Alley GitHub repo. From now on, we’ll save all work related to the Cancer Alley scenario to this same GitHub repo.\nNext, open a new Markdown document and give it a YAML header that includes the title “Louisiana Mortality Analytic Sample”, your name, the date, and “pdf_document” as the output format.\n\n\nJust like we did in the previous tutorial, you’ll want to first load the Louisiana Mortality File. Include the following R code in your Markdown document.\n\nlibrary(readr)\nla_mort &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fzsnhfd3lq80v2o3sag6c/la_mort.csv?rlkey=h1vyjm2b8ppgejgsg3e8evm7i&dl=1\")\n\n\n\n\n\nCancer Alley is typically defined as an 85-mile stretch along the Mississippi river where a high concentration of petrochemical plants are located. These plants are known to emit toxic chemicals, including ethylene oxide, which is a proven carcinogen. The parishes comprising Cancer Alley include: Ascension, East Baton Rouge, Iberville, Jefferson, Orleans, St. Charles, St. James, St. John the Baptist, and West Baton Rouge.\nTo determine whether cancer mortality tends to be elevated in Cancer Alley parishes, we first need to define a new variable that identifies Cancer Alley parishes. We can create this identifier using the following R code:\n\nla_mort$cancer_parish &lt;- ifelse(la_mort$cntyrsd %in% c(5, 33, 47, 51, 71, 89, 93, 95, 121), 1, 0)\n\nHere we’re telling R to create a new variable in the la_mort data set called cancer_parish. The way to read the code on the right hand side of the &lt;- symbol is as follows: “If county of residence was equal to 5, 33, 47, 51, 71, 89, 93, 95, or 121, the cancer_parish variable should take the value of 1, otherwise the cancer_parish variable should take the value of 0.”\nYou’re probably wondering what the numbers 5, 33, 47, 51, 71, 89, 93, 95, and 121 mean. These are county FIPS codes. Each county in the U.S. is given a unique code. In our case, these codes represent the following parishes:\n\n5 - Ascension\n33 - East Baton Rouge\n47 - Iberville\n51 - Jefferson\n71 - Orleans\n89 - St. Charles\n93 - St. James\n95 - St. John the Baptist\n121 - West Baton Rouge\n\nLet’s check the new variable we created to make sure that it defines the correct parishes as belonging to Cancer Alley. We can use the following R code to double check our work:\n\ntable(la_mort$cancer_parish)\ntable(la_mort$cntyrsd[la_mort$cancer_parish == 1])\n\nThe first table command shows us that our new cancer_parish variable has 445,138 rows where the value is equal to 0 and 197,558 rows where the value is equal to 1. That means that, of the 642,696 rows (i.e., deaths) in our data, a little more than 30% represent people who lived in a Cancer Alley parish.\nThe second table command shows us the parish FIPS code values that correspond to a cancer_parish value of 1. We can see that the parish values exactly match the FIPS codes for Cancer Alley parishes, so our code worked as intended. Had we mistyped a parish FIPS code in our ifelse command, we would see the mistake here and could correct it.\n\n\n\nNow that we have created a variable to categorize Cancer Alley parishes, we need to identify which deaths in the Louisiana Mortality File were due to cancer. As you know from the previous tutorial, the death certificate data contain four levels of “cause of death” codes. The most granular level is the ucod variable, which includes ICD 10 codes. The ucr358 variable aggregates these ICD 10 codes into 358 separate cause of death categories. Similarly, the ucr113 and ucr39 variables aggregate the ICD 10 codes into 113 and 39 separate cause of death categories, respectively.\nLet’s start with the most aggregate level of cause of death codes and work our way down from there to see whether it matters which categorization scheme we use. Similar to what we did with Cancer Alley parishes, we want to create a variable that identifies cancer deaths using the ucr39 variable. Here’s the R code we can use to do that:\n\nla_mort$cancer39 &lt;- ifelse(la_mort$ucr39 %in% c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), 1, 0)\n\nRun the following table command to see how many cancer deaths are included in the data when we define a cancer death using the ucr39 variable:\n\ntable(la_mort$cancer39)\n\nYou should see that, according to the ucr39 classification, 138,677 people in the data died from cancer and 504,019 died from non-cancer causes.\nOne thing to notice about the c() vector in our ifelse command is that the numbers corresponding to cancer deaths are sequential - that is they go from 5 to 15 in units of 1. Because of this, we could express our ifelse statement more succinctly as follows:\n\nla_mort$cancer39 &lt;- ifelse(la_mort$ucr39 %in% c(5:15), 1, 0)\n\nThis will give us the exact same result as the previous command, but allows us to avoid typing all the individual cause of death numbers.\nNow see what happens if you use the ucr113 variable to define cancer deaths. Create a variable called cancer113 that uses the ucr113 variable and codes 19 through 44 to define cancer deaths. Do you get the same number of cancer deaths that you did when using ucr39?\nSpoiler: you do not. Let’s investigate the discrepancy in the coding. First, we know that we have 141,747 cancer deaths when we use ucr113 and 138,677 cancer deaths when we use ucr39. That means that 3,070 people who were coded as having died of cancer in the ucr113 variable were coded as dying of some other cause in the ucr39 variable. Let’s see what those causes are:\n\ntable(la_mort$ucr39[la_mort$cancer113 == 1 & la_mort$cancer39 == 0])\n\nTurns out all 3,070 people were coded as dying from “all other disease” in the ucr39 variable. We can dig a little deeper by examining the ICD 10 codes for these 3,070 people. Run the following R code:\n\ntable(la_mort$ucod[la_mort$cancer113 == 1 & la_mort$cancer39 == 0])\n\nThis shows us that all 3,070 people had an ICD 10 cause of death code between D000 and D489. According to the data description, these ICD 10 codes correspond to a category called “in situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior”. Now we can see what’s happening: in situ neoplasms and benign neoplasms are not malignant neoplasms, so these causes of death were not included in our list of cancers when we used ucr39 codes. It’s not all that clear whether we should consider “in situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior” to be deaths caused by cancer. To be conservative, let’s exclude those deaths from our cancer113 variable. You’ll need to recreate the cancer113 variable and exclude code 44. Once you do that, you should find that you get the same number of cancer deaths, 138,677 whether you use ucr39 or ucr113.\nWe’ll skip replicating this exercise using the ucr358 variable, but if you try it on your own, you should find that it also gives you 138,677 deaths attributable to malignant cancers.\n\n\n\nNow that we’ve identified cancer deaths, we’ll want to use this information to build annual parish-specific cancer mortality rates. As the word “rate” implies, we’re going to need to create both a numerator and a denominator. Our numerator should be the count of cancer deaths in each parish by year and our denominator should be parish population. We’ll bring in population measures in the next step, but first, let’s aggregate cancer death counts by parish and year, and that we want to ignore cases where cancer39 is missing (though there shouldn’t be any rows where cancer39 is missing, we’ll still add this option as a precaution).\n\nlibrary(dplyr)\nparish_count &lt;- la_mort %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer39 = sum(cancer39, na.rm = TRUE))\n\nIt’s worth taking a minute to clearly explain what’s going on here. First, we’re loading the dplyr library because we want to use the group_by command and that command is part of the dplyr package. Next, we’re creating a new data frame called parish_count that is going to be a transformation of the la_mort data frame that we’ve been working with. The last two lines of this code tell R that we want to calculate the sum of the cancer39 variable by parish (cntyrsd), cancer alley parishes (cancer_parish), and year.\nWe can get a sense of what this data aggregation looks like by using the summary command (note that summarize and summary are two different commands):\n\nsummary(parish_count$cancer39)\n\nThe results of the summary command tell us that, on average, there were 144.5 cancer deaths per parish per year. The parish-year grouping with the fewest cancer deaths had 3 deaths and the parish-year grouping with the most cancer deaths had 992 cancer deaths.\nWe can also take a more complete look at our aggregate data frame by using the View(parish_count) command. Remember that Markdown doesn’t like the View() command, so you’ll want to type this command into the Console Window command line instead of including it in your Markdown document.\n\n\n\nAs you know, we have a very rough measure of parish population in the mortality file, but it doesn’t provide the level of precision we need to calculate accurate cancer mortality rates. So we’re going to have to bring in parish-level population data from another source.\nFirst you’ll need to download a data file that contains county population estimates for Louisiana. I’ve created this file using data from the Census. If you’re interested, you can find the raw data here. Otherwise, you can download the file you’ll need for this project by using the code below.\n\nla_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/650k1obpczky6bwa19ex6/la_county_pop.csv?rlkey=0aokd9m76q7mxwus97uslsx7g&dl=1\")\n\nOne you read this file into R, you’ll see that there are 24,320 rows and 23 columns. Some of the column names should be displayed in the notes that follow the read_csv command. We’ll go over this population file in more detail in a later class, but you can get some sense of the contents from the column names.\nNow we’ll need to use the join command to merge the population data from the la_pop data frame to the parish_count data frame. However, before we can do that, there’s an important modification that we’ll need to make to our data frames. In order to join two different data frames, we need at least one common variable between the data frames. These common variables are called “key variables”. In this case, we’ll have two key variables: county and year. There’s just one problem here - while the year variable has the same name in both the parish_count and la_pop data frames, the county variable does not. In the parish_count data frame, the variable that identifies county is called cntyrsd, while in the la_pop data frame, the variable that identifies county is called county. Let’s change the name of the cntyrsd variable in the parish_count data frame to county so that the key variable names are consistent across the two data frames:\n\nparish_count &lt;- parish_count %&gt;%\n  rename(county = cntyrsd)\n\nYou can double check to make sure that the rename command actually worked by typing either parish_count or glimpse(parish_count) into the Console Window command line.\nNow that we’ve taken care of the variable name issue, we can join the two data frames:\n\nla_joined &lt;- parish_count %&gt;%\n  inner_join(la_pop, by = c(\"county\", \"year\"))\n\nYou’ll get more practice with joins and key variables in your next homework assignment, but a quick explanation of what is happening here is that R is merging the columns from the parish_count and la_pop data frames into a single data frame called la_joined. The common columns linking the two data frames are the county and year columns. We use the command inner_join because we only want to keep counties and years that are present in both data frames (e.g., the la_pop data frame contains population estimates for 2020, but we only have data through 2019 in the parish_count data frame). This page provides a nice description of the different types of joins and very helpful picture.\nNote that there is a way to use key variables with different names that avoids the renaming step (you’ll see an example of this in ModernDive chapter 3.7.2), but that can get confusing. For the sake of this tutorial, renaming the key variables so that they share the same names in both data frames is more straightforward.\nTake a look at the new la_joined data frame by typing View(la_joined) in the Console Window command line. You should see that the columns from both the parish_count and la_pop data frames are now combined into this new data frame.\n\n\n\nNow that we have a data frame that contains our numerator (cancer deaths by parish and year) and our denominator (parish population by year), we can calculate parish-specific cancer mortality rates. However, one last data modification is required before we do so. If you take a look at either the la_joined or la_pop data frames, you’ll notice that we have population estimates for several different age groups. We’ll calculate age-adjusted cancer mortality rates in an upcoming tutorial. For now, we’ll just calculate crude cancer mortality rates using the entire parish population. Practically, this means we’ll want to delete rows where the agegrp variable is not equal to “all” (you can type table(la_joined$agegrp) to see the different values of the agegrp variable). We’ve seen a couple of different ways to do this in our tutorials and homework assignments. You could use either of the two options below to achieve the same result:\n\nla_joined_all &lt;- subset(la_joined, agegrp == \"all\")\n\nOR\n\nla_joined_all &lt;- la_joined %&gt;%\n  filter(agegrp == \"all\")\n\nYou can double check to make sure the la_joined_all data frame contains only rows where agegrp is equal to “all” by typing table(la_joined_all$agegrp).\nNow it’s pretty easy to calculate parish-level cancer mortality rates by year. We’ll just create a new variable called cancer_rate_total that is the ratio of cancer deaths (cancer39) to population (tot_pop) in each parish in each year.\n\nla_joined_all$cancer_rate_total &lt;- (la_joined_all$cancer39) / (la_joined_all$tot_pop)\n\nLet’s use the summary command to take a look at this new variable.\n\nsummary(la_joined_all$cancer_rate_total)\n\nYou’ll see that these numbers are very small! It’s common to calculate cancer mortality rates as deaths per 100,000 population. Let’s modify our cancer_rate_total variable so that it represents cancer deaths per 100,000 parish population.\n\nla_joined_all$cancer_rate_total &lt;- ((la_joined_all$cancer39) / (la_joined_all$tot_pop / 100000))\n\nIf you run the summary command again, you’ll see that the cancer mortality rates we’ve calculated are the same as before, but we’ve moved the decimal place to make the numbers easier to interpret.\nFinally, let’s create a simple table of parish-level cancer mortality rates for 2019. We can use the subset and kable commands to create the table as follows:\n\nparish_cancer_2019 &lt;- subset(la_joined_all, year == 2019)\nlibrary(knitr)\nkable(parish_cancer_2019[, c(\"county\", \"cancer_rate_total\")])\n\nThis doesn’t quite get us to where we want to be yet (i.e., comparing cancer mortality rates for Cancer Alley parishes to the rest of Louisiana), but we now have an analytic data set that we can use to get there.\nBe sure to save your Markdown file and push the file to your hpam7660_Cancer_Alley GitHub repo."
  },
  {
    "objectID": "assignments/tutorial_3.html",
    "href": "assignments/tutorial_3.html",
    "title": "Tutorial 3 - Data Importing in RStudio Tutorial",
    "section": "",
    "text": "Let’s go back to the scenario I posed at the beginning of the semester:\nYou are a legislative policy advisor working for a Congressional representative from the state of Louisiana. The representative is interested in sponsoring legislation to direct resources to people living in Cancer Alley, an 85-mile tract of land along the Mississippi River where residents have historically experienced high rates of cancer. As part of the policy development process, the representative has tasked you with collecting data on cancer incidence rates for people living in Cancer Alley over the past 20 years and creating a visualization that depicts the disproportionate cancer burden that these people face.\nOver the next few weeks, we’ll work through this scenario using mortality data from the National Vital Statistics System collected by the CDC to create a visualization of cancer mortality rates across Louisiana parishes with a focus on Cancer Alley."
  },
  {
    "objectID": "assignments/tutorial_3.html#importing-louisiana-mortality-data-into-rstudio",
    "href": "assignments/tutorial_3.html#importing-louisiana-mortality-data-into-rstudio",
    "title": "Tutorial 3 - Data Importing in RStudio Tutorial",
    "section": "",
    "text": "Let’s go back to the scenario I posed at the beginning of the semester:\nYou are a legislative policy advisor working for a Congressional representative from the state of Louisiana. The representative is interested in sponsoring legislation to direct resources to people living in Cancer Alley, an 85-mile tract of land along the Mississippi River where residents have historically experienced high rates of cancer. As part of the policy development process, the representative has tasked you with collecting data on cancer incidence rates for people living in Cancer Alley over the past 20 years and creating a visualization that depicts the disproportionate cancer burden that these people face.\nOver the next few weeks, we’ll work through this scenario using mortality data from the National Vital Statistics System collected by the CDC to create a visualization of cancer mortality rates across Louisiana parishes with a focus on Cancer Alley."
  },
  {
    "objectID": "assignments/tutorial_3.html#cdc-wonder",
    "href": "assignments/tutorial_3.html#cdc-wonder",
    "title": "Tutorial 3 - Data Importing in RStudio Tutorial",
    "section": "CDC WONDER",
    "text": "CDC WONDER\nIt turns out that the CDC provides a nice tool for analyzing mortality rates over time and across different places in the U.S. We’ll take a quick look at this tool before we start working with the underlying disaggregated data.\nGo to the CDC Mortality Statistics Page and scroll down to the “CDC WONDER interactive database” link at the bottom of the page. After clicking on that link, you should see something like this:\n\nNotice that there’s a link for something called “Cancer Statistics”. That sounds promising, let’s check that out.\nLooks like they have data on cancer mortality from 1999 to 2020. Click on “Data Request” to see more options.\n\nWe’ll quickly come to see that this isn’t going to work for our purposes. Take a look at the “Group Results By” dropdown menu:\n\nYou’ll notice a few options under “Location”, including Region, Division, State, MSA, and States and Puerto Rico. Region, Division, and State are all too aggregate to allow us to examine cancer mortality across parishes in Louisiana. MSA, on the other hand, is too disaggregate and only includes data for Baton Rouge and New Orleans.\nLuckily, there is another option that will allow us to examine cancer mortality at the county/parish level. Go back to the Wonder menu and click on “Underlying Cause of Death”.\n\nWe have a few options here that we don’t need to get into right now, so let’s just click on the first link: “2018-2021: Underlying Cause of Death by Single-Race Categories”.\n\nAgree to the data use restrictions and you’ll see a request form with a lot of options that we can choose from. Start by clicking the “Group Results By” dropdown and you’ll notice that we can select “County” as our geographic unit. We can give our data request a title - let’s call it “Louisiana Cancer Deaths by Parish, 2018”.\n\nNext, we’ll need to select “Louisiana” as our location. We don’t need to change the “urbanization” category.\n\nWe have the option to select specific age ranges, sexes, races, and Hispanic ethnicity, but let’s leave these as the defaults for now. We’ll select data from 2018 for illustration purposes, but we could pick multiple years if we wanted to.\n\nFinally, we can leave the weekday, autopsy, and place of death options on their default selections. We want to choose “neoplasms” as the cause of death (we could get more detailed here with specific cancer sites, but let’s keep it general for now). We don’t need to make any changes to the “Other options” section.\n Once you click “send” you should see a table that includes parish, counts of cancer deaths, parish population, and death rates.\n\nWe can even click the “Map” tab and get a nice visual of cancer rates across Louisiana."
  },
  {
    "objectID": "assignments/tutorial_3.html#nvss-mortality-files",
    "href": "assignments/tutorial_3.html#nvss-mortality-files",
    "title": "Tutorial 3 - Data Importing in RStudio Tutorial",
    "section": "NVSS Mortality Files",
    "text": "NVSS Mortality Files\nThe aggregate tools that the CDC provides to track cancer mortality are great, but having access to the underlying disaggregated data provides us with a LOT of additional flexibility. To analyze the disaggregated data, we’ll need to use RStudio, but let’s first take a look at the data files on the CDC website. Navigate back to the CDC Mortality Statistics Page and click on the “Public-Use Data Files” link at the bottom of the page. About halfway down, you’ll see a section called “Mortality Multiple Cause Files”.\n\nThese zip files contain the underlying data that the CDC Wonder query tool uses to construct the tables and maps we saw. You won’t need to download these files for this tutorial - I’ve created a file from this data that we’ll use in class. But take a second to click on the “User’s Guide” link. This will take you to the documentation page for the mortality files. Once we load the data in RStudio, we can come back here to see how the variables are defined.\nLoad the mortality file into RStudio using the following command. You can either do this directly in the Console Window’s command line or open an R Script file and run the code from there. We’re not going to be saving any of our work in this tutorial or creating a report, so there’s no need to use Markdown.\n\nla_mort &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fzsnhfd3lq80v2o3sag6c/la_mort.csv?rlkey=h1vyjm2b8ppgejgsg3e8evm7i&dl=1\")\n\nWe’re loading a .csv file that I’ve stored in a Dropbox folder into an R data frame that we’ll call la_mort. Note that the read_csv function is part of the readr package, so you’ll need to be sure you’ve loaded that package before using it.\n\nRStuido tells us that the data frame has 642,696 rows and 29 columns - that’s a LOT of data rows. In fact each row of data in the data frame corresponds to a Louisiana resident’s death between 2005 and 2019. RStudio also tells us that, of the 29 columns, 7 contain data stored as characters “chr” and 22 contain data stored as double precision floating point numbers “dbl” (this is just a fancy way of saying a field that allows long decimals).\nWe can view the data frame tibble by typing la_mort into the Console Window command line (or adding this line to your R Script) and hitting enter. This will allow us to see the first 10 rows of the data frame along with a selection of the variables. We don’t yet know what all of these variables mean, but we’ll work on that in the upcoming classes. For now, let’s take a look at some basic descriptives using a few easy-to-identify variables.\nA couple of new commands are going to be helpful here. First, let’s look at the table command, which will help us preview the data we loaded. Type the following into the Console Window command line or add it to your R Script and run it:\n\ntable(la_mort$year)\n\nFirst note that the syntax is a little different here. We’re not typing the typical new_data_frame &lt;- old_data_frame.... Instead, we type the data frame and the variable we’re interested in separated by the dollar sign. After running this command, you should see a list of each year in the data file and the number of rows corresponding to that year. Remember that each row in this data frame represents a death, so there are approximately 40,000 deaths in Louisiana in each year from 2005 through 2019.\nNow let’s take a look at the breakdown of deaths by sex. Type the following into the Console Window command line or add it to your R Script and run it:\n\ntable(la_mort$sex)\n\nThis tells us that 312,658 rows in the data frame (i.e., deaths) are for females and 330,038 are for males.\nLet’s try one more. Type the following into the Console Window command line or add it to your R Script and run it:\n\ntable(la_mort$age)\n\nHere we’re trying to preview the number of deaths by age, but the output looks very strange. It turns out that the variable age is not measured in years in the data. If you take a look at the data description file (you’ll spend more time on this in a later tutorial), you’ll see that age is measured in years for those who were at least a year old when they die and measured in months, days, or hours for those who were less than a year old when they died. Run the following code to convert age to years (you’ll learn what all the elements of this code means in the Data Wrangling section):\n\nlibrary(dplyr)\n\nla_mort &lt;- la_mort %&gt;%\n  filter(age != 9999) %&gt;%\n  mutate(age = ifelse(age &gt;= 2000, 0, age - 1000))\n\nNow run the table command again and take a look at the age distribution of the data. You should see that all of the ages have been converted to years.\nFinally, notice that you can include more than one argument in the table function. For example, if you wanted to know the number of deaths by sex and year, you could run the following code:\n\ntable(la_mort$year, la_mort$sex)\n\nBe careful with this because adding too many arguments can make the output unwieldy. For example, try the following:\n\ntable(la_mort$year, la_mort$sex, la_mort$age)\n\nThis tries to print deaths by year, sex, and age, but you’ll see that R stops before printing the entire list.\nThat’s it for this tutorial. Next time, we’ll work on defining the variables in the data frame so that we know what information we have to work with."
  },
  {
    "objectID": "assignments/tutorial_1_old.html",
    "href": "assignments/tutorial_1_old.html",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "",
    "text": "In this tutorial, we’re going to get R, RStudio, and R Markdown set up on your computer. It’s important that you follow these steps in order. You may experience errors if you try go out of order. To get started:\n\nDownload and install the most recent version of R. There are versions available for the Windows, Mac, and Linux operating systems. On a Windows computer, you will want to install using the R-x.y.z-win.exe file where x.y.z is a version number. On a Mac, you will want to install using the R-x.y.z.pkg file that is notarized and signed.\nWith R installed, download and install RStudio. RStudio is a type of “integrated development environment” or IDE designed for R. It makes working with R considerably easier and is available for most platforms. It is also free.\nInstall the packages we will use throughout the semester. To do this, either type or copy and paste each of the following lines of code into the “Console” in RStudio (lower left panel by default). Make sure you do this separately for each line. If you are asked if you want to install any packages from source, type “no”. Note that the symbols next to my_package are a less than sign &lt; followed by a minus sign - with no space between them. (Don’t be worried if you see some red text here. Those are usually just messages telling you information about the packages you are installing. Unless you see the word Error you should be fine.)\n\n\nmy_packages &lt;- c(\"tidyverse\", \"usethis\", \"devtools\", \"learnr\",\n                 \"tinytex\", \"gitcreds\", \"gapminder\")\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\n\nFor some things in the course, we’ll need produce PDFs from R and that requires something called LaTeX. If you’ve never heard of that, it’s completely fine and you should just run the following two lines of R code:\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX"
  },
  {
    "objectID": "assignments/tutorial_1_old.html#installing-r-and-rstudio",
    "href": "assignments/tutorial_1_old.html#installing-r-and-rstudio",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "",
    "text": "In this tutorial, we’re going to get R, RStudio, and R Markdown set up on your computer. It’s important that you follow these steps in order. You may experience errors if you try go out of order. To get started:\n\nDownload and install the most recent version of R. There are versions available for the Windows, Mac, and Linux operating systems. On a Windows computer, you will want to install using the R-x.y.z-win.exe file where x.y.z is a version number. On a Mac, you will want to install using the R-x.y.z.pkg file that is notarized and signed.\nWith R installed, download and install RStudio. RStudio is a type of “integrated development environment” or IDE designed for R. It makes working with R considerably easier and is available for most platforms. It is also free.\nInstall the packages we will use throughout the semester. To do this, either type or copy and paste each of the following lines of code into the “Console” in RStudio (lower left panel by default). Make sure you do this separately for each line. If you are asked if you want to install any packages from source, type “no”. Note that the symbols next to my_package are a less than sign &lt; followed by a minus sign - with no space between them. (Don’t be worried if you see some red text here. Those are usually just messages telling you information about the packages you are installing. Unless you see the word Error you should be fine.)\n\n\nmy_packages &lt;- c(\"tidyverse\", \"usethis\", \"devtools\", \"learnr\",\n                 \"tinytex\", \"gitcreds\", \"gapminder\")\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\n\nFor some things in the course, we’ll need produce PDFs from R and that requires something called LaTeX. If you’ve never heard of that, it’s completely fine and you should just run the following two lines of R code:\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX"
  },
  {
    "objectID": "assignments/tutorial_1_old.html#installing-and-configuring-git",
    "href": "assignments/tutorial_1_old.html#installing-and-configuring-git",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Installing and configuring git",
    "text": "Installing and configuring git\nGit is a version control program that helps organize the process of writing and maintaining code. It allows you to maintain a history of edits to your code without having to resort to a set of files like:\nmy_code.R\nmy_code_v1.R\nmy_code_v2_FINAL.R\nThere’s a lot to git and it will be hard to use in the beginning if you’re not familiar, but in the long term there are large benefits. First, when you use git, you are much less likely to encounter a devastating data failure. All of your (committed) changes to your project are preserved, even when you make new changes or you revert old changes.\nGit is also a very useful way for people to collaborate. There is a large community built up around it and, once your projects are publicly available on Github (a website for hosting git repositories), there are a host of ways that people can collaborate with you.\n\nInstall git\nYou might already have git installed on your computer, especially if you have a Mac. To check, in RStudio, click on the Terminal tab in the bottom-left panel (next to the Console tab). Type git --version at the command prompt. If you get a response with a version number, great you are all set. If you get any kind of error message, then you’ll need to install git.\n\n\nGit install for Mac\nGo to the terminal tab in RStudio and type git --version to elicit an offer to install developer command line tools. This will appear as a popup menu on your computer (it may get stuck behind other windows that you already have open, so look around for it if you don’t see it immediately). Accept the offer and click on “install”.\n\n\nGit install for Windows\nDownload Git here and click to install. Click “next” at the GNU General Public License screen. Select an installation path. Note that RStudio will look for Git in the \"C:\\Program Files\" path by default, so that might be a good place to install Git. However, you can install Git using any path you’d like and we can tell RStudio where to find it.\nNext you’ll be confronted with a bunch of different installation options. We can go with the default for all of these options, so you can just click “next” to keep navigating through to the “install” button.\nNow that you’ve installed Git, you’ll want to check to see if RStudio can find it. Go to the terminal tab in RStudio and type git --version. If you see a version listed, then you’re all set and can move on to the next step. If you get an error message saying something along the lines of “git not found” or “git is not recognized”, then you’ll need to point RStudio to the Git installation path.\nYou can do this by navigating to the “Tools” menu in RStudio, selecting “Global Options”, then clicking on the Git/SVN icon in the left menu bar. Enter the Git installation path in the “Git executable:” box and add \\bin\\git to the end of the path (so, for example, if your installation path was C:\\Documents\\R_Stuff\\Git then you want to enter C:\\Documents\\R_Stuff\\Git\\bin\\git.) Click “apply” then click “ok” and then you’ll need to restart RStudio. After the restart go to the terminal window in RStudio and type git --version and you should see something like “git version 2.43.0.windows.1” displayed. Once you see that message, you can move on to the next step."
  },
  {
    "objectID": "assignments/tutorial_1_old.html#setup-a-github-account",
    "href": "assignments/tutorial_1_old.html#setup-a-github-account",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Setup a GitHub account",
    "text": "Setup a GitHub account\nNext, you can setup a GitHub account. You can think of GitHub as similar to Dropbox or Google Drive for your git projects (“repositories”) where everything is public by default. Since you might use this account to interact with potential employers in the future, you should probably pick a professional username.\nOnce you have a GitHub account, you can configure your local git program to interact with your GitHub account. Run the following two lines of code in the Terminal replacing \"Bruce Springsteen\" with your name and \"bruce@tulane.edu\" with your email address used to sign up for GitHub.\ngit config --global user.name \"Bruce Springsteen\"\ngit config --global user.email \"bruce@tulane.edu\"\nYou will not see any output when you enter these commands. To check that they worked, enter the following command at the Terminal:\ngit config --list\nYou should see your name and email address listed in the output."
  },
  {
    "objectID": "assignments/tutorial_1_old.html#set-up-rstudio-to-talk-to-github",
    "href": "assignments/tutorial_1_old.html#set-up-rstudio-to-talk-to-github",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Set up RStudio to talk to GitHub",
    "text": "Set up RStudio to talk to GitHub\nWe also need to set up RStudio to be able to communicate with GitHub securely. This requires a process that we luckily only have to do once. Basically, we need to get a secret code from GitHub and store it in RStudio (kind of like an app-specific password when you’re using two-factor authentication). We can start the process by entering the following code at the R Console (not the Terminal):\n\nusethis::create_github_token()\n\nThis will open a page on GitHub asking you to create a “Personal Access Token” or PAT (this is the secret code). You’ll have to give the PAT a note that describes what it’s for and choose an expiration date. To minimize problems throughout the semester, you should set it to a date after the semester ends such as 12/31/2024. Keep the “scope” selections as they are and click the “Generate Token” button at the bottom of the page.\n\nYou will then see a new screen with a long sequence of letters. This is your token or secret code. You should treat it like a password and do not share it with anyone. Copy this by hitting the button with the two boxes.\n\nOnce you have copied the PAT, call gitcreds::gitcreds_set() from the RStudio console and paste the PAT when prompted. You should see the following:\n&gt; gitcreds::gitcreds_set()\n? Enter password or token: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n-&gt; Adding new credentials...\n-&gt; Removing credentials from cache...\n-&gt; Done.\nOnce this is done, you should be all set for RStudio to communicate with GitHub. If you have any problems with the PAT process or want to know more about it, the Happy Git and GitHub for the useR book has a great chapter about it."
  },
  {
    "objectID": "assignments/tutorial_1_old.html#creating-your-first-repo",
    "href": "assignments/tutorial_1_old.html#creating-your-first-repo",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Creating your first repo",
    "text": "Creating your first repo\nOnce you are logged into Github and at its homepage, you can now create a new repo (shorthand for “repository”). These repositories are like folders in Dropbox except a bit more structured. Create one by clicking on the green “New” button in the top-left of the screen next to “Repositories.” It should look like this:\n\nNow a new screen should pop up requesting some information for the new repo. Give this new Repository the name hpam7660-tutorial1 and set it to be a private repository. You can give it an informative description and check the “Initialize this repository with a README” checkbox. This latter option will add a README to the repository where you can add more information that will display nicely in the repository’s homepage. Your setup should look something like this:\n\nOnce you create your repo, you are ready to connect it to RStudio on your local computer (this process is called “cloning”). The easiest way to do this is go to the repo homepage (you’re already there if you’ve just created it) and click on the green “Code” button toward the top right of the page. When you click that, a popup will appear and you can copy the URL (you can click the little clipboard icon to copy this automatically).\n\nNow, switch to RStudio. Go to the menu bar and hit “File &gt; New Project”. You can then choose what type of project to start. Since we’re importing from Github, we’ll use the “Version Control” option (it’s the bottom of the list).\n\nIn the next menu, choose “Git”.\n\nNow, you can paste the URL in the “Repository URL” box. I’ve noticed that the “Project Directory Name:” field and the “Create project as subdirectory of:” field are typically autofilled on a Mac, but not always on a PC. If those fields don’t autofill, add your repo name as the project directory name and a local folder path where you want to store R files as the subdirectory. Click “Create Project” and you’ll have your project ready to go in RStudio."
  },
  {
    "objectID": "assignments/tutorial_1_old.html#working-with-a-project-in-rstudio",
    "href": "assignments/tutorial_1_old.html#working-with-a-project-in-rstudio",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Working with a project in RStudio",
    "text": "Working with a project in RStudio\nYou’ll now see the files from your Github project in the bottom right window of RStudio.\n\nLet’s edit the README file by clicking on README.md in that file pane. You’ll be able to edit the file in the top-left pane.\n\nChange the header from # hpam7660-tutorial1 to # Introduction and save the file (⌘+S or Ctrl+S). If you click on the “Git” tab in the top right panel, you will a list of the changes you’ve made to the repo since the last commit (you can think of a commit as a more permanent type of saving work to the git repo).\n\nOne thing you’ll notice here is that git thinks that hpam7660-tutorial1.Rproj is a file that maybe should go into the repo. But this file is just for our local copy of RStudio and shouldn’t really go into the repo. To prevent git from bothering us about it every time we open something, we can modify the .gitignore file to tell git to ignore certain files. Open .gitignore and add a new line with *.Rproj on it to tell git to ignore any file with the extension .Rproj. Make sure to save the file.\n\nIf you go back to the Git tab in the top right and refresh (little circular arrow in the top right corner), you see that hpam7660-tutorial1.Rproj is removed from our list. Now we are ready to commit our changes. Click the “Staged” box for .gitignore and README.md and hit the “Commit” button just above the file list. You’ll see a window with the changes that you are about to commit. You can click on different files to see what exactly you are changing. Add a short but informative commit message that describes what you are committing and hit “Commit”. You’ll see a popup window that summarizes the changes. You can click “Close”.\n\nOnce this completes, you can hit the “Push” button in the top right to push that commit back to Github. You’ll see another popup indicating that the push was successful. You can click “Close”.\n\nIf you go back to your repo’s homepage on Github and refresh the page, you’ll see the updates to your README file and the new .gitignore.\n\nFinally, you’ll need to provide me access to this repo (and all future repos for class) so that I can grade your work. To do so, click on “Settings” on your repo homepage and then “Collaborators” at the top of the left panel. Click “Add people”, search for k-callison, and add me as a collaborator.\nYou’re done! You’ve just created your first repo. From now on, we’ll use repos for our tutorials and our assignment submissions."
  },
  {
    "objectID": "assignments/multi_assignment1.html",
    "href": "assignments/multi_assignment1.html",
    "title": "February 29th Assignments",
    "section": "",
    "text": "There are two assignments due on February 29th (Sorry!). One is the third data assignment, which you’ll submit through a GitHub repo like you have with the other data assignments. The second is the problem statement draft, which you’ll submit through Canvas by clicking on the link below.\nRemember, the problem statement should be one or two sentences that defines the problem you’ll address in the policy memo. See the slides from January 25th for tips on constructing your problem statement.\n\nData Assignment 3 - Data Wrangling, Part 2\nPolicy Memo Problem Statement"
  },
  {
    "objectID": "assignments/data_4.html",
    "href": "assignments/data_4.html",
    "title": "Data Assignment 4 - Data Visualization",
    "section": "",
    "text": "I updated this assignment on March 2nd at 2pm. There is no need to redo the assignment if you completed it before the update.\n\nInstructions\nComplete the following assignment based on examples from ModernDive Chapter 2 - Data Visualization. We’re going to do something a little different with this homework assignment than we’ve done in the past. You are not going to create a new GitHub repo for this assignment. Instead, once you’ve completed this assignment, you’ll push it to your hpam7660_Cancer_Alley GitHub repo.\nTo get started, first open the .Rproj file in your hpam7660_Cancer_Alley folder. Then open a new Markdown document and give it a YAML header that includes the title “HPAM 7660 Data Assignment 4”, your name, the date, and “pdf_document” as the output format.\nAs you answer each of the following questions, be sure to include your R code and associated output in your Markdown document. Additionally, add a line or two describing what you’re doing in each code chunk.\n\n\nSteps for Completing the Assignment\n\nDescribe the three essential components of data visualization. How are all three of these components incorporated in ModernDive Figure 2.1?\nFor this assignment, we’re going to use two data frames that we created during the last in-class tutorial. Those data frames were called parish_rates and cancer_alley_rates unless you chose different names. Whenever we use pre-created data frames in a Markdown document, we need to account for a technical issue that arises due to the Markdown knitting process. When you knit a Markdown document in R, the knit is actually executed in a new environment. Since you didn’t create the parish_rates or cancer_alley_rates data frames in this new environment, you’re likely to get an “object not found” error when you go to knit (this will happen even if those data frames are visible in your global environment). There are two solutions that we can use to get around this issue:\n\nWe could include the code to create the parish_rates and cancer_alley_rates data frames from the tutorial directly into our Markdown document. This code will run in the new environment and the document should knit properly.\nWe can use the save.image command in R to save the data frames in our global environment and then include R code in our Markdown document to load those saved data frames. For this solution, first make sure that the parish_rates and cancer_alley_rates data frames are visible in your global environment. If there are other data frames in the global environment in addition to these two, you can either leave them there or you can remove them with the rm(df) command where you replace df with the name of the data frame you’d like to remove. This is only really necessary if you want to limit the size of the file you’re about to save. Next, in the Console Window command line, type save.image (file = \"~/data_4.RData\") where you should replace the “~” with the path that you’d like to use to save the data. Probably best to go with the path that points to your hpam7660_Cancer_Alley folder to keep everything in the same place. Finally, when you write your first R code chunk for this assignment in Step 3, include the following command load(\"~/data_4.RData\") at the top. This will load the necessary data frames into the new environment and the document should knit properly. As an example, here’s the code I would put at the beginning of my Markdown document if I were completing this assignment:\n\n\n\n    load(\"~/Dropbox/Documents/Teaching Materials/Health Policy/GitHub Site/hpam7660-sp24/assignments/data_4.RData\")\n    library(dplyr)\n    library(knitr)\n    library(ggplot2)\n\nIf you decide to go this route, you should add data_4.RData to your .gitignore file or make sure your .gitignore file already includes .RData files. The data_4.RData file will be at least 20mb depending on what all data frames were loaded in your global environment when you saved it, and so pushing it to your GitHub repo may take take a while. Since there’s no reason to actually push this file to your repo, you can avoid doing so by adding it to the .gitignore file.\n\nUse ggplot from the ggplot2 package to create a scatterplot with county FIPS code on the x-axis and cancer mortality rates in 2019 on the y-axis using the parish_rates data frame. Remember that in the parish_rates data frame, county is called cntyrsd and the cancer mortality rate is cancer_rate_adj. And since that data frame includes multiple years of data, you’ll first need to restrict the year to 2019.\nNow create a linegraph that displays cancer mortality rates from 2005 through 2019 for both Cancer Alley parishes and the non-Cancer Alley parishes in Louisiana. To do so, you’ll need to use the cancer_alley_rates data frame we created during the last in-class tutorial that aggregated mortality rates into two groups: Cancer Alley parishes and non-Cancer Alley parishes. Remember that in the cancer_alley_rates data frame, the Cancer Alley parish identifier is called cancer_parish and is equal to 1 for Cancer Alley parishes and is equal to 0 for non-Cancer Alley parishes. The age-adjusted, population weighted cancer mortality rate is called cancer_rate_adj_wt.\n\nThere’s a good chance that you’ll run into the following error if you follow the example code in the ModernDive chapter:\n\nError in `geom_line()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `scale_f()`:\n! A continuous variable cannot be mapped to the linetype aesthetic\nℹ choose a different aesthetic or use `scale_linetype_binned()`\n\nThis error happens because R thinks that the variable cancer_parish is a continuous variable rather than an indicator for whether a specific parish is in Cancer Alley. We need to tell R that the cancer_parish variable is an indicator rather than a continuous variable and we can do that by running the following line of code where df represents whatever name you gave to the data frame that will be used to create the linegraph (this may be cancer_alley_rates or it may be another name if you’ve already created a new data frame from cancer_alley_rates):\n\ndf$cancer_parish &lt;- factor(df$cancer_parish)\n\nThis code tells R that the variable cancer_parish in the df data frame should be interpreted as a factor variable. Once we run this code, we should avoid triggering the error code above.\nYour graph should look like the graph below:\n\n\nWe can use the scale_y_continuous option to change the y-axis values. Let’s change the y-axis scale so that it ranges from 0 to 300. The code to do so is scale_y_continuous(limits = c(0, 300)). This tells R that the y-axis scale is continuous and it ranges from a low of 0 to a high of 300. Remember that each time you add an option to your ggplot command, you’ll want to add a + sign before the new option. Your graph should now look like this:\n\n\n\nThis is looking better, but let’s make a few more adjustments. First, let’s fix the values on the x-axis so that it displays all the years in our data. This is similar to the way we changed the y-axis values in the last step with one slight difference. We want to make sure R includes all values of year between 2005 and 2019, so we use the following ggplot option scale_x_continuous(breaks = seq(2005, 2019, by = 1)).\n\nThis tells R that the x-axis scale is continuous and it contains sequential values of year from 2005 through 2019. Because we’re dealing with a lot of years on the x-axis, the values are going to get crowded. To alleviate that crowding, we can use the theme option to tell R to rotate the year labels by 45 degrees. The full scale_x_continuous and theme code is as follows:\n\nscale_x_continuous(breaks = seq(2005, 2019, by = 1)) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nHere, the theme option is telling R to rotate the x-axis labels by 45 degrees and adjust the horizontal alignment so that the labels don’t overlap. To see what I mean, you can drop the theme option and you’ll end up with a jumbled x-axis.\nYour modified graph should look like this:\n\n\nNow let’s change the legend so that it clearly indicates which line represents the Cancer Alley parishes. Here, we’ll use the scale_linetype_discrete option with the following arguments:\n\n\nscale_linetype_discrete(name = NULL, labels = c(\"Rest of Louisiana\",\"Cancer Alley\"), guide = guide_legend(reverse = TRUE)) \n\nThis tells R that we want to remove the legend title, label the lines “Rest of Louisiana” and “Cancer Alley”, and then switch their ordering in the legend so that “Cancer Alley” comes first. Add this to your ggplot command to produce a graph that looks like this:\n\n\nFinally, let’s title the chart “Cancer Mortality Rate Comparison”, title the y-axis “Cancer Deaths per 100,000 Residents”, and remove the x-axis title since it’s obvious the x-axis scale is in years. We can do this with the labs option as follows:\n\n\nlabs(title = \"Cancer Mortality Rate Comparison\", y = \"Cancer Deaths per 100,000 Residents\", x = NULL) \n\nAnd the output should look like this:\n\nNow we have a pretty good linegraph of cancer rates over time for Cancer Alley and non-Cancer Alley parishes in Louisiana. Perhaps surprisingly, cancer mortality rates are higher in non-Cancer Alley parishes. However, we want to be careful drawing any firm conclusions from this chart since we’re still going to make a few more adjustments to the data at a later date.\nIt turns out that the NIH and CDC have collected data on age-adjusted cancer rates over time for all states. Take a look at this chart for Louisiana and, despite the fact that it’s not broken out by Cancer Alley parishes, you’ll see that our linegraph looks very similar to this one from 2005 through 2019. There’s been a pretty clear decline in cancer mortality in Louisiana over the past 20 plus years and our linegraph indicates that this decline has occurred for both residents of Cancer Alley and non-Cancer Alley parishes.\n\nBefore we finish, let’s look at one more commonly used data visualization - the barchart. We’ll use the barchart to visualize differences in cancer mortality rates across Cancer Alley parishes. First, create a barchart that plots average cancer mortality rates for Cancer Alley parishes (and only Cancer Alley Parishes) between 2005 and 2019. To be clear, you’ll have one value for each Cancer Alley parish and that value will be the average of the annual cancer mortality rates between 2005 and 2019.\n\nYou’ll also need to tell R that county should be treated as a factor variable and not a continuous variable (just like we did with cancer_parish above). Otherwise, you’ll get all values between 0 and 125 on the x-axis, which will create a bunch of empty space. To tell R that county should be treated as a factor variable, you can use this line of code df$county &lt;- factor(df$county) where the df represents whatever name you gave to the data frame that will be used to create the barchart. Your barchart should look like this:\n\n\nNow let’s clean up this barchart the way we did with the linegraph. Change the y-scale so that it ranges from 0 to 300, re-title the y-axis “Cancer Deaths per 100,000 Residents” and the x-axis “County”, and give the chart the title “Average Cancer Alley Mortality Rates by County, 2005-2019”. Your modified barchart should look like this:\n\n\n\nThis looks good, but it would be more helpful if the x-axis labels were the parish names instead of the parish FIPS codes. Add the following option to make this change:\n\n\nscale_x_discrete(labels = c(\"5\" = \"Acension\",\n                              \"33\" = \"East Baton Rouge\",\n                              \"47\" = \"Iberville\",\n                              \"51\" = \"Jefferson\",\n                              \"71\" = \"Orleans\",\n                              \"89\" = \"St. Charles\",\n                              \"93\" = \"St. James\",\n                              \"95\" = \"St. John the Baptist\",\n                              \"121\" = \"West Baton Rouge\")) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nThis code does two things. First, it tells R to replace the county FIPS code labels with parish names on the x-axis. Second it tells R to rotate these x-axis labels by 45 degrees and adjust the horizontal alignment so that the names don’t overlap. Your barchart should now look like this:\n\n\nOnce you’ve finished Step 11, knit your PDF document and push it to your GitHub repo. Make sure the document shows up in the repo, invite me to the repo, and you’re done!"
  },
  {
    "objectID": "assignments/data_2.html",
    "href": "assignments/data_2.html",
    "title": "Data Assignment 2 - Data Wrangling",
    "section": "",
    "text": "Instructions\nComplete the following examples from ModernDive Chapter 3 - Data, Sections 3.1 through 3.4. Before beginning the assignment, create a GitHub repo called hpam7660_data2. Then create a new RStudio project and link it to the new GitHub repo. Once you’ve done that, open a new Markdown document and give it a YAML header that includes the title “HPAM 7660 Data Assignment 2”, your name, the date, and “pdf_document” as the output format.\nAs you answer each of the following questions, be sure to include your R code and associated output in your Markdown document. Additionally, add a line or two describing what you’re doing in each code chunk.\n\n\nSteps for Completing the Assignment\n\nInstall and load the following packages: dplyr and nycflights13.\nThe pipe operator %&gt;% included in the dplyr package allows us to combine multiple operations in R into a single sequential chain of actions. You can think of the pipe operator as a way to streamline and simplify your code. You might remember that you’ve encountered this pipe operator in Data Assignment 1 when transforming data into “tidy” format. Let’s take a closer look at how the pipe operator works.\n\nWe’ll start by creating a new data frame called united_flights that filters the nycflights13 data so that it only contains United Airlines flights. We’ve done something similar before using the following code:\nunited_flights &lt;- filter(flights, carrier == \"UA\")\nWrite a code chunk in your Markdownn document that does the same thing as the code snippet above, but uses the pipe operator, %&gt;%.\n\nLet’s add another restriction to the filter command. We’ll continue to focus on United Airlines flights, but now use the origin variable to restrict the sample to United Airlines flights that departed from LaGuardia Airport (airport code “LGA”).\n\nFirst, write a code chunk that accomplishes this by adding a second argument to your original filter command.\nSecond, write a code chunk that accomplishes this by adding a second filter command and uses the pipe operator.\nThird, show that both options create identical data frames that contain the same number of rows.\n\nSuppose we’re interested in United Airlines flights departing LaGuardia and arriving in either Chicago (“ORD”) or Denver (“DEN”). Write a code chunk that adds this restriction. You can combine all of this into one filter command or use multiple filter commands with the pipe operator.\nNow let’s expand our list of destination airports to include Houston (“IAH”) and Cleveland (“CLE”). We could add each of these individually into our filter command along with “ORD” and “DEN”, but it’s usually preferable to create a vector of list elements and refer to this vector in the filter command.\n\nWrite a code chunk that uses the %in% operator along with the c() function to revise your data frame so that it includes United Airlines flights departing from LaGuardia and arriving in Chicago, Denver, Houston, or Cleveland.\n\nWe’re going to focus on summarizing data next week, but we’ll briefly cover some of the basics now. First, write a code chunk that uses the summarize function to generate the mean and standard deviation of the “arr_delay” variable in the flights data frame. Be sure to account for any missing observations in your code!\n\nNow use the kable() function to add a table to the Markdown document that contains the mean and standard deviation values that you just generated.\n\nSuppose we also want our flight delay summary statistics table to include the total number of observations in the data. Use the n() function to add the number of observations to the table.\nNow let’s look to see what months tend to experience the longest delays. Use the group_by function to generate a data frame that contains the mean, standard deviation, and number of monthly observations of flight delays. Add a table using the kable() function to your Markdown document that summarizes this information. Which month typically sees the longest average delay? Which month typically sees the shortest average delay?\nWe can also group by more than one variable if we’d like. Create another version of the previous table that displays the average delay each month across all three New York airports (“origin”) using the group_by function.\nOnce you’ve finished Step 9, knit your PDF document and push it to your GitHub repo. Make sure the document shows up in the repo, invite me to the repo, and you’re done!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About HPAM 7660",
    "section": "",
    "text": "Course Description\nThe primary aim of this course is to present an overview of health policy in American government, its scope, dynamics, and conceptual and practical dilemmas. It is designed to acquaint students with major issues involved in formulating, implementing, and assessing patterns of decisions established by government. Because the study of policy is essentially interdisciplinary, readings for the course have been drawn from several fields, including sociology, political science, and economics. Specific areas of consideration will be addressed during the seminar through analysis and discussion of the functions of state and local government and various stakeholder groups that attempt to influence governmental action.\n\n\nStudent Learning Objectives\nAfter completing this course, students will be able to do the following: 1. Exhibit a conceptual understanding of the forces and factors accounting for legislation and regulation pertaining to health services.  2. Develop a framework to critically discuss and evaluate policy issues affecting the U.S. health care system.  3. Compare and contrast the public policy objectives of various stakeholders with the realities of the political process.  4. Critically evaluate the research literature that examines the federal and state roles in health services.  5. Evaluate current proposals advanced in Congress and by the private sector that impact health services delivery.  6. Demonstrate a working understanding of the legislative process and the related federal and state regulation processes. \n\n\nCourse Instructors\n\n\n\n\n\n\n   Kevin Callison, PhD\n   Tidewater 1915\n   kcallson@tulane.edu\n   Webpage\n\n\n\n\n\n\n\n\n\n   Chip Kahn, CEO Federation of American Hospitals\n   CKahn@FAH.org\n   Webpage\n\n\n\n\n\nCourse Website\nThe formatting of this website and much of the content is derived from Matt Blackwell’s Gov 50 course at Harvard University, which he has generously made available for use."
  },
  {
    "objectID": "assignments/data_1.html",
    "href": "assignments/data_1.html",
    "title": "Data Assignment 1 - Importing Data",
    "section": "",
    "text": "Instructions\nComplete the following examples from ModernDive Chapter 4 - Data Importing and Tidy Data. Before beginning the assignment, create a GitHub repo called hpam7660_data1. Then create a new RStudio project and link it to the new GitHub repo. Once you’ve done that, open a new Markdown document and give it a YAML header that includes the title “HPAM 7660 Data Assignment 1”, your name, the date, and “pdf_document” as the output format.\nAs you answer each of the following questions, be sure to include your R code and associated output in your Markdown document. Additionally, add a line or two describing what you’re doing in each code chunk.\n\n\nSteps for Completing the Assignment\n\nLoad the following packages: dplyr, readr, tidyr, nycflights13, and fivethirtyeight. You may need to install these packages before loading if you haven’t done so already. Note, however, that it can cause problems to include install.packages commands in Markdown documents. So when creating your Markdown file, you’re fine to add library commands, but you should install any needed packages from the Console command line.\nPreview the drinks data frame from the fivethirtyeight package using one of the methods we covered in Tutorial 2. (Hint: you should avoid using the View() command here because Markdown won’t like it.)\nUsing the help file for the drinks data frame, define each of the variables in the data.\nWhat does it mean for a dataset in R to by “tidy” and how does this relate to the concept of “wide” vs. “long” data?\nFollow the example provided in ModernDive Section 4.2 and create a new data frame called drinks_smaller that subsets the drinks data frame and:\n\nIncludes only data from the U.S., China, Italy, and Saudi Arabia.\nExcludes the total_litres_of_pure_alcohol column.\nRenames the variables “beer_servings”, “spirit_servings”, and “wine_servings” to “beer”, “spirits”, and “wine”.\n\nIs the drinks_smaller data frame in “tidy” format, why or why not?\nConvert the drinks_smaller data frame to “tidy” format. Be sure to describe the commands you’re using for this conversion.\nPreview the new tidy version of the drinks_smaller data frame.\nNow do the same conversion and preview for the airline_safety data frame from the fivethirtyeight package. (Hint: First you’ll need to get rid of some columns in the data frame (there’s an example in the chapter) and then you’ll want the tidy data frame to include the variable fatalities_years that indicates the time period and the variable count that measures the fatality counts).\n\nThus far, we’ve only loaded data frames that have come bundled in R packages. Oftentimes, you’ll want to load data from other sources (e.g., websites, surveys, etc.) and in other formats (e.g., .csv, .xlsx, etc.). Let’s work through an example of loading data from a .csv file.\n\nLoad the dem_score.csv file from this link and save it in a data frame called dem_score. (Hint: refer to ModernDive Chapter 4.1.1 for code on importing a .csv file).\nPreview the data using one of the methods we covered in Tutorial 2. (Hint: you should avoid using the View() command here because Markdown won’t like it.)\nIs this data frame in “tidy” format? If yes, then explain why. If not, then convert the data to “tiny” format.\nOnce you’ve finished Step 12, knit your PDF document and push it to your GitHub repo. Make sure the document shows up in the repo, invite me to the repo, and you’re done!"
  },
  {
    "objectID": "assignments/data_3.html",
    "href": "assignments/data_3.html",
    "title": "Data Assignment 3 - Data Wrangling, Part 2",
    "section": "",
    "text": "Instructions\nComplete the following examples from ModernDive Chapter 3 - Data Wrangling, Sections 3.5 through 3.9. Before beginning the assignment, create a GitHub repo called hpam7660_data3. Then create a new RStudio project and link it to the new GitHub repo. Once you’ve done that, open a new Markdown document and give it a YAML header that includes the title “HPAM 7660 Data Assignment 3”, your name, the date, and “pdf_document” as the output format.\nAs you answer each of the following questions, be sure to include your R code and associated output in your Markdown document. Additionally, add a line or two describing what you’re doing in each code chunk.\n\n\nSteps for Completing the Assignment\n\nLoad the following packages: dplyr, knitr, and nycflights13.\nUse the mutate function along with the air_time and distance variables in the flights data frame to create a new variable called avg_speed that measures a flight’s average air speed in miles per hour. (Hint: You need to be careful here because air_time is measured in minutes and not hours.)\nNow suppose we want to calculate average air speeds by carrier. Use the group_by and summarize commands along with the kable command to create a table of carrier-specific average air speeds. (Hint: be careful of missing values when calculating averages.)\nWe’re primarily interested in average air speeds, but it might also be helpful in some cases to include additional summary statistics in a data table. Add the standard deviation, the minimum and maximum values, and the number of carrier observations to your table.\nNow sort the data by average air speed and recreate your table so that the carriers are listed in descending order of average air speed.\nThis is great, but the carrier abbreviations might be difficult for some people to understand. Use the join command and the carrier names found in the airlines data frame to replace the carrier abbreviations with carrier names in your table. Rename the column that contains carrier names “airline”.\nNow suppose we’re interested in the relationship between average air speed and humidity. The data frame weather includes a variable named humid that lists the humidity at the origin airport for each hour of every day. Join the flights and weather data frames and re-make your table so that it now contains a column that contains the average humidity experienced by each airline. (Hint: you will need to use multiple key variables in your join statement. See ModernDive Section 3.7.3 for an example.)\nFinally, use the select command to reorder your table columns so that they are in the following order: airline, mean_speed, sd_speed, min_speed, max_speed, mean_humidity.\nOnce you’ve finished Step 8, knit your PDF document and push it to your GitHub repo. Make sure the document shows up in the repo, invite me to the repo, and you’re done!"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Having problems: check our Troubleshooting Guide",
    "crumbs": [
      "Resources",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#tutorials",
    "href": "assignments/index.html#tutorials",
    "title": "Assignments",
    "section": "Tutorials",
    "text": "Tutorials\n\nTutorial Instructions\nTutorial 1: Data Visualization\nTutorial 2: Data Wrangling\nTutorial 3: Causality\nTutorial 4: Summarizing Data\nTutorial 5: Correlation and Tidying Data\nTutorial 6: Loops\nTutorial 7: Regression and Sampling\nTutorial 8: Bootstrap",
    "crumbs": [
      "Resources",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#problem-sets",
    "href": "assignments/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\n\nSubmission guide for problem sets\nProblem set 0: Setting up R, RStudio, and GitHub\nProblem set 1: Data visualization\nProblem set 2: Data wrangling\nProblem set 3: Causality\nProblem set 4: Summarizing data\nProblem set 5: Regression\nProblem set 6: Sampling\nProblem set 7: Bootstrap\nProblem set 8: Hypothesis testing",
    "crumbs": [
      "Resources",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#final-project",
    "href": "assignments/index.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\n\nFinal project information\nFinal project milestone 1: Creating a repo",
    "crumbs": [
      "Resources",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/tutorial_1.html",
    "href": "assignments/tutorial_1.html",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "",
    "text": "In this tutorial, we’re going to get R, RStudio, and R Markdown set up on your computer. It’s important that you follow these steps in order. You may experience errors if you try go out of order. To get started:\n\nDownload and install the most recent version of R. There are versions available for the Windows, Mac, and Linux operating systems. On a Windows computer, you will want to install using the R-x.y.z-win.exe file where x.y.z is a version number. On a Mac, you will want to install using the R-x.y.z.pkg file that is notarized and signed.\nWith R installed, download and install RStudio. RStudio is a type of “integrated development environment” or IDE designed for R. It makes working with R considerably easier and is available for most platforms. It is also free.\nInstall the packages we will use throughout the semester. To do this, either type or copy and paste each of the following lines of code into the “Console” in RStudio (lower left panel by default). Make sure you do this separately for each line. If you are asked if you want to install any packages from source, type “no”. Note that the symbols next to my_package are a less than sign &lt; followed by a minus sign - with no space between them. (Don’t be worried if you see some red text here. Those are usually just messages telling you information about the packages you are installing. Unless you see the word Error you should be fine.)\n\n\nmy_packages &lt;- c(\"tidyverse\", \"usethis\", \"devtools\", \"learnr\",\n                 \"tinytex\", \"gitcreds\", \"gapminder\")\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\n\nFor some things in the course, we’ll need produce PDFs from R and that requires something called LaTeX. If you’ve never heard of that, it’s completely fine and you should just run the following two lines of R code:\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX"
  },
  {
    "objectID": "assignments/tutorial_1.html#installing-r-and-rstudio",
    "href": "assignments/tutorial_1.html#installing-r-and-rstudio",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "",
    "text": "In this tutorial, we’re going to get R, RStudio, and R Markdown set up on your computer. It’s important that you follow these steps in order. You may experience errors if you try go out of order. To get started:\n\nDownload and install the most recent version of R. There are versions available for the Windows, Mac, and Linux operating systems. On a Windows computer, you will want to install using the R-x.y.z-win.exe file where x.y.z is a version number. On a Mac, you will want to install using the R-x.y.z.pkg file that is notarized and signed.\nWith R installed, download and install RStudio. RStudio is a type of “integrated development environment” or IDE designed for R. It makes working with R considerably easier and is available for most platforms. It is also free.\nInstall the packages we will use throughout the semester. To do this, either type or copy and paste each of the following lines of code into the “Console” in RStudio (lower left panel by default). Make sure you do this separately for each line. If you are asked if you want to install any packages from source, type “no”. Note that the symbols next to my_package are a less than sign &lt; followed by a minus sign - with no space between them. (Don’t be worried if you see some red text here. Those are usually just messages telling you information about the packages you are installing. Unless you see the word Error you should be fine.)\n\n\nmy_packages &lt;- c(\"tidyverse\", \"usethis\", \"devtools\", \"learnr\",\n                 \"tinytex\", \"gitcreds\", \"gapminder\")\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\n\nFor some things in the course, we’ll need produce PDFs from R and that requires something called LaTeX. If you’ve never heard of that, it’s completely fine and you should just run the following two lines of R code:\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX"
  },
  {
    "objectID": "assignments/tutorial_1.html#testing-your-setup",
    "href": "assignments/tutorial_1.html#testing-your-setup",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Testing Your Setup",
    "text": "Testing Your Setup\nTo verify that everything is working correctly, try running this code RStudio’s “Console” window:\n\nlibrary(tidyverse)\n\nIf you don’t see an error message, then your setup is complete!"
  },
  {
    "objectID": "assignments/tutorial_1.html#next-steps",
    "href": "assignments/tutorial_1.html#next-steps",
    "title": "Tutorial 1 - Getting Started with R, RStudio, Git, and GitHub Tutorial",
    "section": "Next Steps",
    "text": "Next Steps\nIn the next tutorial, we’ll start working with the Behavioral Risk Factor Surveillance System (BRFSS) data. Before then:\n\nMake sure all R packages are installed successfully.\nTry opening and closing RStudio to ensure it loads properly.\nFamiliarize yourself with the RStudio interface. Here’s a quick guide that can help."
  },
  {
    "objectID": "assignments/tutorial_2.html",
    "href": "assignments/tutorial_2.html",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "",
    "text": "In this tutorial, we’ll learn how to write R code using an R Script and using an R Markdown document. You can think of an R Script as a file where you store your R commands so that you don’t have to manually type each command into the Console’s command line (this would get really confusing for large projects!). You can think of an R Markdown document as a combination text editor (like Microsoft Word) and R Script. We’ll see that using an R Markdown document can be a good way to organize our R code and the output that code generates.\nWe’ll work through this tutorial in class using the examples in ModernDive Chapter 1 and you’ll create additional R Markdown documents for other tutorials and homework assignments throughout the semester."
  },
  {
    "objectID": "assignments/tutorial_2.html#navigating-rstudio",
    "href": "assignments/tutorial_2.html#navigating-rstudio",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "",
    "text": "In this tutorial, we’ll learn how to write R code using an R Script and using an R Markdown document. You can think of an R Script as a file where you store your R commands so that you don’t have to manually type each command into the Console’s command line (this would get really confusing for large projects!). You can think of an R Markdown document as a combination text editor (like Microsoft Word) and R Script. We’ll see that using an R Markdown document can be a good way to organize our R code and the output that code generates.\nWe’ll work through this tutorial in class using the examples in ModernDive Chapter 1 and you’ll create additional R Markdown documents for other tutorials and homework assignments throughout the semester."
  },
  {
    "objectID": "assignments/tutorial_2.html#create-a-github-repository-for-tutorial-2",
    "href": "assignments/tutorial_2.html#create-a-github-repository-for-tutorial-2",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "Create a GitHub Repository for Tutorial 2",
    "text": "Create a GitHub Repository for Tutorial 2\nBefore we start with our coding exercises, we’ll want to create a new GitHub repository for Tutorial 2. Follow the instructions listed in Tutorial 1 to create a new GitHub repository called hpam7660-tutorial2."
  },
  {
    "objectID": "assignments/tutorial_2.html#open-a-new-r-project",
    "href": "assignments/tutorial_2.html#open-a-new-r-project",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "Open a new R Project",
    "text": "Open a new R Project\nNavigate to the File -&gt; New Project tab in RStudio. Let’s call this project “Tutorial 2”. Follow the instructions from Tutorial 1 for linking the new project to your newly created GitHub repo."
  },
  {
    "objectID": "assignments/tutorial_2.html#rstudio-window",
    "href": "assignments/tutorial_2.html#rstudio-window",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "RStudio Window",
    "text": "RStudio Window\nLet’s take a minute to briefly go over the RStudio interface. When you open RStudio, you’ll typically see the following three windows.\n\nThere’s a lot more here, but we’ll cover other aspects of RStudio as they come up in our tutorials and problem sets."
  },
  {
    "objectID": "assignments/tutorial_2.html#console-command-line",
    "href": "assignments/tutorial_2.html#console-command-line",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "Console Command Line",
    "text": "Console Command Line\nNow that we have a new project linked to the proper GitHub repo, let’s start by running a few commands in the RStudio Console Window command line. This is the window at the bottom of RStudio (make sure the “Console” tab is selected). Throughout this tutorial, we’ll be working through the examples in Chapter 1.4 of ModernDive.\nFirst we need to install and load the packages we’ll need for the tutorial (you may already have some of these packages installed, but it won’t hurt to reinstall them). Type each of the following commands in to the RStudio command line:\n\ninstall.packages(\"nycflights13\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"knitr\")\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(knitr)\n\n\nExploring Data Frames\nNow that we’ve loaded the packages and libraries, let’s take a look at the data objects contained in nycflights13. To do so, click on the dropdown icon next to “Global Environment” and select “package:nycflights13”.\n\nYou should now see a series of objects in the “Environment” window.\n\nLet’s take a look at each of these objects to see what information they contain. Type the following code into the Console Window command line and hit enter:\n\nflights\n\nYou should now see a “tibble” that displays the first 8 columns and the first 10 rows of the flights data frame. The tibble also tells us that the data frame contains 336,766 more rows (beyond those displayed in the Console Window) and 11 more variables/columns (each of which is listed).\nThere are several other ways that we could examine the flights data frame including:\n\nUsing the View() function to open RStudio’s built-in data viewer. This allows us to scroll through all columns and rows in the data frame.\nUsing the glimpse function, which is part of the dplyr package. This is similar to the tibble that we saw by simply typing flights into the console, but also includes each variables data type (e.g., integer, double, character, etc.)\nUsing the kable() function, which is part of the knitr package. This function is helpful when we want to generate formatted output in an RMarkdown document, but it’s important to note that kable will print the entire data frame by default.\n\nLet’s try some of these out. Use the glimpse function to take a look at the flights data frame and the kable function to examine the airlines data frame.\n\n\nData Manipulation\nNow let’s make some manipulations to the data. Suppose we want a list of flight numbers and delay times for each United Airlines flight that was delayed by 4 hours or longer in January 2013. First, we can subset the data so that we only retain United Airlines flights. We can do this using the carrier variable since the code “UA” in this field identifies United Airlines flights. Go ahead and type the following into the Console command line and hit enter:\n\nua_flights &lt;- filter(flights, carrier == \"UA\")\n\nNotice the new data object in the “Environment” tab called “ua_flights”. Let’s take a look at it using one of the viewing methods above (use whichever you prefer).\nNow we need to determine which UA flights in January 2013 were delayed by 4 hours or longer. We can do another subset of the data using the year, month, and arr_delay variables (notice that the arr_delay variable is measured in minutes). Type the following into the Console command line and hit enter:\n\nua_delay &lt;- filter(ua_flights, year == 2013, month == 1, arr_delay &gt;= 240)\n\nNow we have a data frame that includes all UA flights that were delayed by 4 hours or longer in January 2013. Notice that we didn’t need a separate filter command for each argument (i.e., year, month, and arr_delay), we simply combined them all into a single filter command.\nFinally, since we’re really only interested in the flight numbers and delay times, we can get rid of the extraneous variables in the data frame using the select command. Also,\n\nua_final &lt;- select(ua_delay, flight, arr_delay)\n\nNow that we have our final dataframe, let’s take a look at the table of flight delays using the kable function:\n\nkable(ua_final)"
  },
  {
    "objectID": "assignments/tutorial_2.html#using-r-scripts",
    "href": "assignments/tutorial_2.html#using-r-scripts",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "Using R Scripts",
    "text": "Using R Scripts\nSo far so good, but what happens if we were to close out of RStudio? Or, say 6 months from now, we decide we want to re-generate the table we just made? Entering commands directly into the Console Window command line is not a very good way to structure our workflow because once those comments are gone, they’re gone for good! A better way to track our work and ensure that we can reproduce our results if necessary is to use an R script file to write our code. Let’s take a look.\nNavigate to the File -&gt; New File -&gt; R Script tab in RStudio. This should open a new window in RStudio called the “Source” window and a document called “Untitled1” where you can write and save your R code.\n\nLet’s give this file a name and then recreate our United Airlines flight delay table by writing the code we entered directly into the command line into our R Script.\nTo name the file you can click on the save icon in the menu bar or navigate to File -&gt; Save. Either way, you should be prompted to name the file (which will have a .R extension) and choose a location for saving. Let’s call this file Tutorial_2 and save it in the folder you created for your new hpam7660-tutorial2 GitHub repo. Notice that if you click on the “Git” tab in the upper righthand window, you should see this new R script ready to be staged, committed, and pushed to the GitHub repo.\nNow type the code below into the R script (note that I have combined a couple of lines from the previous version):\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(knitr)\n\nua_delay &lt;- filter(flights, carrier == \"UA\", year == 2013, month == 1, arr_delay &gt;= 240)\nua_final &lt;- select(ua_delay, flight, arr_delay)\nkable(ua_final)\n\nOnce you’ve typed this into your R Script, save it, and click on the “Run” button in the menu bar. You should see the code run and the output table displayed in the Console window. Now if you exit RStudio, you’ll have the script file that you can run at a later date and reproduce the exact same table. Further, once you push the script file to GitHub, you can share it with other collaborators who may be working with you on the project.\nWe’ll push the R script to GitHub in a few minutes, but first let’s try using an R Markdown document to track our coding and it’s associated output."
  },
  {
    "objectID": "assignments/tutorial_2.html#using-r-markdown-documents",
    "href": "assignments/tutorial_2.html#using-r-markdown-documents",
    "title": "Tutorial 2 - Navigating RStudio Tutorial: Using R Scripts and R Markdown Files",
    "section": "Using R Markdown Documents",
    "text": "Using R Markdown Documents\nTo open a new Markdown document navigate to File -&gt; New File -&gt; R Markdown. You should see a popup box with a button in the lower left corner that says “Create Empty Document”. Click that button. We could choose another formatting option, but RStudio will include a bunch of stuff that we don’t need in the final document. You should now see an RMD tab called “Untitled1” at the top of the Source Window. Click over to that tab.\nMarkdown documents always begin with something called a YAML header. The YAML header allows us to add things like a title, author name, date, etc. to the top of our output document. Add the following code (using your name) to the top of your Markdown document:\n\n---\ntitle: \"Tutorial 2\"\nauthor: \"Kevin Callison\"\ndate: \"February 1, 2024\"\noutput: pdf_document\n---\n\nNote that we also tell Markdown the type of output document we want in the YAML header. In this case, we’ll go with a PDF, but other options include an html_document or a word_document.\nNow let’s add the same code we used in our R Script file:\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(knitr)\n\nua_delay &lt;- filter(flights, carrier == \"UA\", year == 2013, month == 1, arr_delay &gt;= 240)\nua_final &lt;- select(ua_delay, flight, arr_delay)\nkable(ua_final)\n\nTo generate the output PDF file, click on the “Knit” button.\n\nYou should see a message in the “Render” tab telling you that the output has been created and now see a PDF file that includes the title, your name, the date, and the R code that you typed. Something like this:\n\nThis is good, but we can do better. For one thing, Markdown doesn’t recognize the R code we included as code - it just sees it as text. But we can tell Markdown that it is R code by adding a code chunk by adding chunk delimiters at the beginning and end of the code:\n\nNow the PDF file contains our R code in nice grey blocks, along with any messages that R generates. More importantly, the PDF file also includes the output generated by the R commands.\n\nIt’s always good practice to annotate our code so that others know what we’re doing and why we’re doing it. Add some text to your Markdown files like I’ve done below:\n\nYour final result should look something like this:\n\nWe’re almost done, but we need to push our Markdown file to GitHub before we wrap up. If you click the “Git” tab in the upper right window, you should see the Markdown file listed. Now follow the same process we used to push our project files in Tutorial 1:\n\nCheck the “Staged” box.\nClick the “Commit” button.\nAdd a commit message. Maybe something like “Add Tutorial 2 Markdown File”.\nClick the “Commit” button.\nClose the Git Commit text box popup.\nClick the “Push” button.\nClose the Git Push popup.\n\nGo to your GitHub page and make sure the file shows up in your hpam7660-tutorial2 repo. Invite me to the repo.\nCongratulations! You made it all the way through!"
  },
  {
    "objectID": "assignments/tutorial_4.html",
    "href": "assignments/tutorial_4.html",
    "title": "Tutorial 4 - Data Description Tutorial",
    "section": "",
    "text": "In this tutorial, we’ll write code to import the Louisiana Mortality File (we already did this in Tutorial 3) and identify each of the variables included in the data. It’s important to know what each variable is measuring before we can create an analytic file for analysis.\nBefore beginning the tutorial, create a GitHub repo called hpam7660_Cancer_Alley. Then create a new RStudio project and link it to the new GitHub repo. Once you’ve done that, open a new Markdown document and give it a YAML header that includes the title “Louisiana Mortality Data Description”, your name, the date, and “pdf_document” as the output format.\n\n\nWrite some code that uses the read_csv command to load the LA Mortality File into RStudio and then create a list of each data element (variable name) in the file.\n\n\n\nEach year of the data has its own associated data documentation on the CDC site. We could go through the documentation file for each year, but it turns out that the data fields don’t change much from year to year. So, for our purposes, we can use the 2019 data documentation file to describe the data set. Open the documentation file for the 2019 Mortality Multiple Cause-of-Death Files.\nEach variable in the LA Mortality File has a corresponding entry in the documentation file. For example, the first variable listed when you preview the data frame should be “restatus”. If you search through the documentation file for “restatus”, you won’t find anything. However, if you search instead for “status”, the first result should be a data item called “resident status”.\nSounds like “resident status” is probably a good fit for our “restatus” variable. The definition for the “resident status” variable is included on the third page of the documentation file.\n\nYou can see that this variable provides some information on whether the place where a death occurred is also that person’s place of residence (note that we’re only interested in United States Occurence for our purposes). So, on your Markdown document, begin your data element list as follows:\n\nrestatus, Resident Status\n\nState and county of Occurrence and Residence are the same.\nState of Occurrence and Residence are the same, but County is different.\nState of Occurrence and Residence are different, but both are in the U.S.\nState of Occurrence is one of the 50 States or D.C., but Place of Residence is outside the U.S.\n\n\nHint: to italicize a word in a Markdown document write it like this: *restatus*\nContinue to build your data element list by adding each variable in the data set and its corresponding definition from the documentation file. Note that for some variables, the descriptions and code values will be lengthy, so you’ll have to abbreviate.\nOnce you’re finished, save the Markdown document as Cancer Alley Data Description, push it to your hpam7660_Cancer_Alley repo, invite me to the repo, and you’re done!"
  },
  {
    "objectID": "assignments/tutorial_4.html#importing-and-describing-the-louisiana-mortality-files",
    "href": "assignments/tutorial_4.html#importing-and-describing-the-louisiana-mortality-files",
    "title": "Tutorial 4 - Data Description Tutorial",
    "section": "",
    "text": "In this tutorial, we’ll write code to import the Louisiana Mortality File (we already did this in Tutorial 3) and identify each of the variables included in the data. It’s important to know what each variable is measuring before we can create an analytic file for analysis.\nBefore beginning the tutorial, create a GitHub repo called hpam7660_Cancer_Alley. Then create a new RStudio project and link it to the new GitHub repo. Once you’ve done that, open a new Markdown document and give it a YAML header that includes the title “Louisiana Mortality Data Description”, your name, the date, and “pdf_document” as the output format.\n\n\nWrite some code that uses the read_csv command to load the LA Mortality File into RStudio and then create a list of each data element (variable name) in the file.\n\n\n\nEach year of the data has its own associated data documentation on the CDC site. We could go through the documentation file for each year, but it turns out that the data fields don’t change much from year to year. So, for our purposes, we can use the 2019 data documentation file to describe the data set. Open the documentation file for the 2019 Mortality Multiple Cause-of-Death Files.\nEach variable in the LA Mortality File has a corresponding entry in the documentation file. For example, the first variable listed when you preview the data frame should be “restatus”. If you search through the documentation file for “restatus”, you won’t find anything. However, if you search instead for “status”, the first result should be a data item called “resident status”.\nSounds like “resident status” is probably a good fit for our “restatus” variable. The definition for the “resident status” variable is included on the third page of the documentation file.\n\nYou can see that this variable provides some information on whether the place where a death occurred is also that person’s place of residence (note that we’re only interested in United States Occurence for our purposes). So, on your Markdown document, begin your data element list as follows:\n\nrestatus, Resident Status\n\nState and county of Occurrence and Residence are the same.\nState of Occurrence and Residence are the same, but County is different.\nState of Occurrence and Residence are different, but both are in the U.S.\nState of Occurrence is one of the 50 States or D.C., but Place of Residence is outside the U.S.\n\n\nHint: to italicize a word in a Markdown document write it like this: *restatus*\nContinue to build your data element list by adding each variable in the data set and its corresponding definition from the documentation file. Note that for some variables, the descriptions and code values will be lengthy, so you’ll have to abbreviate.\nOnce you’re finished, save the Markdown document as Cancer Alley Data Description, push it to your hpam7660_Cancer_Alley repo, invite me to the repo, and you’re done!"
  },
  {
    "objectID": "assignments/tutorial_6.html",
    "href": "assignments/tutorial_6.html",
    "title": "Summarizing Data Tutorial - Louisiana Mortality File",
    "section": "",
    "text": "In the previous tutorial, we calculated crude cancer mortality rates across Louisiana parishes from 2005 through 2019. Now we’ll refine our method for calculating those rates by adjusting for different age distributions across parishes and over time.\nIt’s important to age-adjust our mortality rates because different parishes have different age profiles and that could bias our estimated rates. For example, suppose that parish A has a relatively high proportion of older people and parish B has a relatively high proportion of younger people. Crude cancer mortality rates would likely show higher rates among parish A than parish B. However, this may not actually be the case once we account for the fact that the older population in parish A is more prone to cancer.\nAge-adjustment is really a way to determine how cancer mortality rates would differ between parish A and parish B if both parishes had the same population age distribution.\nLet’s start by opening the .Rproj file in your hpam7660_Cancer_Alley folder. Then open the Markdown document you used in the previous tutorial to create the analytic sample. We can continue working off of this same Markdown document as we refine our sample in this tutorial.\n\n\nTo calculate age-adjusted cancer mortality rates, we need to generate cancer death counts by parish and year for each age group in our data. Start by reading in the raw Louisiana cancer mortality file (you may be able to skip this step if the la_mort data frame is still loaded in your project environment).\n\nlibrary(readr)\nla_mort &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fzsnhfd3lq80v2o3sag6c/la_mort.csv?rlkey=h1vyjm2b8ppgejgsg3e8evm7i&dl=1\")\n\nWe now need to repeat a couple of steps that we’ve seen before. First, we’ll create an indicator for whether a parish is in Cancer Alley:\n\nla_mort$cancer_parish &lt;- ifelse(la_mort$cntyrsd %in% c(5, 33, 47, 51, 71, 89, 93, 95, 121), 1, 0)\n\nThen we’ll create an indicator for a cancer death:\n\nla_mort$cancer39 &lt;- ifelse(la_mort$ucr39 %in% c(5:15), 1, 0)\n\nNext, we need to aggregate to the parish level like we did before, but with one important difference - we need to create age-specific counts of parish-level cancer deaths. To do so, we need to assign people in the mortality file to these age groups using the age variable. But remember, the age variable in the mortality file uses some strange values (refer to the codebook you created for specifics). First we’ll need to replace values of the age variable with age in years. Use the following code to do so:\n\nlibrary(dplyr)\nla_mort_age &lt;- la_mort %&gt;%\n  filter(age != 9999)\nla_mort_age$age &lt;- ifelse(la_mort_age$age &lt; 2000, la_mort_age$age - 1000, 0)\n\nThere are a few people for whom age in the la_mort file is missing and coded 9999. First, we’re dropping those people from the data by creating a new data frame called la_mort_age where anyone with an age equal to 9999 is excluded. The next line tells R to replace the age variable with values equal to the current value minus 1000 if the current value is less than 2000. Remember that a leading digit of 1 in this variable indicates that age is recorded in years and the remaining three digits are age in years. So by subtracting 1000, we’re left with age in years for those whose age was recorded in years (confusing I know!). Everyone with a leading digit between 2 and 9 died before they turned 1 year old and so their age is measured in months, days, etc. Here we’re telling R to change all those values to 0 since we only want age measured in years.\nNow that we have the ages for all individuals in our data represented as age in years, we can categorize ages based on the categories available in the denominator file, which are as follows:\n0 to 4\n5 to 9\n10 to 14\n15 to 19\n20 to 24\n25 to 29\n30 to 34\n35 to 39\n40 to 44\n45 to 49\n50 to 54\n55 to 59\n60 to 64\n65 to 69\n70 to 74\n75 to 79\n80 to 84\n85 and above\n\nHere’s the R code that will allow us to do that:\n\nage_breaks &lt;- c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, Inf)\nage_labels &lt;- c(\"0_4\", \"5_9\", \"10_14\", \"15_19\", \"20_24\", \"25_29\", \"30_34\", \"35_39\", \n                \"40_44\", \"45_49\", \"50_54\", \"55_59\", \"60_64\", \"65_69\", \"70_74\", \n                \"75_79\", \"80_84\", \"85+\")\nla_mort_age$agegrp &lt;- as.character(cut(la_mort_age$age, breaks = age_breaks, labels = age_labels, right = FALSE))\n\nThe first line in this code is telling R to define a vector called age_breaks where the “break” corresponds to the lower bound for each age group (e.g., 0 for 0 to 4, 5 for 5 to 9, etc.). The second line is defining values for each age category. Note that I’ve defined these values so that they correspond exactly to the values in the denominator file (la_pop). Finally, the third line is creating a new variable in the la_mort_age data frame called agegrp using the cut function that categorizes the numerical age variable into discrete intervals. Here, the discrete intervals are defined by the values in the age_breaks vector and labeled using the values in the age_labels vector. The right = FALSE option tells R to exclude the next break age from the current age interval (e.g., exclude 5 from the 0-5 interval, exclude 10 from the 5-10 interval, and so on). Also, note that I called this new variable agegrp and created it as a character variable so that it would match the name and variable type of the agegrp variable in the la_pop data. This is because we’ll use agegrp as a key variable when we’re joining the two data frames.\n\n\n\nLast time, when we aggregated to the parish level, we used the following code:\n\nparish_count &lt;- la_mort %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer39 = sum(cancer39, na.rm = TRUE))\n\nIf you remember, this code created a data frame that included the overall count of cancer deaths by parish and year, but did not differentiate by age. Let’s modify this code so that we aggregate to the parish-age-year level instead of just the parish-year level. To do so, we’ll just need to add agegrp to our group_by function:\n\nparish_count_age &lt;- la_mort_age %&gt;%\n  group_by(cntyrsd, cancer_parish, agegrp, year) %&gt;%\n  summarize(cancer39 = sum(cancer39, na.rm = TRUE))\n\nAfter running this, you should see that the parish_count_age data frame contains counts of cancer deaths for each age category we defined above at the parish-year level.\n\n\n\nNow it’s time to join the parish_count_age data frame that we just created with the la_pop data frame that contains our population denominators. First, load the la_pop data if it’s not already in your project environment:\n\nla_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/650k1obpczky6bwa19ex6/la_county_pop.csv?rlkey=0aokd9m76q7mxwus97uslsx7g&dl=1\")\n\nRemember that last time, we changed the key variable names in the the cancer count data to match the names in the population data (e.g., cntyrsd became county). This time, we’ll run the join with the different key variable names so you can see what that looks like.\nHere’s the code we used last time to do the join:\n\nla_joined &lt;- parish_count %&gt;%\n  inner_join(la_pop, by = c(\"county\", \"year\"))\n\nWe’ll modify that code this time to account for the different key variable names (cntyrsd and county) and because we now have a new key variable to include: agegrp.\n\nla_joined &lt;- parish_count_age %&gt;%\n  inner_join(la_pop, by = c(\"cntyrsd\" = \"county\", \"year\", \"agegrp\"))\n\nThis code tells R that we want to create a new data frame called la_joined that is formed using an inner_join on the parish_count_age and la_pop data frames. They key variables for R to join on are county (which is called cntyrsd in the parish_count_age data frame and called county in the la_pop data frame), year, and age group.\nIf you take a look at the new la_joined data frame, you’ll see that we have cancer death counts and population counts for each age in each parish-year. We also have population counts by age-race-ethnicity, which we’ll use later on.\n\n\n\nThe process of age-adjusting parish cancer mortality rates consists of standardizing age distributions so that when comparing cancer mortality across parish or over time, we’re comparing mortality rates for people who are the “same age” (in a statistical sense). Standardizing age distributions involves choosing a reference population and calculating weighted mortality rates based on that population. Let’s walk through how this is done.\nSTANDARD POPULATION\nFirst we need to choose a standard population age distribution. Most commonly, people use the age distribution in the U.S. at the time of the 2000 census.\n\nSource: https://seer.cancer.gov/stdpopulations/stdpop.19ages.html\n\n\nAge\n2000 U.S. Standard (in millions)\n\n\n\n\n0 to 4\n69,135\n\n\n5 to 9\n72,533\n\n\n10 to 14\n73,032\n\n\n15 to 19\n72,169\n\n\n20 to 24\n66,478\n\n\n25 to 29\n64,529\n\n\n30 to 34\n71,044\n\n\n35 to 39\n80,762\n\n\n40 to 44\n81,851\n\n\n45 to 49\n72,118\n\n\n50 to 54\n62,716\n\n\n55 to 59\n48,454\n\n\n60 to 64\n38,793\n\n\n65 to 69\n34,264\n\n\n70 to 74\n31,773\n\n\n75 to 79\n26,999\n\n\n80 to 84\n17,842\n\n\n85+\n15,508\n\n\n\nHere’s a .csv file of the standard population age distribution. Read it into R and join it with your la_joined data frame.\n\nstnrd_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/xzd2o5lza237so6vamqwb/stnrd_pop.csv?rlkey=zp90au2tuq6eptvi1yiyjfzua&dl=1\")\nla_joined_stnrd &lt;- la_joined %&gt;%\n  inner_join(stnrd_pop, by = \"agegrp\")\n\nNote that this code uses an inner_join command on the key variable agegrp. Both the la_joined and stnrd_pop data frames contain age ranges and so this join statement will add the standard population column in the table above to our la_joined data frame.\nSTANDARD POPULATION WEIGHTS\nNow that we have the standard population age distribution, we need to use this information to create population weights. The idea here is that we calculate the share of the total population represented by each age group. We’ll then multiply this share by the age-specific cancer mortality rates we calculated above.\nRun the following code to calculate the population weights:\n\nla_joined_stnrd$stnrd_pop_weight &lt;- (la_joined_stnrd$stnrd_pop) / (sum(stnrd_pop$stnrd_pop))\n\nThis code creates a new variable called stnrd_pop_weight that is calculated as the age-specific standard population divided by the total standard population. In other words, the value of this stnrd_pop_weight variable is each age group’s share of the total population.\nAPPLYING STANDARD POPULAION WEIGHTS\nNow we want to multiply the age-specific cancer mortality rates (that we actually haven’t created yet in this tutorial) by this new stnrd_pop_weight variable. In the last tutorial, we calculated cancer mortality rates by dividing the total count of cancer deaths by the total population in each parish and then multiplied by 100,000 so that rates were per 100,000 population. We’ll do the same thing here, but this time rates will be age-specific and we’ll multiply the rates by the stnrd_pop_weight variable.\n\nla_joined_stnrd$cancer_rate_adj &lt;- ((la_joined_stnrd$cancer39) / (la_joined_stnrd$tot_pop / 100000)) * la_joined_stnrd$stnrd_pop_weight\n\nThis code tells R to create a new variable called cancer_rate_adj that is equal to the count of cancer deaths cancer39 divided by the total population tot_pop (which has been divided by 100,000) and multiplied by the stnrd_pop_weight variable. However, the big difference between what we’re doing this time and what we did in the last tutorial is that the rows in our la_joined_stnrd data frame represent age-specific cancer death counts and populations and not overall counts at the parish level.\nAGGREGATE OVER AGE\nOur goal here is to calculate a single age-adjusted cancer mortality rate for each parish in each year. Now that we’ve calculated cancer mortality rates for each age group in each parish-year, we just need to aggregate over those age groups to get the age-adjusted parish level cancer mortality rate. We can do that as follows:\n\nparish_rates &lt;- la_joined_stnrd %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer_rate_adj = sum(cancer_rate_adj, na.rm = TRUE), cancer39 = sum(cancer39), tot_pop = \n              sum(tot_pop))\nparish_rates$cancer_rate_crude &lt;- (parish_rates$cancer39) / (parish_rates$tot_pop / 100000)\n\nThis code creates a new data frame called parish_rates that contains aggregated cancer morality rates (cancer_rate_adj) by parish-year. I’ve also included columns for the total count of cancer deaths in that parish-year and the total population in that parish-year. We’ll use those columns shortly. Finally, notice that the last line in the code chunk above creates a variable called cancer_rate_crude. This is just the non-age-adjusted parish cancer mortality rate that we calculated in the last tutorial.\nThis step may be particularly confusing, so if there’s one way to wrap your head around what’s happening here, it’s this: values in the la_joined_stnrd data frame rows were measured at the age-parish-year level, while values in the parish_rates data frame are measured at the parish-year level.\nLet’s take a look at differences between crude and adjusted cancer rates. We can recreate the table of 2019 cancer rates that we made at the end of the last tutorial. Here was the code we used:\n\nparish_cancer_2019 &lt;- subset(la_joined_all, year == 2019)\nlibrary(knitr)\nkable(parish_cancer_2019[, c(\"county\", \"cancer_rate_total\")])\n\nLet’s modify this code to include the adjusted rate and change the names to reflect our new data frame names:\n\nparish_cancer_2019 &lt;- subset(parish_rates, year == 2019)\nkable(parish_cancer_2019[, c(\"cntyrsd\", \"cancer_rate_crude\", \"cancer_rate_adj\")])\n\n\n\n\nYou’ll notice there are some big differences between some of the crude and age-adjusted cancer mortality rates. This tends to happen when a parish’s population (and hence the number of cancer deaths in that parish is small). We’ll want to account for this before we compare cancer mortality rates for Cancer Alley and non-Cancer Alley parishes. We can do this by weighting our aggregated (Cancer Alley vs. non-Cancer Alley) means by parish populations. In other words, when comparing a single “Cancer Alley mortality rate” to a single “Non-Cancer Alley mortality rate”, these rates are comprised of an average of rates for each parish in each grouping. But we’d like the more populous parishes in those groupings to contribute more to the overall average mortality rate rather than weighting each parish equally.\nWe can create population weights (different from standard population weights) by first multiplying our age-adjusted parish cancer mortality rates by the total parish population, adding up each of those values for all the Cancer Alley parishes and the non-Cancer Alley parishes separately, and then dividing the sum of those values by the total population in Cancer Alley and non-Cancer Alley parishes (kind of confusing, I know). Here’s the code to do that:\n\nparish_rates$pop_weight &lt;- (parish_rates$cancer_rate_adj) * (parish_rates$tot_pop)\ncancer_alley_rates &lt;- parish_rates %&gt;%\n  group_by(cancer_parish, year) %&gt;%\n  summarize(cancer_rate_adj_wt = sum(pop_weight) / sum(tot_pop))\n\nThe first line of the code creates a new variable in the parish_rates data frame called pop_weight that is equal to the product of each parish’s age-adjusted cancer mortality rate (cancer_rate_adj) and that parish’s total population (tot_pop). The rest of the code sums up these values over Cancer Alley and non-Cancer Alley parishes and divides the weighted mortality rates by the total population in each group.\n\n\n\nNow that we have our age-adjusted and population-weighted parish cancer mortality rates, we can create a table to compare annual cancer mortality rates for Cancer Alley and non-Cancer Alley parishes. Just like we did before, let’s use the kable command to create the table:\n\nkable(cancer_alley_rates)\n\nThis is ok, but it would be much easier to read this table if we had a column of cancer mortality rates for Cancer Alley parishes and a separate column of cancer mortality rates for non-Cancer Alley Parishes. To get there, we can split the cancer_alley_rates data frame by the cancer_parish indicator and then join the two data frames back together:\n\ncancer_alley &lt;- \n  subset(cancer_alley_rates, cancer_parish == 1, select = c(cancer_rate_adj_wt, year)) %&gt;%\n  rename(cancer_alley_rate = cancer_rate_adj_wt)\nno_cancer_alley &lt;- \n  subset(cancer_alley_rates, cancer_parish == 0, select = c(cancer_rate_adj_wt, year)) %&gt;%\n  rename(no_cancer_alley_rate = cancer_rate_adj_wt)\ncancer_alley_table &lt;- cancer_alley %&gt;%\n  inner_join(no_cancer_alley, by = \"year\")\ncancer_alley_table &lt;- cancer_alley_table[,c(\"year\", \"cancer_alley_rate\", \"no_cancer_alley_rate\")]\nkable(cancer_alley_table)\n\nThis looks much better and provides a nice way to summarize age-adjusted, population weighted, cancer mortality rates for Cancer Alley and non-Cancer Alley parishes. In the next tutorial, we’ll refine our definition of Cancer Alley parishes and run some subgroup analyses that will allow us to examine cancer mortality rates by cancer type and by race and ethnicity.\nBe sure to save your Markdown file and push the file to your hpam7660_Cancer_Alley GitHub repo."
  },
  {
    "objectID": "assignments/tutorial_6.html#refining-our-analyic-sample",
    "href": "assignments/tutorial_6.html#refining-our-analyic-sample",
    "title": "Summarizing Data Tutorial - Louisiana Mortality File",
    "section": "",
    "text": "In the previous tutorial, we calculated crude cancer mortality rates across Louisiana parishes from 2005 through 2019. Now we’ll refine our method for calculating those rates by adjusting for different age distributions across parishes and over time.\nIt’s important to age-adjust our mortality rates because different parishes have different age profiles and that could bias our estimated rates. For example, suppose that parish A has a relatively high proportion of older people and parish B has a relatively high proportion of younger people. Crude cancer mortality rates would likely show higher rates among parish A than parish B. However, this may not actually be the case once we account for the fact that the older population in parish A is more prone to cancer.\nAge-adjustment is really a way to determine how cancer mortality rates would differ between parish A and parish B if both parishes had the same population age distribution.\nLet’s start by opening the .Rproj file in your hpam7660_Cancer_Alley folder. Then open the Markdown document you used in the previous tutorial to create the analytic sample. We can continue working off of this same Markdown document as we refine our sample in this tutorial.\n\n\nTo calculate age-adjusted cancer mortality rates, we need to generate cancer death counts by parish and year for each age group in our data. Start by reading in the raw Louisiana cancer mortality file (you may be able to skip this step if the la_mort data frame is still loaded in your project environment).\n\nlibrary(readr)\nla_mort &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fzsnhfd3lq80v2o3sag6c/la_mort.csv?rlkey=h1vyjm2b8ppgejgsg3e8evm7i&dl=1\")\n\nWe now need to repeat a couple of steps that we’ve seen before. First, we’ll create an indicator for whether a parish is in Cancer Alley:\n\nla_mort$cancer_parish &lt;- ifelse(la_mort$cntyrsd %in% c(5, 33, 47, 51, 71, 89, 93, 95, 121), 1, 0)\n\nThen we’ll create an indicator for a cancer death:\n\nla_mort$cancer39 &lt;- ifelse(la_mort$ucr39 %in% c(5:15), 1, 0)\n\nNext, we need to aggregate to the parish level like we did before, but with one important difference - we need to create age-specific counts of parish-level cancer deaths. To do so, we need to assign people in the mortality file to these age groups using the age variable. But remember, the age variable in the mortality file uses some strange values (refer to the codebook you created for specifics). First we’ll need to replace values of the age variable with age in years. Use the following code to do so:\n\nlibrary(dplyr)\nla_mort_age &lt;- la_mort %&gt;%\n  filter(age != 9999)\nla_mort_age$age &lt;- ifelse(la_mort_age$age &lt; 2000, la_mort_age$age - 1000, 0)\n\nThere are a few people for whom age in the la_mort file is missing and coded 9999. First, we’re dropping those people from the data by creating a new data frame called la_mort_age where anyone with an age equal to 9999 is excluded. The next line tells R to replace the age variable with values equal to the current value minus 1000 if the current value is less than 2000. Remember that a leading digit of 1 in this variable indicates that age is recorded in years and the remaining three digits are age in years. So by subtracting 1000, we’re left with age in years for those whose age was recorded in years (confusing I know!). Everyone with a leading digit between 2 and 9 died before they turned 1 year old and so their age is measured in months, days, etc. Here we’re telling R to change all those values to 0 since we only want age measured in years.\nNow that we have the ages for all individuals in our data represented as age in years, we can categorize ages based on the categories available in the denominator file, which are as follows:\n0 to 4\n5 to 9\n10 to 14\n15 to 19\n20 to 24\n25 to 29\n30 to 34\n35 to 39\n40 to 44\n45 to 49\n50 to 54\n55 to 59\n60 to 64\n65 to 69\n70 to 74\n75 to 79\n80 to 84\n85 and above\n\nHere’s the R code that will allow us to do that:\n\nage_breaks &lt;- c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, Inf)\nage_labels &lt;- c(\"0_4\", \"5_9\", \"10_14\", \"15_19\", \"20_24\", \"25_29\", \"30_34\", \"35_39\", \n                \"40_44\", \"45_49\", \"50_54\", \"55_59\", \"60_64\", \"65_69\", \"70_74\", \n                \"75_79\", \"80_84\", \"85+\")\nla_mort_age$agegrp &lt;- as.character(cut(la_mort_age$age, breaks = age_breaks, labels = age_labels, right = FALSE))\n\nThe first line in this code is telling R to define a vector called age_breaks where the “break” corresponds to the lower bound for each age group (e.g., 0 for 0 to 4, 5 for 5 to 9, etc.). The second line is defining values for each age category. Note that I’ve defined these values so that they correspond exactly to the values in the denominator file (la_pop). Finally, the third line is creating a new variable in the la_mort_age data frame called agegrp using the cut function that categorizes the numerical age variable into discrete intervals. Here, the discrete intervals are defined by the values in the age_breaks vector and labeled using the values in the age_labels vector. The right = FALSE option tells R to exclude the next break age from the current age interval (e.g., exclude 5 from the 0-5 interval, exclude 10 from the 5-10 interval, and so on). Also, note that I called this new variable agegrp and created it as a character variable so that it would match the name and variable type of the agegrp variable in the la_pop data. This is because we’ll use agegrp as a key variable when we’re joining the two data frames.\n\n\n\nLast time, when we aggregated to the parish level, we used the following code:\n\nparish_count &lt;- la_mort %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer39 = sum(cancer39, na.rm = TRUE))\n\nIf you remember, this code created a data frame that included the overall count of cancer deaths by parish and year, but did not differentiate by age. Let’s modify this code so that we aggregate to the parish-age-year level instead of just the parish-year level. To do so, we’ll just need to add agegrp to our group_by function:\n\nparish_count_age &lt;- la_mort_age %&gt;%\n  group_by(cntyrsd, cancer_parish, agegrp, year) %&gt;%\n  summarize(cancer39 = sum(cancer39, na.rm = TRUE))\n\nAfter running this, you should see that the parish_count_age data frame contains counts of cancer deaths for each age category we defined above at the parish-year level.\n\n\n\nNow it’s time to join the parish_count_age data frame that we just created with the la_pop data frame that contains our population denominators. First, load the la_pop data if it’s not already in your project environment:\n\nla_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/650k1obpczky6bwa19ex6/la_county_pop.csv?rlkey=0aokd9m76q7mxwus97uslsx7g&dl=1\")\n\nRemember that last time, we changed the key variable names in the the cancer count data to match the names in the population data (e.g., cntyrsd became county). This time, we’ll run the join with the different key variable names so you can see what that looks like.\nHere’s the code we used last time to do the join:\n\nla_joined &lt;- parish_count %&gt;%\n  inner_join(la_pop, by = c(\"county\", \"year\"))\n\nWe’ll modify that code this time to account for the different key variable names (cntyrsd and county) and because we now have a new key variable to include: agegrp.\n\nla_joined &lt;- parish_count_age %&gt;%\n  inner_join(la_pop, by = c(\"cntyrsd\" = \"county\", \"year\", \"agegrp\"))\n\nThis code tells R that we want to create a new data frame called la_joined that is formed using an inner_join on the parish_count_age and la_pop data frames. They key variables for R to join on are county (which is called cntyrsd in the parish_count_age data frame and called county in the la_pop data frame), year, and age group.\nIf you take a look at the new la_joined data frame, you’ll see that we have cancer death counts and population counts for each age in each parish-year. We also have population counts by age-race-ethnicity, which we’ll use later on.\n\n\n\nThe process of age-adjusting parish cancer mortality rates consists of standardizing age distributions so that when comparing cancer mortality across parish or over time, we’re comparing mortality rates for people who are the “same age” (in a statistical sense). Standardizing age distributions involves choosing a reference population and calculating weighted mortality rates based on that population. Let’s walk through how this is done.\nSTANDARD POPULATION\nFirst we need to choose a standard population age distribution. Most commonly, people use the age distribution in the U.S. at the time of the 2000 census.\n\nSource: https://seer.cancer.gov/stdpopulations/stdpop.19ages.html\n\n\nAge\n2000 U.S. Standard (in millions)\n\n\n\n\n0 to 4\n69,135\n\n\n5 to 9\n72,533\n\n\n10 to 14\n73,032\n\n\n15 to 19\n72,169\n\n\n20 to 24\n66,478\n\n\n25 to 29\n64,529\n\n\n30 to 34\n71,044\n\n\n35 to 39\n80,762\n\n\n40 to 44\n81,851\n\n\n45 to 49\n72,118\n\n\n50 to 54\n62,716\n\n\n55 to 59\n48,454\n\n\n60 to 64\n38,793\n\n\n65 to 69\n34,264\n\n\n70 to 74\n31,773\n\n\n75 to 79\n26,999\n\n\n80 to 84\n17,842\n\n\n85+\n15,508\n\n\n\nHere’s a .csv file of the standard population age distribution. Read it into R and join it with your la_joined data frame.\n\nstnrd_pop &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/xzd2o5lza237so6vamqwb/stnrd_pop.csv?rlkey=zp90au2tuq6eptvi1yiyjfzua&dl=1\")\nla_joined_stnrd &lt;- la_joined %&gt;%\n  inner_join(stnrd_pop, by = \"agegrp\")\n\nNote that this code uses an inner_join command on the key variable agegrp. Both the la_joined and stnrd_pop data frames contain age ranges and so this join statement will add the standard population column in the table above to our la_joined data frame.\nSTANDARD POPULATION WEIGHTS\nNow that we have the standard population age distribution, we need to use this information to create population weights. The idea here is that we calculate the share of the total population represented by each age group. We’ll then multiply this share by the age-specific cancer mortality rates we calculated above.\nRun the following code to calculate the population weights:\n\nla_joined_stnrd$stnrd_pop_weight &lt;- (la_joined_stnrd$stnrd_pop) / (sum(stnrd_pop$stnrd_pop))\n\nThis code creates a new variable called stnrd_pop_weight that is calculated as the age-specific standard population divided by the total standard population. In other words, the value of this stnrd_pop_weight variable is each age group’s share of the total population.\nAPPLYING STANDARD POPULAION WEIGHTS\nNow we want to multiply the age-specific cancer mortality rates (that we actually haven’t created yet in this tutorial) by this new stnrd_pop_weight variable. In the last tutorial, we calculated cancer mortality rates by dividing the total count of cancer deaths by the total population in each parish and then multiplied by 100,000 so that rates were per 100,000 population. We’ll do the same thing here, but this time rates will be age-specific and we’ll multiply the rates by the stnrd_pop_weight variable.\n\nla_joined_stnrd$cancer_rate_adj &lt;- ((la_joined_stnrd$cancer39) / (la_joined_stnrd$tot_pop / 100000)) * la_joined_stnrd$stnrd_pop_weight\n\nThis code tells R to create a new variable called cancer_rate_adj that is equal to the count of cancer deaths cancer39 divided by the total population tot_pop (which has been divided by 100,000) and multiplied by the stnrd_pop_weight variable. However, the big difference between what we’re doing this time and what we did in the last tutorial is that the rows in our la_joined_stnrd data frame represent age-specific cancer death counts and populations and not overall counts at the parish level.\nAGGREGATE OVER AGE\nOur goal here is to calculate a single age-adjusted cancer mortality rate for each parish in each year. Now that we’ve calculated cancer mortality rates for each age group in each parish-year, we just need to aggregate over those age groups to get the age-adjusted parish level cancer mortality rate. We can do that as follows:\n\nparish_rates &lt;- la_joined_stnrd %&gt;%\n  group_by(cntyrsd, cancer_parish, year) %&gt;%\n  summarize(cancer_rate_adj = sum(cancer_rate_adj, na.rm = TRUE), cancer39 = sum(cancer39), tot_pop = \n              sum(tot_pop))\nparish_rates$cancer_rate_crude &lt;- (parish_rates$cancer39) / (parish_rates$tot_pop / 100000)\n\nThis code creates a new data frame called parish_rates that contains aggregated cancer morality rates (cancer_rate_adj) by parish-year. I’ve also included columns for the total count of cancer deaths in that parish-year and the total population in that parish-year. We’ll use those columns shortly. Finally, notice that the last line in the code chunk above creates a variable called cancer_rate_crude. This is just the non-age-adjusted parish cancer mortality rate that we calculated in the last tutorial.\nThis step may be particularly confusing, so if there’s one way to wrap your head around what’s happening here, it’s this: values in the la_joined_stnrd data frame rows were measured at the age-parish-year level, while values in the parish_rates data frame are measured at the parish-year level.\nLet’s take a look at differences between crude and adjusted cancer rates. We can recreate the table of 2019 cancer rates that we made at the end of the last tutorial. Here was the code we used:\n\nparish_cancer_2019 &lt;- subset(la_joined_all, year == 2019)\nlibrary(knitr)\nkable(parish_cancer_2019[, c(\"county\", \"cancer_rate_total\")])\n\nLet’s modify this code to include the adjusted rate and change the names to reflect our new data frame names:\n\nparish_cancer_2019 &lt;- subset(parish_rates, year == 2019)\nkable(parish_cancer_2019[, c(\"cntyrsd\", \"cancer_rate_crude\", \"cancer_rate_adj\")])\n\n\n\n\nYou’ll notice there are some big differences between some of the crude and age-adjusted cancer mortality rates. This tends to happen when a parish’s population (and hence the number of cancer deaths in that parish is small). We’ll want to account for this before we compare cancer mortality rates for Cancer Alley and non-Cancer Alley parishes. We can do this by weighting our aggregated (Cancer Alley vs. non-Cancer Alley) means by parish populations. In other words, when comparing a single “Cancer Alley mortality rate” to a single “Non-Cancer Alley mortality rate”, these rates are comprised of an average of rates for each parish in each grouping. But we’d like the more populous parishes in those groupings to contribute more to the overall average mortality rate rather than weighting each parish equally.\nWe can create population weights (different from standard population weights) by first multiplying our age-adjusted parish cancer mortality rates by the total parish population, adding up each of those values for all the Cancer Alley parishes and the non-Cancer Alley parishes separately, and then dividing the sum of those values by the total population in Cancer Alley and non-Cancer Alley parishes (kind of confusing, I know). Here’s the code to do that:\n\nparish_rates$pop_weight &lt;- (parish_rates$cancer_rate_adj) * (parish_rates$tot_pop)\ncancer_alley_rates &lt;- parish_rates %&gt;%\n  group_by(cancer_parish, year) %&gt;%\n  summarize(cancer_rate_adj_wt = sum(pop_weight) / sum(tot_pop))\n\nThe first line of the code creates a new variable in the parish_rates data frame called pop_weight that is equal to the product of each parish’s age-adjusted cancer mortality rate (cancer_rate_adj) and that parish’s total population (tot_pop). The rest of the code sums up these values over Cancer Alley and non-Cancer Alley parishes and divides the weighted mortality rates by the total population in each group.\n\n\n\nNow that we have our age-adjusted and population-weighted parish cancer mortality rates, we can create a table to compare annual cancer mortality rates for Cancer Alley and non-Cancer Alley parishes. Just like we did before, let’s use the kable command to create the table:\n\nkable(cancer_alley_rates)\n\nThis is ok, but it would be much easier to read this table if we had a column of cancer mortality rates for Cancer Alley parishes and a separate column of cancer mortality rates for non-Cancer Alley Parishes. To get there, we can split the cancer_alley_rates data frame by the cancer_parish indicator and then join the two data frames back together:\n\ncancer_alley &lt;- \n  subset(cancer_alley_rates, cancer_parish == 1, select = c(cancer_rate_adj_wt, year)) %&gt;%\n  rename(cancer_alley_rate = cancer_rate_adj_wt)\nno_cancer_alley &lt;- \n  subset(cancer_alley_rates, cancer_parish == 0, select = c(cancer_rate_adj_wt, year)) %&gt;%\n  rename(no_cancer_alley_rate = cancer_rate_adj_wt)\ncancer_alley_table &lt;- cancer_alley %&gt;%\n  inner_join(no_cancer_alley, by = \"year\")\ncancer_alley_table &lt;- cancer_alley_table[,c(\"year\", \"cancer_alley_rate\", \"no_cancer_alley_rate\")]\nkable(cancer_alley_table)\n\nThis looks much better and provides a nice way to summarize age-adjusted, population weighted, cancer mortality rates for Cancer Alley and non-Cancer Alley parishes. In the next tutorial, we’ll refine our definition of Cancer Alley parishes and run some subgroup analyses that will allow us to examine cancer mortality rates by cancer type and by race and ethnicity.\nBe sure to save your Markdown file and push the file to your hpam7660_Cancer_Alley GitHub repo."
  },
  {
    "objectID": "assignments/tutorial_9.html",
    "href": "assignments/tutorial_9.html",
    "title": "Regression Tutorial - Parish Cancer Rates Controlling for Educational Attainment",
    "section": "",
    "text": "This mini-tutorial provides an example of controlling for potential confounders using regression analysis. In this case, we’ll add parish-level smoking rates to our Cancer Alley data and generate estimates of parish cancer mortality rates that control for differences in smoking rates."
  },
  {
    "objectID": "assignments/tutorial_9.html#load-cancer-alley-data",
    "href": "assignments/tutorial_9.html#load-cancer-alley-data",
    "title": "Regression Tutorial - Parish Cancer Rates Controlling for Educational Attainment",
    "section": "Load Cancer Alley Data",
    "text": "Load Cancer Alley Data\nFirst, we’ll need to load the data frames we created in previous Cancer Alley tutorials. If you’ve saved these data frames to an image, you can load that image as follows\n\nlibrary(tidyverse)\nlibrary(knitr)\nload(\"~/Dropbox/Documents/Teaching Materials/Health Policy/GitHub Site/hpam7660-sp24//assignments/tutorial_8.RData\")\n\nOtherwise, you’ll need to re-run your code that creates the parish_rates data frame. Once you have that data frame loaded in your environment, you can do some data cleaning as follows:"
  },
  {
    "objectID": "assignments/tutorial_9.html#join-smoking-rate-data",
    "href": "assignments/tutorial_9.html#join-smoking-rate-data",
    "title": "Regression Tutorial - Parish Cancer Rates Controlling for Educational Attainment",
    "section": "Join Smoking Rate Data",
    "text": "Join Smoking Rate Data\n\nparish_rates &lt;- parish_rates %&gt;%\n  filter(black == 0) %&gt;%\n  subset(, select = c(\"cntyrsd\", \"cancer_parish\", \"year\", \"total_rate_adj\", \"total_weight\", \"population\"))\n\nHere, I’m modifying the parish_rates data so that it only includes the columns that I want to use for this mini-tutorial and so that it restricts the sample to parish cancer mortality rates for white people. The reason that I’m restricting the sample to white people is that I don’t have smoking rates by race and ethnicity (if this were a real research project, that’s something I’d want to obtain). And since whites have higher smoking rates and comprise a greater share of the population in Louisiana than other races and ethnicities, it makes sense to restrict this example to the confounding effects of smoking on cancer mortality for whites.\nNext, we’ll read in the smoking rate data:\n\nla_smoke &lt;- \n  read_csv(\"https://www.dropbox.com/scl/fi/fh5gokmw3so6xfc5rthbl/la_smoke.csv?rlkey=f0vy11mvqcd37htsz8ubovtb8&dl=1\")\n\nAnd then join that data to our parish_rates data using the key variables of cntyrsd (called fips in the smoking rate data) and year:\n\nla_smoke_rates &lt;- parish_rates %&gt;%\n  inner_join(la_smoke, by = c(\"cntyrsd\" = \"fips\", \"year\"))"
  },
  {
    "objectID": "assignments/tutorial_9.html#regress-parish-cancer-rates-on-parish-smoking-rates",
    "href": "assignments/tutorial_9.html#regress-parish-cancer-rates-on-parish-smoking-rates",
    "title": "Regression Tutorial - Parish Cancer Rates Controlling for Educational Attainment",
    "section": "Regress Parish Cancer Rates on Parish Smoking Rates",
    "text": "Regress Parish Cancer Rates on Parish Smoking Rates\nNow we’ll run a regression that estimates the relationship between the share of people who smoke in a parish in a given year and the total cancer mortality rate in that year (there are some other terms included in the regression that aren’t that important for the purposes of this tutorial.)\n\nmodel &lt;- lm(total_rate_adj ~ smoke_perct + as.factor(year) + as.factor(cntyrsd), data = la_smoke_rates)\n\nThe lm() command tells R that I want to run a linear regression model. You can visualize this as drawing a straight line (i.e., linear) through a plot of points where smoking rate is on the x-axis and cancer mortality rate is on the y-axis.\nNext, I want to predict parish-level cancer mortality rates holding smoking rates constant (i.e., controlling for smoking rates):\n\nla_smoke_rates$total_rate_hat &lt;- predict(model, newdata = la_smoke_rates)\n\nThis code creates a new variable called total_rate_hat that represents cancer mortality rates holding smoking rates constant."
  },
  {
    "objectID": "assignments/tutorial_9.html#create-population-weighted-measures",
    "href": "assignments/tutorial_9.html#create-population-weighted-measures",
    "title": "Regression Tutorial - Parish Cancer Rates Controlling for Educational Attainment",
    "section": "Create Population Weighted Measures",
    "text": "Create Population Weighted Measures\nNext, I want to weight this rate by population just like we did for the cancer mortality rates we created previously:\n\nla_smoke_rates$smoke_weight &lt;- (la_smoke_rates$total_rate_hat) * (la_smoke_rates$population)\nla_smoke_final &lt;- la_smoke_rates %&gt;%\n  group_by(cntyrsd, year) %&gt;%\n  summarize(\n    total_rate_adj_wt = sum((total_weight) / sum(population), na.rm = TRUE),\n    smoke_rate_adj_wt = sum((smoke_weight) / sum(population), na.rm = TRUE)\n  )\n\nThat’s it. We’re done. Now we can compare the parish-level cancer mortality rates to the cancer mortality rates that control for the share of the population that smokes to see the difference. Here’s an example for one parish:"
  },
  {
    "objectID": "assignments/tutorial_9.html#output-results",
    "href": "assignments/tutorial_9.html#output-results",
    "title": "Regression Tutorial - Parish Cancer Rates Controlling for Educational Attainment",
    "section": "Output Results",
    "text": "Output Results\n\nla_smoke_filtered &lt;- subset(la_smoke_final, cntyrsd == 1)\nkable(la_smoke_filtered)\n\n\n\n\ncntyrsd\nyear\ntotal_rate_adj_wt\nsmoke_rate_adj_wt\n\n\n\n\n1\n2010\n233.1905\n208.5036\n\n\n1\n2011\n239.2326\n205.4408\n\n\n1\n2012\n212.9739\n205.9210\n\n\n1\n2013\n203.9621\n202.7800\n\n\n1\n2014\n171.4920\n201.0846\n\n\n1\n2015\n199.0707\n192.4747\n\n\n1\n2016\n175.8754\n199.2939\n\n\n1\n2017\n202.7927\n192.2017\n\n\n1\n2018\n182.7003\n191.0731\n\n\n1\n2019\n163.6795\n186.1962"
  },
  {
    "objectID": "materials/01_intro.html",
    "href": "materials/01_intro.html",
    "title": "HPAM 7660 - Week 1 Materials",
    "section": "",
    "text": "Slides - Course Introduction",
    "crumbs": [
      "Course Materials",
      "1: Week 1 Materials, Jan. 16 & Jan. 18"
    ]
  },
  {
    "objectID": "materials/01_intro.html#tuesday-january-14",
    "href": "materials/01_intro.html#tuesday-january-14",
    "title": "HPAM 7660 - Week 1 Materials",
    "section": "",
    "text": "Slides - Course Introduction",
    "crumbs": [
      "Course Materials",
      "1: Week 1 Materials, Jan. 16 & Jan. 18"
    ]
  },
  {
    "objectID": "materials/01_intro.html#thursday-january-16",
    "href": "materials/01_intro.html#thursday-january-16",
    "title": "HPAM 7660 - Week 1 Materials",
    "section": "Thursday, January 16",
    "text": "Thursday, January 16\n\nAssignment - Introduce Yourself\nSlides - U.S. Health Policy Landscape",
    "crumbs": [
      "Course Materials",
      "1: Week 1 Materials, Jan. 16 & Jan. 18"
    ]
  },
  {
    "objectID": "materials/03_computing.html",
    "href": "materials/03_computing.html",
    "title": "HPAM 7660 - Week 3 Materials",
    "section": "",
    "text": "Video - Alice Bartlett - Git for Humans\nTutorial - Tutorial 1 - Getting Started with R, RStuido, Git, and GitHub (we’ll do this in class together)"
  },
  {
    "objectID": "materials/03_computing.html#tuesday-january-30",
    "href": "materials/03_computing.html#tuesday-january-30",
    "title": "HPAM 7660 - Week 3 Materials",
    "section": "",
    "text": "Video - Alice Bartlett - Git for Humans\nTutorial - Tutorial 1 - Getting Started with R, RStuido, Git, and GitHub (we’ll do this in class together)"
  },
  {
    "objectID": "materials/03_computing.html#thursday-february-1",
    "href": "materials/03_computing.html#thursday-february-1",
    "title": "HPAM 7660 - Week 3 Materials",
    "section": "Thursday, February 1",
    "text": "Thursday, February 1\n\nReading - ModernDive - Ch. 1: Getting Started with Data in R\nTutorial - Tutorial 2 - Navigating RStudio: Using R Scripts and Markdown Files (we’ll do this in class together)"
  },
  {
    "objectID": "materials/05_fah.html",
    "href": "materials/05_fah.html",
    "title": "HPAM 7660 - Week 5 Materials",
    "section": "",
    "text": "No Class - Happy Mardi Gras!"
  },
  {
    "objectID": "materials/05_fah.html#tuesday-february-13",
    "href": "materials/05_fah.html#tuesday-february-13",
    "title": "HPAM 7660 - Week 5 Materials",
    "section": "",
    "text": "No Class - Happy Mardi Gras!"
  },
  {
    "objectID": "materials/05_fah.html#thursday-february-15",
    "href": "materials/05_fah.html#thursday-february-15",
    "title": "HPAM 7660 - Week 5 Materials",
    "section": "Thursday, February 15",
    "text": "Thursday, February 15\n\nSlides - Effective Advocacy\nSlides - Political Context"
  },
  {
    "objectID": "materials/07_summarizing.html",
    "href": "materials/07_summarizing.html",
    "title": "HPAM 7660 - Week 7 Materials",
    "section": "",
    "text": "No Class - Capitol Trip"
  },
  {
    "objectID": "materials/07_summarizing.html#tuesday-february-27",
    "href": "materials/07_summarizing.html#tuesday-february-27",
    "title": "HPAM 7660 - Week 7 Materials",
    "section": "",
    "text": "No Class - Capitol Trip"
  },
  {
    "objectID": "materials/07_summarizing.html#thursday-february-29",
    "href": "materials/07_summarizing.html#thursday-february-29",
    "title": "HPAM 7660 - Week 7 Materials",
    "section": "Thursday, February 29",
    "text": "Thursday, February 29\n\nReading - ModernDive Chapter 3.5 through 3.9 - Data Wrangling\nAssignment - Data Assignment 3\nAssignment - Policy Memo Problem Statement\nTutorial - Tutorial 6 - Summarizing Data Tutorial - Louisiana Mortality File"
  },
  {
    "objectID": "materials/09_random_regress.html",
    "href": "materials/09_random_regress.html",
    "title": "HPAM 7660 - Week 9 Materials",
    "section": "",
    "text": "Slides - Policy Evaluation - Estimating Treatment Effects"
  },
  {
    "objectID": "materials/09_random_regress.html#tuesday-march-12",
    "href": "materials/09_random_regress.html#tuesday-march-12",
    "title": "HPAM 7660 - Week 9 Materials",
    "section": "",
    "text": "Slides - Policy Evaluation - Estimating Treatment Effects"
  },
  {
    "objectID": "materials/09_random_regress.html#thursday-march-14",
    "href": "materials/09_random_regress.html#thursday-march-14",
    "title": "HPAM 7660 - Week 9 Materials",
    "section": "Thursday, March 14",
    "text": "Thursday, March 14\n\nAssignment - Policy Memo Preliminary Reference List\nSlides - Policy Evaluation - Regression"
  },
  {
    "objectID": "materials/12_quasi_experiments.html",
    "href": "materials/12_quasi_experiments.html",
    "title": "HPAM 7660 - Week 12 Materials",
    "section": "",
    "text": "Slides - Quasi-Experimental Research Designs"
  },
  {
    "objectID": "materials/12_quasi_experiments.html#tuesday-april-2",
    "href": "materials/12_quasi_experiments.html#tuesday-april-2",
    "title": "HPAM 7660 - Week 12 Materials",
    "section": "",
    "text": "Slides - Quasi-Experimental Research Designs"
  },
  {
    "objectID": "materials/12_quasi_experiments.html#thursday-april-4",
    "href": "materials/12_quasi_experiments.html#thursday-april-4",
    "title": "HPAM 7660 - Week 12 Materials",
    "section": "Thursday, April 4",
    "text": "Thursday, April 4\n\nAssignment - Policy Memo Issue Analysis (See here for an example. You can also find a link to this document on the Resources page.)\nSlides - Quasi-Experimental Research Designs - Medicaid & Mortality"
  },
  {
    "objectID": "materials/15_dc_experience.html",
    "href": "materials/15_dc_experience.html",
    "title": "HPAM 7660 - Week 15 Materials",
    "section": "",
    "text": "Zoom Link\n\n\n\nTime\nPanelists\n\n\n\n\n12:30pm to 1:15pm\nAdvocacy  - Lauren Crawford Shaver, Senior Managing Director, FTI Consulting\n\n\n1:15pm to 2:00pm\nFederal Agencies  - Jim Mathews, Principal, Health Policy Alternatives, former MedPAC Advisory Commission Executive Director  Slides\n\n\n2:00pm to 2:30pm\nBreak\n\n\n2:30pm to 3:15pm\nMedia  - Amy Goldstein, Brookings Institute Visiting Fellow, Washington Post Staff Writer  - Robert King, Deputy Editor Politico  Example reporting:  Goldstein - Medicaid & COVID  King - Medicare Advantage & ACA\n\n\n3:15pm to 4:00pm\nThink Tanks and Foundations  - Jim Capretta, Senior Fellow, American Enterprise Institute  - Tricia Neuman, Senior Vice President, Executive Director for Program and Medicare Policy and Senior Advisor to the President, Kaiser Family Foundation"
  },
  {
    "objectID": "materials/15_dc_experience.html#tuesday-april-23-virtual-d.c.-experience-schedule",
    "href": "materials/15_dc_experience.html#tuesday-april-23-virtual-d.c.-experience-schedule",
    "title": "HPAM 7660 - Week 15 Materials",
    "section": "",
    "text": "Zoom Link\n\n\n\nTime\nPanelists\n\n\n\n\n12:30pm to 1:15pm\nAdvocacy  - Lauren Crawford Shaver, Senior Managing Director, FTI Consulting\n\n\n1:15pm to 2:00pm\nFederal Agencies  - Jim Mathews, Principal, Health Policy Alternatives, former MedPAC Advisory Commission Executive Director  Slides\n\n\n2:00pm to 2:30pm\nBreak\n\n\n2:30pm to 3:15pm\nMedia  - Amy Goldstein, Brookings Institute Visiting Fellow, Washington Post Staff Writer  - Robert King, Deputy Editor Politico  Example reporting:  Goldstein - Medicaid & COVID  King - Medicare Advantage & ACA\n\n\n3:15pm to 4:00pm\nThink Tanks and Foundations  - Jim Capretta, Senior Fellow, American Enterprise Institute  - Tricia Neuman, Senior Vice President, Executive Director for Program and Medicare Policy and Senior Advisor to the President, Kaiser Family Foundation"
  },
  {
    "objectID": "materials/15_dc_experience.html#thursday-april-25-virtual-d.c.-experience-schedule",
    "href": "materials/15_dc_experience.html#thursday-april-25-virtual-d.c.-experience-schedule",
    "title": "HPAM 7660 - Week 15 Materials",
    "section": "Thursday, April 25 Virtual D.C. Experience Schedule",
    "text": "Thursday, April 25 Virtual D.C. Experience Schedule\nZoom Link\n\n\n\nTime\nPanelists\n\n\n\n\n1:30pm to 2:00pm\nFederal Agencies  - Erin Richardson, Chief of Staff, Office of the CMS Administrator\n\n\n2:00pm to 2:30pm\nLegislative Branch  - Amy Hall, Staff Director, Subcommittee on Health, U.S. House of Representatives Committee on Ways and Means\n\n\n2:30pm to 2:45pm\nBreak\n\n\n2:45pm to 3:30pm\nExecutive Branch  - Chris Jennings, Jennings Policy Strategies, former Obama Administration Senior Advisor for Health Policy and Coordinator of Health Reform\n\n\n3:30pm to 4:15pm\nWinston Fellows  - Katarina Morgan  - Ari Panzer"
  },
  {
    "objectID": "materials/index.html",
    "href": "materials/index.html",
    "title": "Course Slides",
    "section": "",
    "text": "Below you will find links to the slides used in class. I will typically post slides before each class period.\n\nCourse Introduction Slides",
    "crumbs": [
      "Overview",
      "Course Slides"
    ]
  },
  {
    "objectID": "resources/R_tables.html",
    "href": "resources/R_tables.html",
    "title": "Making Tables with R",
    "section": "",
    "text": "This guide will cover the basics of exporting summary statistics and regression coefficients from R to formatted tables in LaTeX and Word. There are several ways to create LaTeX tables using R, but I’m going to focus on using the stargazer package for this tutorial. You can download an R Script file that contains all of the code I used to generate the table examples here.\n\n\nR\nFirst, you’ll need to load the stargazer package in R (be sure to install the package before trying to load):\n\nlibrary(stargazer)\n\nLaTeX\nYou’ll also need some way to compile LaTeX files. Overleaf is probably the most commonly used compiler, though you could also download a compiler, like TeXLive to your computer. I recommend Overleaf because it stores all of your projects online and makes for easy collaboration.\nYou’ll need to add the following packages to your LaTeX preamble in order to compile the tables in this guide. These packages need to be added in addition to any other packages you need to compile your document.\n\nWord\nThere is no setup to do in Word. We’ll export R tables to .rtf files and you can open these files directly in Word.\n\n\n\nRun the following R code that uses the webuse package to load an example data set and tabulate some basic summary statistics. Again, you’ll need to be sure the webuse package is installed before trying to use it.\n\nwebuse::webuse(\"census\")\nlibrary(dplyr)\ncensus &lt;- as.data.frame(census)\nvars &lt;- c(\"pop\", \"pop65p\", \"death\", \"marriage\", \"divorce\")\nfor (i in vars) {\n  census[[i]] &lt;- census[[i]] / 1000\n}\nmydata &lt;- census %&gt;%\n  select(pop, pop65p, medage, death, marriage, divorce)\nlibrary(stargazer)\nstargazer(mydata, \n          type = \"latex\", \n          out=\"table_1.tex\", \n          header = FALSE, \n          float = FALSE\n          )\n\nHere we’re loading the sample tibble census and converting it to a data frame using the as.data.frame() command (this is necessary because stargazer works with data frames and not tibbles). The for loop is dividing each variable by 1,000. Then we’re using the select command to choose which variables we want to include in our summary statistics table. Finally, we use the stargazer command to create the LaTeX summary statistic code telling R that we want to save the .tex file in the working directory. The float = FALSE option removes some LaTeX table header statements that I prefer to control directly in LaTeX instead of having them as part of the table output and the header = FALSE option removes a citation note from the .tex output.\nThis table looks ok, but let’s make some adjustments. First, let’s move the number of observations column so that it’s the last column in the table instead of the first column. Second, let’s change the variable names so that they are more informative and limit the number of decimal places to 2.\n\nstargazer(mydata, \n          type = \"latex\", \n          out=\"table_2.tex\", \n          header = FALSE, \n          float = FALSE, \n          digits = 2, \n          summary.stat = c(\"mean\", \"sd\", \"min\", \"max\", \"n\"),\n          covariate.labels=c(\"Population\", \"Pop, 65 and older\", \"Median age\", \"Number of deaths\", \n                             \"Number of marriages\", \"Number of divorces\")\n          )\n\nUnfortunately, there’s no way that I know of (at least no easy way) to change the column labels in stargazer’s basic summary statistics table. So, for now, I think this is the best we can do.\nOftentimes we want summary statistics tables that compare characteristics for a treatment group and a control group. Run the following R code that designates certain states in the census data as “treated” states and the others as “control states”. The following code will generate a treatment vs. control summary statistics table that displays the mean and standard deviation of the variables of interest and presents the information in the standard format of estimate over the standard deviation.\nA note of caution here. This code is extremely hacky and there’s probably a much better and more efficient way to accomplish what I’m doing here. However, I have yet to find the more elegant solution and so am providing code that will get the job done.\n\nsummary_stats &lt;- census %&gt;%\n  mutate(treat = ifelse(state %in% c(\"Louisiana\", \"Texas\", \"Arkansas\", \"Mississippi\"), 1, 0)) %&gt;%\n  group_by(treat) %&gt;%\n  summarise(\n    Population = format(round(mean(pop, na.rm = TRUE), 2), nsmall = 2),\n    sd_pop = format(round(sd(pop, na.rm = TRUE), 2), nsmall = 2),\n    `Pop, 65 and older` = format(round(mean(pop65p, na.rm = TRUE), 2), nsmall = 2),\n    sd_pop65p = format(round(sd(pop65p, na.rm = TRUE), 2), nsmall = 2),\n    `Median age` = format(round(mean(medage, na.rm = TRUE), 2), nsmall = 2),\n    sd_medage = format(round(sd(medage, na.rm = TRUE), 2), nsmall = 2),\n    `Number of deaths` = format(round(mean(death, na.rm = TRUE), 2), nsmall = 2),\n    sd_death = format(round(sd(death, na.rm = TRUE), 2), nsmall = 2),\n    `Number of marriages` = format(round(mean(marriage, na.rm = TRUE), 2), nsmall = 2),\n    sd_marriage = format(round(sd(marriage, na.rm = TRUE), 2), nsmall = 2),\n    `Number of divorces` = format(round(mean(divorce, na.rm = TRUE), 2), nsmall = 2),\n    sd_divorce = format(round(sd(divorce, na.rm = TRUE), 2), nsmall = 2)\n  )\n\nsummary_stats[, grepl(\"^sd_\", colnames(summary_stats))] &lt;- \n  apply(summary_stats[, grepl(\"^sd_\", colnames(summary_stats))], 2, \n        function(x) paste0(\"(\", x, \")\"))\n\nsummary_stats &lt;- summary_stats %&gt;%\n  arrange(desc(treat)) %&gt;%\n  select(-1)\n\nnames(summary_stats)[names(summary_stats) == \"sd_pop\"] &lt;- \" \"\nnames(summary_stats)[names(summary_stats) == \"sd_pop65p\"] &lt;- \"  \"\nnames(summary_stats)[names(summary_stats) == \"sd_medage\"] &lt;- \"   \"\nnames(summary_stats)[names(summary_stats) == \"sd_death\"] &lt;- \"    \"\nnames(summary_stats)[names(summary_stats) == \"sd_marriage\"] &lt;- \"     \"\nnames(summary_stats)[names(summary_stats) == \"sd_divorce\"] &lt;- \"      \"\n\nstargazer(summary_stats, \n          type = \"latex\", \n          out=\"table_4.tex\", \n          header = FALSE, \n          float = FALSE, \n          digits = 2, \n          summary = FALSE,\n          mean.sd = TRUE,\n          flip = TRUE,\n          covariate.labels = c(\"\", \"Treatment\", \"Control\"),\n          )\n\nThis tutorial is still a work on progress. More to come on exporting regression tables from R to LaTeX."
  },
  {
    "objectID": "resources/R_tables.html#exporting-r-tables-to-latex-and-word",
    "href": "resources/R_tables.html#exporting-r-tables-to-latex-and-word",
    "title": "Making Tables with R",
    "section": "",
    "text": "This guide will cover the basics of exporting summary statistics and regression coefficients from R to formatted tables in LaTeX and Word. There are several ways to create LaTeX tables using R, but I’m going to focus on using the stargazer package for this tutorial. You can download an R Script file that contains all of the code I used to generate the table examples here.\n\n\nR\nFirst, you’ll need to load the stargazer package in R (be sure to install the package before trying to load):\n\nlibrary(stargazer)\n\nLaTeX\nYou’ll also need some way to compile LaTeX files. Overleaf is probably the most commonly used compiler, though you could also download a compiler, like TeXLive to your computer. I recommend Overleaf because it stores all of your projects online and makes for easy collaboration.\nYou’ll need to add the following packages to your LaTeX preamble in order to compile the tables in this guide. These packages need to be added in addition to any other packages you need to compile your document.\n\nWord\nThere is no setup to do in Word. We’ll export R tables to .rtf files and you can open these files directly in Word.\n\n\n\nRun the following R code that uses the webuse package to load an example data set and tabulate some basic summary statistics. Again, you’ll need to be sure the webuse package is installed before trying to use it.\n\nwebuse::webuse(\"census\")\nlibrary(dplyr)\ncensus &lt;- as.data.frame(census)\nvars &lt;- c(\"pop\", \"pop65p\", \"death\", \"marriage\", \"divorce\")\nfor (i in vars) {\n  census[[i]] &lt;- census[[i]] / 1000\n}\nmydata &lt;- census %&gt;%\n  select(pop, pop65p, medage, death, marriage, divorce)\nlibrary(stargazer)\nstargazer(mydata, \n          type = \"latex\", \n          out=\"table_1.tex\", \n          header = FALSE, \n          float = FALSE\n          )\n\nHere we’re loading the sample tibble census and converting it to a data frame using the as.data.frame() command (this is necessary because stargazer works with data frames and not tibbles). The for loop is dividing each variable by 1,000. Then we’re using the select command to choose which variables we want to include in our summary statistics table. Finally, we use the stargazer command to create the LaTeX summary statistic code telling R that we want to save the .tex file in the working directory. The float = FALSE option removes some LaTeX table header statements that I prefer to control directly in LaTeX instead of having them as part of the table output and the header = FALSE option removes a citation note from the .tex output.\nThis table looks ok, but let’s make some adjustments. First, let’s move the number of observations column so that it’s the last column in the table instead of the first column. Second, let’s change the variable names so that they are more informative and limit the number of decimal places to 2.\n\nstargazer(mydata, \n          type = \"latex\", \n          out=\"table_2.tex\", \n          header = FALSE, \n          float = FALSE, \n          digits = 2, \n          summary.stat = c(\"mean\", \"sd\", \"min\", \"max\", \"n\"),\n          covariate.labels=c(\"Population\", \"Pop, 65 and older\", \"Median age\", \"Number of deaths\", \n                             \"Number of marriages\", \"Number of divorces\")\n          )\n\nUnfortunately, there’s no way that I know of (at least no easy way) to change the column labels in stargazer’s basic summary statistics table. So, for now, I think this is the best we can do.\nOftentimes we want summary statistics tables that compare characteristics for a treatment group and a control group. Run the following R code that designates certain states in the census data as “treated” states and the others as “control states”. The following code will generate a treatment vs. control summary statistics table that displays the mean and standard deviation of the variables of interest and presents the information in the standard format of estimate over the standard deviation.\nA note of caution here. This code is extremely hacky and there’s probably a much better and more efficient way to accomplish what I’m doing here. However, I have yet to find the more elegant solution and so am providing code that will get the job done.\n\nsummary_stats &lt;- census %&gt;%\n  mutate(treat = ifelse(state %in% c(\"Louisiana\", \"Texas\", \"Arkansas\", \"Mississippi\"), 1, 0)) %&gt;%\n  group_by(treat) %&gt;%\n  summarise(\n    Population = format(round(mean(pop, na.rm = TRUE), 2), nsmall = 2),\n    sd_pop = format(round(sd(pop, na.rm = TRUE), 2), nsmall = 2),\n    `Pop, 65 and older` = format(round(mean(pop65p, na.rm = TRUE), 2), nsmall = 2),\n    sd_pop65p = format(round(sd(pop65p, na.rm = TRUE), 2), nsmall = 2),\n    `Median age` = format(round(mean(medage, na.rm = TRUE), 2), nsmall = 2),\n    sd_medage = format(round(sd(medage, na.rm = TRUE), 2), nsmall = 2),\n    `Number of deaths` = format(round(mean(death, na.rm = TRUE), 2), nsmall = 2),\n    sd_death = format(round(sd(death, na.rm = TRUE), 2), nsmall = 2),\n    `Number of marriages` = format(round(mean(marriage, na.rm = TRUE), 2), nsmall = 2),\n    sd_marriage = format(round(sd(marriage, na.rm = TRUE), 2), nsmall = 2),\n    `Number of divorces` = format(round(mean(divorce, na.rm = TRUE), 2), nsmall = 2),\n    sd_divorce = format(round(sd(divorce, na.rm = TRUE), 2), nsmall = 2)\n  )\n\nsummary_stats[, grepl(\"^sd_\", colnames(summary_stats))] &lt;- \n  apply(summary_stats[, grepl(\"^sd_\", colnames(summary_stats))], 2, \n        function(x) paste0(\"(\", x, \")\"))\n\nsummary_stats &lt;- summary_stats %&gt;%\n  arrange(desc(treat)) %&gt;%\n  select(-1)\n\nnames(summary_stats)[names(summary_stats) == \"sd_pop\"] &lt;- \" \"\nnames(summary_stats)[names(summary_stats) == \"sd_pop65p\"] &lt;- \"  \"\nnames(summary_stats)[names(summary_stats) == \"sd_medage\"] &lt;- \"   \"\nnames(summary_stats)[names(summary_stats) == \"sd_death\"] &lt;- \"    \"\nnames(summary_stats)[names(summary_stats) == \"sd_marriage\"] &lt;- \"     \"\nnames(summary_stats)[names(summary_stats) == \"sd_divorce\"] &lt;- \"      \"\n\nstargazer(summary_stats, \n          type = \"latex\", \n          out=\"table_4.tex\", \n          header = FALSE, \n          float = FALSE, \n          digits = 2, \n          summary = FALSE,\n          mean.sd = TRUE,\n          flip = TRUE,\n          covariate.labels = c(\"\", \"Treatment\", \"Control\"),\n          )\n\nThis tutorial is still a work on progress. More to come on exporting regression tables from R to LaTeX."
  },
  {
    "objectID": "resources/Stata_templates.html",
    "href": "resources/Stata_templates.html",
    "title": "STATA Code Templates",
    "section": "",
    "text": "This guide will…\n##Log File Code Template##\n\ncapture log close\nlocal date : di %tdCY-N-D date(c(current_date), \"DMY\")\nlocal logfile \"$logs/visit_regs_`date'.log\"\nlog using \"`logfile'\", replace text"
  },
  {
    "objectID": "resources/VSCode_Git.html",
    "href": "resources/VSCode_Git.html",
    "title": "Using GitHub with VS Code",
    "section": "",
    "text": "This guide will cover the basics of linking a VS Code project to a GitHub repository and pushing commits. Before beginning this tutorial, you’ll need to install Git on your computer (see this guide for instructions). You’ll also need to download and install Visual Studio Code.\n\n\nAfter installing the software on your computer, you’ll need to create a GitHub repo for your project. If this is your first time creating a GitHub repo, you can follow this guide. Next, open VS Code and make sure that Git is enabled in your settings as follows:\n\n\n\n\n\nOne you’ve created a repo and made sure that Git is enabled in VS Code, you can clone your GitHub repo to your computer using VS Code as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you’ve linked your VS Code project to your GitHub repo, you can now push file changes to GitHub. Let’s walk through an example of editing a file and pushing those edits to GitHub.\n\n\nHere VS Code will ask you if you want to stage all of your changes and commit them directly. You can go ahead and answer “always” to this prompt so that you don’t have to go through the extra step of staging your changes in the future.\n\nBefore selecting “Sync Changes” make sure the branch name in VS Code (here it’s called “main”) matches the branch name on GitHub, otherwise you’ll get an error when you try and push your changes. If the names don’t match, you can click on the branch name in the lower left-hand corner of VS Code and change the name.\n\nThe default VS Code setting is to both push and pull changes to and from GitHub simultaneously (this is in contrast to R Studio, for example, which requires separate push/pull requests).\n\nOnce you’ve synced the changes, you will see that the Source Control window is empty and your changes have been pushed to GitHub (we’ll see this from the GitHub perspective shortly.)\n\nTo track project changes through various commits, you can go back to the Explorer Window and click on “Timeline”.\n\nSelect a commit to review and you will see the changes highlighted in the Edit Window.\n\nFinally, refresh your GitHub repo, open the edited file, and you should see the changes that you just made."
  },
  {
    "objectID": "resources/VSCode_Git.html#vs-code-and-github",
    "href": "resources/VSCode_Git.html#vs-code-and-github",
    "title": "Using GitHub with VS Code",
    "section": "",
    "text": "This guide will cover the basics of linking a VS Code project to a GitHub repository and pushing commits. Before beginning this tutorial, you’ll need to install Git on your computer (see this guide for instructions). You’ll also need to download and install Visual Studio Code.\n\n\nAfter installing the software on your computer, you’ll need to create a GitHub repo for your project. If this is your first time creating a GitHub repo, you can follow this guide. Next, open VS Code and make sure that Git is enabled in your settings as follows:\n\n\n\n\n\nOne you’ve created a repo and made sure that Git is enabled in VS Code, you can clone your GitHub repo to your computer using VS Code as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you’ve linked your VS Code project to your GitHub repo, you can now push file changes to GitHub. Let’s walk through an example of editing a file and pushing those edits to GitHub.\n\n\nHere VS Code will ask you if you want to stage all of your changes and commit them directly. You can go ahead and answer “always” to this prompt so that you don’t have to go through the extra step of staging your changes in the future.\n\nBefore selecting “Sync Changes” make sure the branch name in VS Code (here it’s called “main”) matches the branch name on GitHub, otherwise you’ll get an error when you try and push your changes. If the names don’t match, you can click on the branch name in the lower left-hand corner of VS Code and change the name.\n\nThe default VS Code setting is to both push and pull changes to and from GitHub simultaneously (this is in contrast to R Studio, for example, which requires separate push/pull requests).\n\nOnce you’ve synced the changes, you will see that the Source Control window is empty and your changes have been pushed to GitHub (we’ll see this from the GitHub perspective shortly.)\n\nTo track project changes through various commits, you can go back to the Explorer Window and click on “Timeline”.\n\nSelect a commit to review and you will see the changes highlighted in the Edit Window.\n\nFinally, refresh your GitHub repo, open the edited file, and you should see the changes that you just made."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "HPAM 7660 Resources",
    "section": "",
    "text": "Description here. If you are interested, please email me your resume by Friday, March 15th at noon."
  },
  {
    "objectID": "resources/index.html#fah-fellowship-description",
    "href": "resources/index.html#fah-fellowship-description",
    "title": "HPAM 7660 Resources",
    "section": "",
    "text": "Description here. If you are interested, please email me your resume by Friday, March 15th at noon."
  },
  {
    "objectID": "resources/index.html#policy-memo-example",
    "href": "resources/index.html#policy-memo-example",
    "title": "HPAM 7660 Resources",
    "section": "Policy Memo Example",
    "text": "Policy Memo Example\n\nHPAM 7660 Policy Memo Example"
  },
  {
    "objectID": "resources/index.html#software-installation-guides",
    "href": "resources/index.html#software-installation-guides",
    "title": "HPAM 7660 Resources",
    "section": "Software Installation Guides",
    "text": "Software Installation Guides\n\nGetting Software Set Up\nTroubleshooting Guide"
  },
  {
    "objectID": "resources/index.html#r-tutorials",
    "href": "resources/index.html#r-tutorials",
    "title": "HPAM 7660 Resources",
    "section": "R Tutorials",
    "text": "R Tutorials\n\nHands On Programming with R\nR for Data Science\nR Programming Cheatsheet\n\n\nR Markdown\n\nR Markdown Tutorial\nR Markdown: The Definitive Guide\nR Markdown Cheatsheet\n\n\n\nMaking Tables with R\n\nR to LaTeX/Word Guide"
  },
  {
    "objectID": "resources/index.html#stata-tutuorials",
    "href": "resources/index.html#stata-tutuorials",
    "title": "HPAM 7660 Resources",
    "section": "STATA Tutuorials",
    "text": "STATA Tutuorials\n\nMaking Tables with STATA\n\nSTATA to LaTeX/Word Guide\n\n\n\nSTATA Code Templates\n\nCode templates"
  },
  {
    "objectID": "resources/index.html#git-and-github-tutorials",
    "href": "resources/index.html#git-and-github-tutorials",
    "title": "HPAM 7660 Resources",
    "section": "Git and GitHub Tutorials",
    "text": "Git and GitHub Tutorials\n\nHappy Git with R\nVS Code and GitHub\nOverleaf and GitHub"
  },
  {
    "objectID": "resources/index.html#vs-code-tutorials",
    "href": "resources/index.html#vs-code-tutorials",
    "title": "HPAM 7660 Resources",
    "section": "VS Code Tutorials",
    "text": "VS Code Tutorials\n\nVS Code Installation and Extensions"
  },
  {
    "objectID": "resources/speaker-series.html",
    "href": "resources/speaker-series.html",
    "title": "Gov 50 Speaker Series",
    "section": "",
    "text": "More details to come…"
  },
  {
    "objectID": "resources/troubleshooting.html",
    "href": "resources/troubleshooting.html",
    "title": "Troubleshooting for RStudio and GitHub",
    "section": "",
    "text": "If you are missing your Git tab in RStudio, the most likely culprit is that you simply don’t have the RStudio project for the repository open in RStudio. In the upper right-hand corner of RStudio, you can see the current project that is open. If you do not have a project open, you might see this:\n\nTo get to your project, simply click on that Project: (None) button to reveal a list of recent projects, from which you will usually see the one you are working on:"
  },
  {
    "objectID": "resources/troubleshooting.html#missing-git-tab-in-rstudio",
    "href": "resources/troubleshooting.html#missing-git-tab-in-rstudio",
    "title": "Troubleshooting for RStudio and GitHub",
    "section": "",
    "text": "If you are missing your Git tab in RStudio, the most likely culprit is that you simply don’t have the RStudio project for the repository open in RStudio. In the upper right-hand corner of RStudio, you can see the current project that is open. If you do not have a project open, you might see this:\n\nTo get to your project, simply click on that Project: (None) button to reveal a list of recent projects, from which you will usually see the one you are working on:"
  },
  {
    "objectID": "resources/troubleshooting.html#errors-pushing-to-github",
    "href": "resources/troubleshooting.html#errors-pushing-to-github",
    "title": "Troubleshooting for RStudio and GitHub",
    "section": "Errors pushing to GitHub",
    "text": "Errors pushing to GitHub\nIf you trying to push to GitHub and you get an error saying something like:\n\n/usr/bin/git push origin HEAD:refs/heads/main To https://github.com/gov50-f23/gov-50-hw-2-mattblackwell.git ! [rejected] HEAD -&gt; main (non-fast-forward) error: failed to push some refs to ‘https://github.com/gov50-f23/gov-50-hw-2-mattblackwell.git’ hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: ‘git pull …’) before pushing again. hint: See the ‘Note about fast-forwards’ in ‘git push –help’ for details.\n\nThis can happen if you edit the repository manually on the GitHub website rather than in your local version. Sometimes you can fix this problem by simply hitting the “Pull” button in RStudio:\n\nIf this solves your problem, great! If you get an error message when trying to pull, then you’ll need to resolve the conflicts manually. First start a new session of RStudio by going to the Session menu and hitting “New Session”:\n\nThis will open a new RStudio window. In this new window, we are going to create a new project from the same repository. When creating this new project, be sure to add _new to the end of the project directory name:\n\nNow you should have two RStudio sessions with two different projects: the original local one and a new one directly from GitHub:\n\nAssuming you want to overwrite whatever is on the GitHub website with what is on you local computer, copy the contents of your Rmd file from the old RStudio project to the same Rmd file in the new RStudio project. In the new RStudio project, knit the Rmd file, commit any changes, and then push to GitHub. Thew new RStudio project should be all synced now.\nOnce you are confident that the new project has all of the changes that you want, simply delete the old RStudio project from your computer."
  },
  {
    "objectID": "resources/troubleshooting.html#files-larger-than-100-mb",
    "href": "resources/troubleshooting.html#files-larger-than-100-mb",
    "title": "Troubleshooting for RStudio and GitHub",
    "section": "Files larger than 100 MB",
    "text": "Files larger than 100 MB\nIf you get a push error complaining about files greater than 100 MB, you will need to follow similar steps to the “pull error” steps in the last section. Once you have copied over the contents of your Rmd files, you can then add your data files. For files over 100 MB, you will add them to the the .gitignore file of your new repository. To do this, add the file to your new repository and it will show up in the Git tab. Right-click on the new file in the Git tab and hit the Ignore button:\n\nA dialog box will open and you can hit “Save” which will add or amend a .gitignore file in your repository. You should then stage and commit that .gitignore file and push.\n\nA good practice would be to now write an Rmd file or R script that will load the ignored big data file, subset it to certain rows and columns and then save the file as a csv file using write_csv(). Once you get that csv file to under 100 MB, you can commit that file and use it as the main data file in your main Rmd file."
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Course Staff",
    "section": "",
    "text": "Kevin Callison\n   Tidewater 1915\n   &lt;a href=“mailto:kcallison@tulane.edu”&gt;kcallison@tulane.edu\n   ?var:instructor.twitter\n   Schedule an appointment\n\n\n\n\n\n\n\n\n\n\n\n\n   Laura Royden\n   lroyden@g.harvard.edu\n\n\n\n\n\n\n\n\n\n\n\n\n   Ahmet Akbiyik\n   ahmetakbiyik@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ada Cruz ’24\n   acruz@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Katherine Jackson ’25\n   katherinejackson@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   James René Jolin ’24\n   jamesjolin@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ethan Miles\n   ethanmiles@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Jerry Min\n   jiemin@fas.harvard.edu\n\n\n\n\n\n\n\n\n\n   Pranav Moudgalya ’26\n   pmoudgalya@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Christopher Shen ’26\n   christopher_shen@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Julio Solis Arce\n   jsolisarce@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Andy Wang ’23\n   azwang@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Shriya Yarlagadda ’25\n   syarlagadda1@college.harvard.edu\n\n\n\n\n\n\n\n\n\n\n\n\n   Frank T. Berrios ’25\n\n\n\n\n\n\n\n\n\n   Mitja Bof ’26\n\n\n\n\n\n\n\n\n\n   Kate De Groote ’24\n\n\n\n\n\n\n\n\n\n   Alina Esanu ’24\n\n\n\n\n\n\n\n\n\n   Eric Forteza ’24\n\n\n\n\n\n\n\n\n\n   Alex Heuss ’26\n\n\n\n\n\n\n\n\n\n   Tracy Jiang ’24\n\n\n\n\n\n\n\n\n\n   Ryan McCarthy ’24\n\n\n\n\n\n\n\n\n\n   Zachary Mecca ’24\n\n\n\n\n\n\n\n\n\n   Chris Mesfin ’26\n\n\n\n\n\n\n\n\n\n   Vivian Nguyen ’24\n\n\n\n\n\n\n\n\n\n   Isa Peña ’24\n\n\n\n\n\n\n\n\n\n   Julia Poulson ’26\n\n\n\n\n\n\n\n\n\n   Gowri Rangu ’26\n\n\n\n\n\n\n\n\n\n   Azeez Richardson ’25\n\n\n\n\n\n\n\n\n\n   Cameron Snowden ’25\n\n\n\n\n\n\n\n\n\n   Jason Wang ’24"
  },
  {
    "objectID": "staff.html#head-instructor",
    "href": "staff.html#head-instructor",
    "title": "Course Staff",
    "section": "",
    "text": "Kevin Callison\n   Tidewater 1915\n   &lt;a href=“mailto:kcallison@tulane.edu”&gt;kcallison@tulane.edu\n   ?var:instructor.twitter\n   Schedule an appointment"
  },
  {
    "objectID": "staff.html#head-teaching-fellow",
    "href": "staff.html#head-teaching-fellow",
    "title": "Course Staff",
    "section": "",
    "text": "Laura Royden\n   lroyden@g.harvard.edu"
  },
  {
    "objectID": "staff.html#teaching-fellows",
    "href": "staff.html#teaching-fellows",
    "title": "Course Staff",
    "section": "",
    "text": "Ahmet Akbiyik\n   ahmetakbiyik@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ada Cruz ’24\n   acruz@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Katherine Jackson ’25\n   katherinejackson@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   James René Jolin ’24\n   jamesjolin@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ethan Miles\n   ethanmiles@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Jerry Min\n   jiemin@fas.harvard.edu\n\n\n\n\n\n\n\n\n\n   Pranav Moudgalya ’26\n   pmoudgalya@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Christopher Shen ’26\n   christopher_shen@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Julio Solis Arce\n   jsolisarce@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Andy Wang ’23\n   azwang@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Shriya Yarlagadda ’25\n   syarlagadda1@college.harvard.edu"
  },
  {
    "objectID": "staff.html#course-assistants",
    "href": "staff.html#course-assistants",
    "title": "Course Staff",
    "section": "",
    "text": "Frank T. Berrios ’25\n\n\n\n\n\n\n\n\n\n   Mitja Bof ’26\n\n\n\n\n\n\n\n\n\n   Kate De Groote ’24\n\n\n\n\n\n\n\n\n\n   Alina Esanu ’24\n\n\n\n\n\n\n\n\n\n   Eric Forteza ’24\n\n\n\n\n\n\n\n\n\n   Alex Heuss ’26\n\n\n\n\n\n\n\n\n\n   Tracy Jiang ’24\n\n\n\n\n\n\n\n\n\n   Ryan McCarthy ’24\n\n\n\n\n\n\n\n\n\n   Zachary Mecca ’24\n\n\n\n\n\n\n\n\n\n   Chris Mesfin ’26\n\n\n\n\n\n\n\n\n\n   Vivian Nguyen ’24\n\n\n\n\n\n\n\n\n\n   Isa Peña ’24\n\n\n\n\n\n\n\n\n\n   Julia Poulson ’26\n\n\n\n\n\n\n\n\n\n   Gowri Rangu ’26\n\n\n\n\n\n\n\n\n\n   Azeez Richardson ’25\n\n\n\n\n\n\n\n\n\n   Cameron Snowden ’25\n\n\n\n\n\n\n\n\n\n   Jason Wang ’24"
  }
]